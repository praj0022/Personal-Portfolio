[
  {
    "objectID": "child_pages/modified_data.html",
    "href": "child_pages/modified_data.html",
    "title": "Output Files",
    "section": "",
    "text": "This section provides a preview of the original dataset used in this analysis.\n\n\nShow code\n## Preview of Original Data\n\nlibrary(DT)\nlibrary(here)\n\n# Load original dataset\noriginal_data &lt;- readRDS(\n  here::here(\"posts/Proj_3_wcd2/raw_data/loanData.rds\")\n)\n\ndatatable(\n  head(original_data, 20),     # show first 20 rows\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,           # display 20 rows\n    scrollX = TRUE,            # horizontal scrolling\n    scrollY = \"500px\",         # vertical scrolling\n    paging = FALSE,            # no page numbers — all 20 rows visible\n    autoWidth = TRUE\n  ),\n  caption = \"Preview of the Original Loan Data (First 20 Rows)\"\n)"
  },
  {
    "objectID": "child_pages/modified_data.html#original-data",
    "href": "child_pages/modified_data.html#original-data",
    "title": "Output Files",
    "section": "",
    "text": "This section provides a preview of the original dataset used in this analysis.\n\n\nShow code\n## Preview of Original Data\n\nlibrary(DT)\nlibrary(here)\n\n# Load original dataset\noriginal_data &lt;- readRDS(\n  here::here(\"posts/Proj_3_wcd2/raw_data/loanData.rds\")\n)\n\ndatatable(\n  head(original_data, 20),     # show first 20 rows\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,           # display 20 rows\n    scrollX = TRUE,            # horizontal scrolling\n    scrollY = \"500px\",         # vertical scrolling\n    paging = FALSE,            # no page numbers — all 20 rows visible\n    autoWidth = TRUE\n  ),\n  caption = \"Preview of the Original Loan Data (First 20 Rows)\"\n)"
  },
  {
    "objectID": "child_pages/modified_data.html#modified-data",
    "href": "child_pages/modified_data.html#modified-data",
    "title": "Output Files",
    "section": "Modified Data",
    "text": "Modified Data\nThis section provides a preview of the modified dataset after performing the de-identification and cleaning steps.\n\n\nShow code\nlibrary(here)\nlibrary(knitr)\nlibrary(readxl)\n\n\n# Load the modified dataset\nrelease_dataset &lt;- read.csv(\n  here::here(\"posts/Proj_3_wcd2/data/release_dataset.csv\")\n)\n\n# Show first 10 rows\nkable(head(release_dataset, 10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\norig_chn\nloan_age\norig_trm\noltv\nocltv\nfthb_flg\npurpose\nprop_typ\nnum_unit\nocc_stat\nstate\nmi_pct\naqsn_dte\norig_dte\nmsa\nfcc_cost\npp_cost\nar_cost\nie_cost\ntax_cost\nns_procs\nce_procs\nrmw_procs\no_procs\nrepch_flag\nlpi_dte\nfcc_dte\ndisp_dte\nservicer\nf30_dte\nf60_dte\nf90_dte\nf120_dte\nf180_dte\nfce_dte\nf180_upb\nfce_upb\nf30_upb\nf60_upb\nf90_upb\nmod_flag\nfmod_dte\nfmod_upb\npfg_cost\nnet_loss\nnet_sev\nmodtot_cost\nage_group\nincome_main\nlast_activity_quarter\nfirst_date_quarter\ncscore_b_gen\ndti_p\nloan_amount\ninterest_rate\nlast_rate\nseller_anon\n\n\n\n\n1\nR\nNA\n360\n62\n62\nN\nR\nSF\n1\nP\nCO\nNA\n/Use-12-01\n2016-12-01\n19740\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nQuicken Loans Inc.\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n534968.18\n2019-Q3\n2017-Q1\n700\n34.31025\n170237\n(4,6]\n(4,6]\nSeller 1\n\n\n2\nB\n54\n360\n75\n75\nN\nP\nSF\n2\nI\nCT\nNA\n/Use-12-01\n2017-02-01\n25540\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nNationStar Mortgage, LLC\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(38,48]\n887391.10\n2021-Q3\n2017-Q2\n800\n33.40396\n91535\n(4,6]\n(4,6]\nSeller 2\n\n\n3\nR\nNA\n240\n75\n75\nN\nC\nSF\n1\nI\nTN\nNA\n/Use-12-01\n2016-12-01\n34980\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nQuicken Loans, Llc\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(38,48]\n734186.16\n2020-Q1\n2017-Q1\n700\n39.89688\n113777\n(2,4]\n(2,4]\nSeller 1\n\n\n4\nR\n55\n360\n33\n33\nN\nR\nSF\n1\nP\nCA\nNA\n/Use-12-01\n2017-01-01\n41940\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nMUFG Union Bank, National Association\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(38,48]\n478838.56\n2021-Q3\n2017-Q1\n700\n33.16592\n204159\n(2,4]\n(2,4]\nSeller 3\n\n\n5\nC\n56\n360\n67\n67\nN\nC\nSF\n1\nP\nAR\nNA\n/Use-12-01\n2016-12-01\n22900\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nOther\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n684206.82\n2021-Q3\n2017-Q1\n700\n33.13404\n103263\n(2,4]\n(2,4]\nSeller 3\n\n\n6\nC\nNA\n360\n90\n90\nN\nP\nPU\n1\nP\nAR\n25\n/Use-12-01\n2017-03-01\n22220\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nOther\n2019-04-01\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n110661.4\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n39822.61\n2019-Q2\n2017-Q2\n800\n37.99148\n104733\n(4,6]\n(4,6]\nSeller 4\n\n\n7\nC\n57\n360\n75\n75\nN\nP\nCO\n1\nI\nFL\nNA\n/Use-12-01\n2016-11-01\n33100\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nNew Residential Mortgage LLC\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n709393.14\n2021-Q3\n2017-Q1\n800\n38.44388\n88679\n(4,6]\n(4,6]\nSeller 3\n\n\n8\nR\n56\n180\n59\n59\nN\nC\nSF\n1\nP\nOH\nNA\n/Use-12-01\n2016-12-01\n49660\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nCitizens Bank, National Association\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n713221.74\n2021-Q3\n2017-Q1\n800\n23.96745\n54017\n(2,4]\n(2,4]\nSeller 3\n\n\n9\nC\nNA\n360\n80\n80\nY\nP\nSF\n1\nP\nTX\nNA\n/Use-12-01\n2017-01-01\n12420\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nSuntrust Bank\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n716033.97\n2019-Q2\n2017-Q1\n700\n29.35040\n166669\n(4,6]\n(4,6]\nSeller 5\n\n\n10\nR\nNA\n360\n65\n65\nN\nP\nSF\n1\nP\nTN\nNA\n/Use-12-01\n2017-03-01\n28940\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\n\n\n\nPingora Loan Servicing, LLC\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0\nNA\nNA\nNA\nNA\nNA\nNA\n(28,38]\n745701.72\n2021-Q2\n2017-Q2\n700\n46.95794\n234125\n(4,6]\n(4,6]\nSeller 3"
  },
  {
    "objectID": "child_pages/modified_data.html#read-me",
    "href": "child_pages/modified_data.html#read-me",
    "title": "Output Files",
    "section": "Read Me",
    "text": "Read Me\n\n\nShow code\nreadme_path &lt;- here::here(\"posts/Proj_3_wcd2/README-Pooja-Rajendran-Raju.txt\")\n\nreadme_text &lt;- readLines(readme_path, warn = FALSE, encoding = \"UTF-8\")\n\ncat(paste(readme_text, collapse = \"\\n\"))\n\n\n1. Origin of data: The release_dataset is derived from loan performance data provided by ABC Bank. The original dataset contained personal information such as names, ages, and demographic variables, as well as transaction information related to mortgage loans. The data was collected as part of ABC Bank's efforts to analyse and understand the credit performance of mortgage loans.\n\n2. Files included with data: \n\na. Data Code Book: Comprehensive reference guide for understanding the variables and structure of the dataset. It provides detailed descriptions of each variable, including their names, types, values, and any transformations or processing applied to them. \n\nb. README file: Provides information about the dataset, including its origin, contents, and usage guidelines.\n\n3. Author and the date of release: The dataset was processed and released by Pooja Rajendran Raju on 17th May 2024. \n\n4. Data Usage and Assumptions:\nUsers can utilise the dataset for analysing mortgage loan performance trends and patterns. \n\nAssumptions made during data processing include:\n\na. De-identification of personal information: Personal information such as names and addresses has been removed or replaced with anonymized identifiers to protect individuals' privacy and and age has been binned into age groups to preserve privacy.\n\nb. Transactional data anonymization: Transactional details have been aggregated or generalised to prevent identification of individual loan transactions.\n\nc. Compliance with regulations: The dataset has been processed in compliance with relevant privacy and data protection laws and regulations, ensuring that individuals' rights are respected and protected."
  },
  {
    "objectID": "child_pages/modified_data.html#data-code-book",
    "href": "child_pages/modified_data.html#data-code-book",
    "title": "Output Files",
    "section": "Data Code Book",
    "text": "Data Code Book\n\n\nShow code\nlibrary(DT)\nlibrary(janitor)\nlibrary(dplyr)  \n\n\ncodebook_path &lt;- here::here(\"posts/Proj_3_wcd2/data/data-dictionary-Pooja-Rajendran-Raju.xlsx\")\n\ncodebook &lt;- read_excel(codebook_path)\n\n# Remove completely blank columns\ncodebook &lt;- codebook %&gt;% select(where(~ !all(is.na(.))))\n\n# Clean column names (no weird characters)\ncodebook &lt;- clean_names(codebook)\n\n\ndatatable(\n  codebook,\n  rownames = FALSE,\n  options = list(\n    scrollX = TRUE,         # horizontal scroll\n    scrollY = \"500px\",      # fixed height + vertical scroll\n    paging = TRUE,          # keep table tidy\n    autoWidth = TRUE,\n    pageLength = 20,\n\n    # FORCE specific column widths (adjustable)\n    columnDefs = list(\n      list(width = \"100px\", targets = 0),\n      list(width = \"120px\", targets = 1),\n      list(width = \"180px\", targets = 2),\n      list(width = \"350px\", targets = 3),   # Description column\n      list(width = \"220px\", targets = 4),\n      list(width = \"120px\", targets = 5),\n      list(width = \"150px\", targets = 6)\n    )\n  )\n) %&gt;%\n  formatStyle(\n    columns = names(codebook),\n    `white-space` = \"normal\",   # wrap long text\n    `word-wrap` = \"break-word\"\n  )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Here’s a peek at my exploration of all things data",
    "section": "",
    "text": "Exploring Finanace Data using PCA and Factor Analysis\n\n\n\nGroup Project\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\nPooja Rajendran Raju and Others\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Dietary Trends\n\n\n\nExploration\n\n\n\n\n\n\n\n\n\nJul 1, 2025\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nPrinciple Component Analysis and Classifiers\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction - Exploring Machine Learning\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nThe Relationship Between Weather and Atmospheric Pollutants\n\n\n\nGroup Project\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\nTravis Rutledge and Pooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Isolation to Connection: How the World Managed Mental Health in 2020\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nDo different sensors at the same location report the same values?\n\n\n\nGroup Project\n\n\n\n\n\n\n\n\n\nSep 24, 2024\n\n\nPooja Rajendran Raju & Thi My Ngoc Tran\n\n\n\n\n\n\n\n\n\n\n\n\nDiving Deeper into Data Exploration: Visualisation\n\n\n\nExploration\n\n\n\n\n\n\n\n\n\nSep 16, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nRenewable Energy Transition over Decades Around the World\n\n\n\nGroup Project\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nEchidna Group\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking Down Different Media Formats\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nAug 22, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nCoping Methods App Documentation\n\n\n\nApp\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nDiving Deeper into Data Exploration: Olympic Data\n\n\n\nExploration\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nDiving Deeper into Data Exploration: Introduction\n\n\n\nExploration\n\n\n\n\n\n\n\n\n\nAug 5, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nPatagonia - Part 2 SCM Practices and Strategies\n\n\n\nSupply Chain\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nExploring data methods to uncover how individuals managed anxiety and depression in 2020\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nData deidentification and modification\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nPatagonia - Exploring Sustainable Supply Chain\n\n\n\nSupply Chain\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Fundamentals of Data\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nPooja Rajendran Raju\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/scm1/index.html",
    "href": "posts/scm1/index.html",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "Established in 1973 by Yvon Chouinard, Patagonia began as a small climbing gear business before diversifying into outdoor apparel. The brand swiftly gained renown for its durable outdoor clothing. From the outset, Patagonia prioritized environmental sustainability and ethics, pioneering initiatives like recycled materials and fair labor standards, earning recognition for its Fair Trade Certified™ apparel.\n\n\n\nProlonging the longevity of its products and reducing waste. Example: Worn Wear Program promotes sustainability by repairing and recycling outdoor apparel and gear.\nProviding a lifetime warranty for all items sold.\nSelecting products based on a combination of functional necessity and environmental impact, utilizing metrics like environmental profit and loss evaluations.\nIts core products include outdoor apparel: Jackets, Fleeces, Base Layers, Backpacks & Gears and Footwear\n\n\n\n\n\nPatagonia’s customer base consists primarily of environmentally conscious individuals who value transparency, sustainability, and ethical practices along with individuals interested in sports and outdoors adventures.\nThey are attracted to Patagonia’s commitment to disclosing the origin and manufacturing process of its products and sustainability measures undertaken at every stage. Materials used by company Environmental Responsibility\n\n\n\n\n\nAs of May 2023, Patagonia is partnered with 70 suppliers globally. Here are a few examples:\n\n\nFinished Goods Outerwear : Bangladesh: Youngone - KSL , Colombia: Supertex - Eje Cafetero\nFinished Goods Boots : Portugal: HUGAL - Indústria de Calçado, Lda\nFinished Goods Sportswear: Sri Lanka Hirdaramani - Kahathuduwa\n\n\nPatagonia updates a comprehensive list of suppliers biannually, in May and November, making it publicly available. Supplier List.\nPatagonia collaborates with suppliers committed to sustainability, fair labor, and transparent supply chains, implementing multi-tier checks.\n\n\n\n\n\n\n\nPatagonia using different selling formats through both physical and online channels including:\n\n\nCompany-Owned Retail stores: It approximately boasts of 70 retail stores globally, located in cities like New York, Chicago in the United States, Paris and Amsterdam in Europe and Osaka, Tokyo in Asia, among others.\nOnline Retail: The company hosts its own website to sell products. Where customers can browse and purchase items online. Official Website\nAuthorized Dealers: It has a network of dealers whose ideals align with their pro-environmental motives. A list of its dealers is shared on their website. To list a few: Evo, Great Outdoor Provision Co., The Trail Head and Pack & Paddle\nWholesale Partnerships: The company engages in wholesale distribution, enabling other retailers to carry and sell Patagonia products in their stores. A few wholesale dealers are: LA Vintage, Aggregator Wholesale\n\n\n\n\n\n\nPatagonia’s holistic sustainability approach, transparency, and advocacy make it a compelling case study at the intersection of business, ethics, and environmental conservation amidst the rise of fast fashion.\nIt prioritizes product innovation using sustainable materials and conventional manufacturing to reduce its environmental footprint and maintains robust environmental and animal welfare programs for material sourcing.\nInitiatives such as the following distinguish it from others in the industry: Supply Chain Environmental Responsibility Program , Responsible Wool Standard, Fair Trade, Living Wage, Regenerative Organic Certification\n\n\n\n\n\n\nCSR Initiatives Source: (Patagonia, 2021)\n\n\n\n\n\nPatagonia collaborates with NGOs, governments, academic institutions, and research centers to develop environmentally responsible supply chains, supporting its sustainability goals. Additionally, it allocates 1% of its annual revenue to environmental initiatives through the “1% for the Planet Foundation.”\n\n\n\n\n\nThe data collection involved thorough research from Patagonia’s official website, blogs, founder’s notes, company history, and supplier list. Most data came from open-source information provided by the company.\nAcademic research articles and papers sourced from platforms such as ResearchGate, Scholarship @ Claremont, and Wiley Online Library were consulted. Business-related data was also obtained from Statista to enrich the analysis.\nAll sources are referenced below."
  },
  {
    "objectID": "posts/scm1/index.html#core-products-and-services",
    "href": "posts/scm1/index.html#core-products-and-services",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "Prolonging the longevity of its products and reducing waste. Example: Worn Wear Program promotes sustainability by repairing and recycling outdoor apparel and gear.\nProviding a lifetime warranty for all items sold.\nSelecting products based on a combination of functional necessity and environmental impact, utilizing metrics like environmental profit and loss evaluations.\nIts core products include outdoor apparel: Jackets, Fleeces, Base Layers, Backpacks & Gears and Footwear"
  },
  {
    "objectID": "posts/scm1/index.html#customer-base",
    "href": "posts/scm1/index.html#customer-base",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "Patagonia’s customer base consists primarily of environmentally conscious individuals who value transparency, sustainability, and ethical practices along with individuals interested in sports and outdoors adventures.\nThey are attracted to Patagonia’s commitment to disclosing the origin and manufacturing process of its products and sustainability measures undertaken at every stage. Materials used by company Environmental Responsibility"
  },
  {
    "objectID": "posts/scm1/index.html#suppliers",
    "href": "posts/scm1/index.html#suppliers",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "As of May 2023, Patagonia is partnered with 70 suppliers globally. Here are a few examples:\n\n\nFinished Goods Outerwear : Bangladesh: Youngone - KSL , Colombia: Supertex - Eje Cafetero\nFinished Goods Boots : Portugal: HUGAL - Indústria de Calçado, Lda\nFinished Goods Sportswear: Sri Lanka Hirdaramani - Kahathuduwa\n\n\nPatagonia updates a comprehensive list of suppliers biannually, in May and November, making it publicly available. Supplier List.\nPatagonia collaborates with suppliers committed to sustainability, fair labor, and transparent supply chains, implementing multi-tier checks."
  },
  {
    "objectID": "posts/scm1/index.html#functional-factors",
    "href": "posts/scm1/index.html#functional-factors",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "Patagonia using different selling formats through both physical and online channels including:\n\n\nCompany-Owned Retail stores: It approximately boasts of 70 retail stores globally, located in cities like New York, Chicago in the United States, Paris and Amsterdam in Europe and Osaka, Tokyo in Asia, among others.\nOnline Retail: The company hosts its own website to sell products. Where customers can browse and purchase items online. Official Website\nAuthorized Dealers: It has a network of dealers whose ideals align with their pro-environmental motives. A list of its dealers is shared on their website. To list a few: Evo, Great Outdoor Provision Co., The Trail Head and Pack & Paddle\nWholesale Partnerships: The company engages in wholesale distribution, enabling other retailers to carry and sell Patagonia products in their stores. A few wholesale dealers are: LA Vintage, Aggregator Wholesale"
  },
  {
    "objectID": "posts/scm1/index.html#what-makes-patagonia-unique",
    "href": "posts/scm1/index.html#what-makes-patagonia-unique",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "Patagonia’s holistic sustainability approach, transparency, and advocacy make it a compelling case study at the intersection of business, ethics, and environmental conservation amidst the rise of fast fashion.\nIt prioritizes product innovation using sustainable materials and conventional manufacturing to reduce its environmental footprint and maintains robust environmental and animal welfare programs for material sourcing.\nInitiatives such as the following distinguish it from others in the industry: Supply Chain Environmental Responsibility Program , Responsible Wool Standard, Fair Trade, Living Wage, Regenerative Organic Certification\n\n\n\n\n\n\nCSR Initiatives Source: (Patagonia, 2021)\n\n\n\n\n\nPatagonia collaborates with NGOs, governments, academic institutions, and research centers to develop environmentally responsible supply chains, supporting its sustainability goals. Additionally, it allocates 1% of its annual revenue to environmental initiatives through the “1% for the Planet Foundation.”"
  },
  {
    "objectID": "posts/scm1/index.html#data-collection",
    "href": "posts/scm1/index.html#data-collection",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "",
    "text": "The data collection involved thorough research from Patagonia’s official website, blogs, founder’s notes, company history, and supplier list. Most data came from open-source information provided by the company.\nAcademic research articles and papers sourced from platforms such as ResearchGate, Scholarship @ Claremont, and Wiley Online Library were consulted. Business-related data was also obtained from Statista to enrich the analysis.\nAll sources are referenced below."
  },
  {
    "objectID": "posts/scm1/index.html#dimensions-of-customer-value",
    "href": "posts/scm1/index.html#dimensions-of-customer-value",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "Dimensions of Customer Value",
    "text": "Dimensions of Customer Value\n\nConformance to Requirements\n\n\nPatagonia emphasizes quality, longevity, and eco-friendliness in its products, using top-notch materials and ethical production standards to ensure environmental sustainability. Customers value the brand’s dedication to crafting dependable and ethically sourced outdoor clothing and equipment.\n\n\nProduct Selection\n\n\nPatagonia provides a wide range of products suitable for outdoor enthusiasts, including technical outerwear, performance gear, casual clothing and accessories.\nIt caters to different activities and lifestyles producing high performing gear, striking a balance between functionality and sustainability.\n\n\nPrice and Brand\n\n\nPatagonia is a premium outdoor apparel brand with higher-priced products as compared to its competitors, due to their emphasis on quality, durability, and ethical values, which customers value and are willing to invest in.\nIts positive reputation for ethical business practices and environmental care enhances the value of its products and customers perception of the brand.\n\n\nValue-added Services\n\n\nPatagonia provides additional services to enhance customer satisfaction. Repair Services and programs such as Worn Wear encourage customers to extend the life of their clothing by repairing, reusing, and recycling Patagonia products.\nPatagonia offers generous product warranties and guarantees on their products.\n\n\nRelationships and Experiences\n\n\nPatagonia emphasizes authentic connections with customers through personalized interactions and community engagement. Through events, workshops, and online platforms, outdoor enthusiasts have opportunities to connect, exchange experiences, and gain knowledge collectively."
  },
  {
    "objectID": "posts/scm1/index.html#service-quality-discussion",
    "href": "posts/scm1/index.html#service-quality-discussion",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "Service Quality Discussion",
    "text": "Service Quality Discussion\nSERVQUAL Service quality model in Patagonia.\n\nReliability\n\n\nPatagonia guarantees the quality and durability of its products through lifetime warranties and repair services. Customers can rely on their Patagonia gear to perform effectively in diverse outdoor environments.\n\n\nResponsiveness\n\n\nPatagonia’s customer support team is recognized for its prompt handling of inquiries, offering product suggestions, and swiftly resolving concerns. Its online resources and educational content provide access to information and support for guidance on sustainability, product maintenance, or outdoor pursuits.\n\n\nAssurance\n\n\nPatagonia builds trust through transparency, sustainability, and ethical conduct. Its transparent supply chain, manufacturing procedures, and environmental efforts, assures customers of its integrity and commitment to social and environmental ethics. Fair Trade Certified™ products guarantee fair wages and safe working conditions, reinforcing Patagonia’s ethical commitments.\n\n\nEmpathy\n\n\nPatagonia shows empathy by considering customer feedback and integrating it into product design and improvement. Through customer feedback platforms like surveys and online forums, the company provides avenues for customers to express their experiences and ideas directly to Patagonia.\n\n\nTangibles\n\n\nPatagonia’s focus on the design and layout of its retail stores, longevity of its products, and efficiency of its communication materials, like website content and product packaging elevates the holistic customer journey and strengthens its brand identity."
  },
  {
    "objectID": "posts/scm1/index.html#service-gap-discussion",
    "href": "posts/scm1/index.html#service-gap-discussion",
    "title": "Patagonia - Exploring Sustainable Supply Chain",
    "section": "Service Gap Discussion",
    "text": "Service Gap Discussion\n\nConsidering the brand’s dedication to eco-friendly supply chains, there may be unique challenges in product availability and inventory management, especially for popular or seasonal items. Variations in customer preferences, traditional manufacturing processes, and ongoing audits to ensure suppliers adhere to sustainable practices could result in occasional product shortages or unavailability.\nPatagonia invested in a new inventory analytics model by Aviana which aims to enhance merchandising and supply planning, providing detailed inventory forecasts as said by Sandy Buechley, Business Intelligence Manager, Patagonia"
  },
  {
    "objectID": "posts/assign01-praj0022/index.html",
    "href": "posts/assign01-praj0022/index.html",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "",
    "text": "The data to used is available from Tidy Tuesday 28 May 2024 page. Downloaded the data from here, using the tidytuesdayR package.\nIn addition the gardenR package, available from remotes::install_github(\"llendway/gardenR\") has extra details about the garden."
  },
  {
    "objectID": "posts/assign01-praj0022/index.html#introduction",
    "href": "posts/assign01-praj0022/index.html#introduction",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "",
    "text": "The data to used is available from Tidy Tuesday 28 May 2024 page. Downloaded the data from here, using the tidytuesdayR package.\nIn addition the gardenR package, available from remotes::install_github(\"llendway/gardenR\") has extra details about the garden."
  },
  {
    "objectID": "posts/assign01-praj0022/index.html#exploration",
    "href": "posts/assign01-praj0022/index.html#exploration",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "Exploration",
    "text": "Exploration\n\nHow many types of vegetables were grown in each year?\n\n\n# Reading the data\nplanting_2020 &lt;- read.csv(\"data/planting_2020\")\nplanting_2021 &lt;- read.csv(\"data/planting_2021\")\n\n\n# Counting number of unique vegetables in 2020\nvegetables_2020 &lt;- planting_2020 |&gt; \n                   select(vegetable) |&gt;\n                   unique() |&gt; \n                   count() |&gt;\n                   rename(count_2020 = n )\n\n# Counting number of unique vegetables in 2021\nvegetables_2021 &lt;- planting_2021 |&gt; \n                   select(vegetable) |&gt;\n                   unique() |&gt;\n                   count() |&gt; \n                   rename(count_2021 = n )\n\ncbind(vegetables_2020, vegetables_2021)\n\n  count_2020 count_2021\n1         31         26\n\n\n\nIn 2020, 31 types of vegetables were grown, where as in 2021 only 26 types of vegetables were grown.\n\n\nHow many vegetables were grown in 2020 that were not grown in 2021?\n\n\n# Creating variable with all unique vegetables in 2020\nunique_2020 &lt;- planting_2020 |&gt; \n                   select(vegetable) |&gt;\n                   unique()\n\n# Creating variable with all unique vegetables in 2021\nunique_2021 &lt;- planting_2021 |&gt; \n                   select(vegetable) |&gt;\n                   unique()\n\n# Using anti_join to get the vegetables grown in 2020 but not in 2021 \ndifference_2020 &lt;- anti_join(unique_2020, unique_2021, by = c(\"vegetable\"))\n\n# Counting the number of vegetables grown in 2020 but not in 2021 \nnrow(difference_2020)\n\n[1] 10\n\ndifference_2020\n\n          vegetable\n1       Swiss chard\n2          broccoli\n3          pumpkins\n4       hot peppers\n5  brussels sprouts\n6          jalapeño\n7             melon\n8          rudabaga\n9      strawberries\n10         kohlrabi\n\n\n\n10 vegetables were grown in 2020 that were not grown in 2021. The vegetables are listed in the table above.\n\n\nWhat are some of the data recording errors that can be seen by comparing vegetables grown in each year?\n\n\nThe names of vegetables and varieties are not recorded in a consistent manner. In some places, the first letter is capitalized and in other places its not. Variations in the use of upper and lower case when spelling the same vegetable/variety.\nFor different years, the same vegetable is spelled differently. Mistakes in spelling the vegetable.\nIn some places the vegetables/variety is written in singular and other places its in plural\nExamples in planting data for 2020 and 2021:\n\n\nIn planting_2021 - swiss chard ; In planting _2020 - Swiss Chard\n\n\nHere, S in “swiss” is in uppercase for 2020, whereas in 2021 it is lower case.\n\n\nIn planting_2021 - pumpkin ; In planting _2020 - pumpkins\n\n\nHere, “pumpkin” is written as singular in 2021, whereas in 2020 it is written in plural.\n\n\nIn planting_2021 - rutabaga ; In planting _2020 - rudabaga\n\n\nHere, “rutabaga” is misspelled while recording data in 2020 as “rudabaga” instead as rutabaga.\nSimilar inconsistencies can be found in the variable “variety”.\nThese data recording errors can lead to incorrect analysis as functions like “unique” are case sensitive."
  },
  {
    "objectID": "posts/assign01-praj0022/index.html#exploring-harvest-data-from-two-years",
    "href": "posts/assign01-praj0022/index.html#exploring-harvest-data-from-two-years",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "Exploring harvest data from two years",
    "text": "Exploring harvest data from two years\n\nJoining the harvest data for the two years, after adding a new variable each, called year.\n\n\n# Reading the data\nharvest_2020 &lt;- read.csv(\"data/harvest_2020\")\nharvest_2021 &lt;- read.csv(\"data/harvest_2021\")\n\n\n# Adding variable year to harvest_2020\nharvest_2020 &lt;- harvest_2020 |&gt; \n                mutate( year = year(date))\n\n# Adding variable year to harvest_2021\nharvest_2021 &lt;- harvest_2021 |&gt; \n                mutate( year = year(date))\n\n# Joining Data \ncombined_harvest &lt;- full_join(harvest_2020, harvest_2021)\n\n\nFor the tomatoes compareing the weight of the harvest in both years.\n\n\n# Calculating the weight of harvest for tomatoes in 2020\nweight_2020 &lt;- harvest_2020 |&gt; \n               filter(vegetable == \"tomatoes\") |&gt; \n               select(weight) |&gt;\n               sum()\n\n# Calculating the weight of harvest for tomatoes in 2021\nweight_2021 &lt;- harvest_2021 |&gt; \n               filter(vegetable == \"tomatoes\") |&gt; \n               select(weight) |&gt;\n               sum()\n\ncbind(weight_2020, weight_2021)\n\n     weight_2020 weight_2021\n[1,]      158231      141509\n\n\n\nThe weight of harvest for tomatoes in 2020 is 158231 grams and the weight of harvest for tomatoes in 2021 is 141509 grams. The weight of harvest is more in 2020 as compared to that in 2021.\n\n\nExplaining what might be a problem when comparing the tomato yield from these two years.\n\n\ninconsistency &lt;- combined_harvest |&gt;\n     select(vegetable, variety, year) |&gt;\n     filter(vegetable == \"tomatoes\") |&gt;\n     group_by(variety, year) |&gt;\n     count() |&gt; \n     pivot_wider(names_from = year, values_from = n)\n\nhead(inconsistency, 6)\n\n# A tibble: 6 × 3\n# Groups:   variety [6]\n  variety     `2020` `2021`\n  &lt;chr&gt;        &lt;int&gt;  &lt;int&gt;\n1 Amish Paste     30     19\n2 Better Boy      23     NA\n3 Big Beef        21     24\n4 Black Krim      12     12\n5 Bonny Best      27     15\n6 Brandywine      16     NA\n\n\n\nBased on the output of the code above, we can observe that in both years the same varieties of tomatoes were not harvested. There is no consistency with the varieties harvested in both years.\nFurther, for same varieties harvested in both years, the number of harvests are different."
  },
  {
    "objectID": "posts/assign01-praj0022/index.html#exploring-planting-data-with-garden-layout",
    "href": "posts/assign01-praj0022/index.html#exploring-planting-data-with-garden-layout",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "Exploring planting data with garden layout",
    "text": "Exploring planting data with garden layout\nThe planting data has the label of the garden plot where each vegetable is grown. The gardenR package has the spatial coordinates of these spots in the garden.\n\nJoin the planting_2020 and planting_2021 data, after adding a variable labeling the year. And then joining with the garden_coords data, so that the locations of each plot are recorded.\n\n\n# Reading garden_coords data\ngarden_coords &lt;- read.csv(\"data/garden_coords\")\n\n\n# Adding variable year to harvest_2020\nplanting_2020 &lt;- planting_2020 |&gt; \n                mutate( year = year(date))\n\n# Adding variable year to harvest_2021\nplanting_2021 &lt;- planting_2021 |&gt; \n                mutate( year = year(date))\n\n# Joining planting data of 2020 and 2021\ncombined_20_21 &lt;- full_join(planting_2020, planting_2021)\n\n# Joining combined planting data of 2020 and 2021 with garden coordinates\nplanting_coords &lt;- full_join(combined_20_21 , garden_coords, by = \"plot\")\n\n\nPlot garden layout, with locations of tomatoes coloured, facetted by year.\n\n\n# Creating data for labels\nfor_labs &lt;- garden_coords |&gt;\n  group_by(plot) |&gt;\n  summarize(x = mean(x), y = mean(y), .groups = 'drop')\n\n# Plotting the garden layout\nggplot() +\n  geom_polygon(data = planting_coords |&gt; filter(!is.na(year)),                \n               aes(x = x, y = y, group = plot), \n               color = \"black\") +\n  geom_polygon(data = planting_coords |&gt; filter(vegetable == \"tomatoes\"),\n               aes(x = x, y = y, group = plot), \n               fill = \"red\", \n               alpha = 0.5, \n               color = \"black\") + \n  geom_text(data = for_labs, \n            aes(x = x, y = y, label = plot), \n            color = \"yellow\",\n            size = 5) +\n  theme_void() +\n  theme(panel.background = element_rect(fill = \"darkgray\")) + \n  facet_wrap(~year)\n\n\n\n\nGarden Plot Layout\n\n\n\n\n\nThe above plot shows the garden plot with the locations of tomatoes highlighted in red for years 2020 and 2021.\nIn 2020, plots N and J are fully planted with tomatoes, while plot O has been planted with tomatoes along with few other vegetables.\nIn 2021, plots N and D are fully planted with tomatoes, while plot O has been planted with tomatoes along with few other vegetables."
  },
  {
    "objectID": "posts/assign01-praj0022/index.html#generative-ai-analysis",
    "href": "posts/assign01-praj0022/index.html#generative-ai-analysis",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "Generative AI analysis",
    "text": "Generative AI analysis\n\nI used generative AI to understand what kind of data inconsistencies to look for in the datasets.\nI used it to find information on functions like unique, full_join and bind_rows.\nFurther, I used it to understand how to layer the geom_polygon function to make the garden plot, coloring the locations of tomatoes differently. I faced issues while trying to color the plot of tomatoes differently and I wasn’t sure how to add two layers of geom_polygon to the same plot and if it is possible at all. With the use of ChatGPT, I was able to arrive at the solution using the questions in the link to my ChatGPT chat history.\nI also used it to understand if the function “filter” can be used when passing data to geom_polygon and I was able to clarify my doubt with the help of ChatGPT."
  },
  {
    "objectID": "posts/assign01-praj0022/index.html#citations",
    "href": "posts/assign01-praj0022/index.html#citations",
    "title": "Diving Deeper into Data Exploration: Introduction",
    "section": "Citations",
    "text": "Citations\n\nLendway, L (2024). gardenR: Gardening Resources and Tools. R package https://github.com/llendway/gardenR\nWickham, H, Chang, W, Wickham, M, Hester, J, Bryan, J, & others (2024). tidyverse: R package version 2.0.0, https://CRAN.R-project.org/package=tidyverse\nOpenAI, 2024. ChatGPT (Version 4) https://www.openai.com/chatgpt"
  },
  {
    "objectID": "posts/Proj_5_colb/index.html",
    "href": "posts/Proj_5_colb/index.html",
    "title": "Renewable Energy Transition over Decades Around the World",
    "section": "",
    "text": "The report centers on the comprehensive analysis tracing the trajectory of energy generation trends spanning ancient to modern times, elucidating the notable transition from traditional fuels to renewables. It underscores the historical dominance of fossil fuels, the dwindling role of nuclear power, and the accelerating adoption of renewables, particularly in leading nations such as China and Germany.\nFurthermore, the report meticulously examines the global surge in renewable energy utilization across diverse countries, exploring the factors contributing to the gradual rise in world averages. Emphasizing the pivotal role of renewable sources like solar, wind, hydro, and geothermal, it advocates for sustainable solutions with minimal environmental footprint.\nBeyond energy statistics, the study delves into the intricate relationship between the proportion of renewable consumption and countries’ GDP, seeking to unravel the nuanced dynamics between these two vital factors. By weaving together these insights, the report offers a holistic understanding of the evolving energy landscape and underscores the imperative for strategic shifts towards sustainable energy practices worldwide."
  },
  {
    "objectID": "posts/Proj_5_colb/index.html#research-question",
    "href": "posts/Proj_5_colb/index.html#research-question",
    "title": "Renewable Energy Transition over Decades Around the World",
    "section": "Research Question",
    "text": "Research Question\nOur research is guided by the following objectives:\n\nTo Analyze electricity generation trends from 1985 to 2021 to gain insights into shifts and trends in renewable energy sources. It aims to provide an overview of our current standing in producing reliable, sustainable, and clean electricity and to identify areas for improvement. Furthermore, to comprehensively grasp renewable energy generation across various geographical regions, we aim to identify the countries that have been leading in renewable energy production over recent years.\nTo investigate, which five countries have shown the most significant advancement in the proportion of renewable energy, and approximately from which regions do these countries originate?\nTo delve into the correlation between share of renewable consumption and countries’ GDP, aiming to picture the nuanced relationship between energy preference and economic excellence"
  },
  {
    "objectID": "posts/Proj_5_colb/index.html#dataset-introduction",
    "href": "posts/Proj_5_colb/index.html#dataset-introduction",
    "title": "Renewable Energy Transition over Decades Around the World",
    "section": "Dataset Introduction",
    "text": "Dataset Introduction\n\nRenewable Share Energy: Includes information on the share of primary energy consumption from renewable sources. The ratio of renewable energy use in countries around the world from 1965 to 2022 is recorded. This report will use data from 1985-2021. The average data for each continent and the world will be shaved off during the data cleaning process because it will not be used in this analysis.\nRenewable electricity generation, World: The data provides insights into the progression of renewable energy sources over time and facilitates comparative analysis across diverse geographical regions.\nShare of electricity generation from fossil fuels, renewables and nuclear: The dataset offers comprehensive insights into electricity generation, covering Fossil Fuels, Nuclear Power, and Renewable Sources since the 1960s.\n\n\n\n\nVariable table\n\n\nVariable_Names\nDescription\n\n\n\n\nEntity\nThe name of the country\n\n\nCode\nCountry Name Abbreviation\n\n\nYear\nThe year of the data\n\n\nRenewables%\nThe percentage of renewable energy to all energy used\n\n\ndifference\nThe renewables% changes between 1985 and 2021."
  },
  {
    "objectID": "posts/Proj_5_colb/index.html#data-collection",
    "href": "posts/Proj_5_colb/index.html#data-collection",
    "title": "Renewable Energy Transition over Decades Around the World",
    "section": "Data Collection",
    "text": "Data Collection\n\nLicenses: All visualizations, data, and articles produced by Our World in Data are open access under the Creative Commons BY license.\nSource of Datasets: ourworldindata.org\nThe data is sourced from the following avenues for their respective datasets:\n\n\nRenewable energy usage rates for multiple countries from 1965 to 2022: Energy Institute\nRenewable Energy Generation: Energy Institute - Statistical Review of World Energy (2023)\nShare of electricity generation from fossil fuels, renewables and nuclear: Ember - Yearly Electricity Data (2023); Ember - European Electricity Review (2022); Energy Institute - Statistical Review of World Energy (2023)\nLow-carbon energy consumption: Energy Institute’s Statistical Review of World Energy and GDP per Capita data from the World Bank’s World Development Indicators (WDI)"
  },
  {
    "objectID": "posts/Proj_5_colb/index.html#data-processing-and-analysis",
    "href": "posts/Proj_5_colb/index.html#data-processing-and-analysis",
    "title": "Renewable Energy Transition over Decades Around the World",
    "section": "Data Processing and Analysis",
    "text": "Data Processing and Analysis\n\nThe foremost step is the procedure of tidying the datasets. Wherein, it comprises of subjecting the datasets to the removal of missing values, redundant variables and furthermore, renaming the variables to make them more meaningful.\nThe datasets then undergo further processing tailored to extract specific insights. This involves employing mathematical functions, such as calculating averages, summarizing values over multiple years, filtering and extracting specific data required to understand latest trends across different geographical locations.These operations are conducted using the R programming language within the RStudio environment. Primarily, the library tidyverse Wickham et al. (2019) is utilized to facilitate these analytical procedures.\nFinally, the mathematical analyses conducted on the data are translated into visual representations, such as graphical forms, to facilitate thorough analysis and presentation of the findings. Primarily, the ggplot2 library Wickham (2016) is employed to transform the analytical results into appropriate graphical representations.\n\n\nEnergy Generation\n\n\n\n\n\n\n\n\nFigure 1: Evolution of Energy Generation from Fossil Fuels, Renewables, and Nuclear Sources\n\n\n\n\n\n\nNotably in Figure 1, Fossil fuels remain predominant for electricity generation, with a recent decline observed since 2015, despite a temporary decrease between 1990 and 2000. (primaryconsumption?)\nRenewable energy shows consistent growth, becoming the second most utilized energy source, particularly surging around 2000. Bhuiyan et al. (2022)\nNuclear energy, initially growing, experiences a decline since approximately 1998, attributed to safety concerns and regulatory uncertainty.\n\n\n\n\n\nTable 1: Countries Leading in Renewable Energy Production from 2018 to 2021\n\n\n\n\n\n\nEntity\nCode\nYear\nOther_Renewables\nSolar\nWind\nHydro\n\n\n\n\nChina\nCHN\n2021\n165.926\n327.0\n655.8\n1300.000\n\n\nChina\nCHN\n2020\n135.626\n261.1\n466.5\n1321.709\n\n\nChina\nCHN\n2019\n112.725\n224.0\n405.3\n1272.538\n\n\nChina\nCHN\n2018\n93.725\n176.9\n365.8\n1198.887\n\n\n\n\n\n\n\n\n\nThe generated dataframe offers a concise summary, as displayed in Table 1, presenting the leading countries in each year for renewable energy generation across various sources.\nChina consistently leads in recent years, excelling in generating the highest quantity of renewable energy across diverse resources such as wind, solar, hydro, and others.\n\n\n\nGrowth Rate of the Proportion of Renewable Energy in Total Energy Usage\n\n\n\n\n\n\n\n\nFigure 2: Top 5 countries renewable energy used increasing dunring the period\n\n\n\n\n\nIdentification of Top Five Countries: - Through data analysis, the top five countries with the highest growth rates in renewable energy usage between 1985 and 2021 were identified.\n\nThe following Table 2 presents the filtered data.\n\n\n\n\n\nTable 2: Data of Renewable Energy Usage Rates for Top Five Countries\n\n\n\n\n\n\n\nEntity\ndifference\nCode\nYear\nRenewables (% equivalent primary energy)\n\n\n\n\nCroatia\n18.21315\nHRV\n1991\n19.53725\n\n\nCroatia\n18.21315\nHRV\n1992\n15.99459\n\n\nCroatia\n18.21315\nHRV\n1995\n19.16081\n\n\nCroatia\n18.21315\nHRV\n1996\n24.70118\n\n\nCroatia\n18.21315\nHRV\n1993\n16.08737\n\n\n\n\n\n\n\n\n\n\n\nRenewable Souces as Opposed to Non-Rewnewable Sources\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Between Renewable and Non Renewable Energy Use versus Countries’ GDP\n\n\n\n\n\n\n\n\nFigure 3: Distribution of Low Carbon Energy Usage and GDP per Capita\n\n\n\n\n\n\nIn Figure 3, using dplyr library Wickham et al. (2023) we can seen that most of countries with low GDP per Capita tend to have also minimal share of low carbon energy usage and also the opposite. There are only 6 countries that have more than 40% of low carbon energy. The distribution of countries are concentrated in 0-25% of low carbon energy usage of total and 0-50000 dollars of GDP per Capita."
  },
  {
    "objectID": "posts/hddagroup/index.html",
    "href": "posts/hddagroup/index.html",
    "title": "Exploring Finanace Data using PCA and Factor Analysis",
    "section": "",
    "text": "Data Description\nThe SampleJ dataset contains monthly stock returns for around 100 NYSE firms from 2005–2019, forming a balanced panel. The Market dataset, provides a time series of S&P 500 returns over the same period. Both datasets are tidy and complete. Principal Component and Factor Analysis are applied to study common factors and industry-level exposure to systematic risk.\nSummary Statistics\n\n\n\n\nTable 1: Summary Statistics for Market and Industry Returns\n\n\n\n\n\n\nIndustry\nMean\nSD\nMin\nMax\n\n\n\n\nMarket Return\n0.7152\n4.114\n-17.23\n11.35\n\n\nOverall Stock Return\n0.0083\n0.117\n-0.88\n3.58\n\n\nConstruction\n0.0074\n0.166\n-0.53\n1.50\n\n\nFinance_RealEstate\n0.0069\n0.082\n-0.83\n2.45\n\n\nManufacturing\n0.0101\n0.155\n-0.86\n3.58\n\n\nMining\n0.0083\n0.110\n-0.32\n0.54\n\n\nRetail\n0.0028\n0.162\n-0.45\n1.45\n\n\nServices\n0.0108\n0.153\n-0.88\n1.95\n\n\nTransport_Utilities\n0.0111\n0.122\n-0.73\n1.06\n\n\nWholesale\n0.0053\n0.102\n-0.57\n0.55\n\n\n\n\n\n\n\n\nTable 1 highlight a clear contrast between market-wide and industry returns. The S&P 500 averages 0.72% per month with higher volatility (SD = 4.11%), reflecting major shocks such as the 2008 crisis and subsequent recovery.\nIn contrast, SampleJ stocks average near zero (0.008%) with lower volatility (0.12%), showing how diversification reduces individual fluctuations. Volatility differs across sectors, Transport & Utilities, Manufacturing, and Services show the largest swings, while Retail and Construction are more cyclical. Finance/Real Estate and Wholesale remain comparatively stable but still decline during market downturns.\nVisual Analysis\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nFigure 1 shows that market returns are far more dispersed, with a few extreme highs and lows. In contrast, stock returns are tightly clustered around zero but contain many more outliers, indicating that while overall volatility is low, individual stocks can still experience sharp positive or negative movements.\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\nThe boxplots in Figure 2 show that while most industries have medians near zero, their spreads and outliers differ considerably. Mining, Construction, and Retail display the widest IQRs and most extremes, making them highly volatile. Services and Manufacturing earn higher average returns but still show broad variation, reflecting cyclical risk. Finance/Real Estate and Wholesale appear more stable with tighter ranges, while Transport & Utilities lies in between, showing moderate volatility with occasional large movements.\n\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nFigure 3 compares the average yearly S&P 500 returns with the yearly averages from the SampleJ stocks. The market clearly dipped in 2008 and bounced back in 2009, showing the effect of the global financial crisis. Stock returns stayed closer to zero but still moved in the same direction, meaning that even though individual stocks were less volatile, they were still influenced by the wider market. These fluctuations also align with the spike in outliers, showing how market shocks can push returns far from normal levels.\n\nOutlier Analysis\nStock level Outlier Analysis:\nEach firm’s return history was checked using the IQR rule, which identified 796 outliers from 2005–2019. Even after removing the 2008–2009 crisis years, 566 remained. This shows that while the global financial crisis created many outliers through systematic shocks, others arose from ongoing idiosyncratic changes within specific firms and industries.\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nFigure 4 shows the largest spike in 2008–2009, mainly in Finance & Real Estate, with noticeable increases in Transport & Utilities, Services, and Manufacturing. Outlier counts drop after the crisis but don’t disappear, with smaller rises in 2011 and 2018–2019. This pattern highlights how both systematic and idiosyncratic risks continue to influence returns even in calmer markets.\nThese outliers reflect real variation, not errors. Keeping them preserves important information on how industries co-move and how risk spreads under stress. They were retained but scaled through standardisation to reduce their influence on the results.\nPCA on Sample Data\nPCA was applied to the standardised stock return data to uncover the main factors driving variation across industries.\nFrom the PCA summary, PC1 explains about 30% of total variation and PC2 adds just over 7%, together accounting for around 37%. Later components each explain less than 4%, and over 19 are needed to reach 70%. This shows that variation in stock returns is spread across many dimensions, though PC1 still captures the largest share.\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nFigure 5 shows the variance explained by the first 10 principal components. Based on the elbow rule, we retain up to PC2, which together explain about 40% of total variation. The Kaiser rule suggests keeping more components, but this is less practical.\nEvaluating Loadings\n\n\n\n\nTable 2: PCA Loadings for PC1 and PC2 with Industries\n\n\n\n\n\n\nStock\nPC1\nPC2\nIndustry\n\n\n\n\nH75274\n0.11\n-0.13\nFinance_RealEstate\n\n\nB83188\n0.11\n-0.02\nMining\n\n\nE88953\n0.14\n0.00\nTransport_Utilities\n\n\nI86594\n0.11\n0.12\nServices\n\n\nH81564\n0.09\n0.13\nFinance_RealEstate\n\n\nE77259\n0.03\n-0.05\nTransport_Utilities\n\n\n\n\n\n\n\n\nFrom Table 2, the loadings indicate how much each stock contributes to each component.\nPC1 – Overall Market Movement All industries load positively on PC1, showing that most stocks move in the same overall direction. This reflects broad market behaviour. Finance & Real Estate, Construction, and Transport & Utilities have the strongest loadings, meaning they’re most sensitive to market wide shocks. In contrast, Services and Mining load weaker likely more influenced by sector specific factors.\nPC2 – Industry Contrasts PC2 shows both positive and negative loadings, some industries rise when others fall. It separates sectors with opposing behaviour: Retail, Services, and Transport & Utilities load positively, while Finance and Mining lean negative, indicating differences between consumer-driven and financial/resource-based industries.\nPC1 VS Market\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nFigure 6 compares the PCA scores for PC1 with S&P 500 market returns over the same period. The two series move closely together, with peaks and dips almost perfectly aligned. While there are small differences in magnitude, this strong relationship shows that PC1 captures systematic market movements and can be seen as the main market factor.\nAlthough PC1 explains most of the variation, later components such as PC2 represent idiosyncratic or sector-specific effects that move differently from the market.\nBiplot\n\n\n\n\n\n\n\n\nFigure 7\n\n\n\n\n\nFigure 7 shows how industries contribute to overall variation in stock returns, with arrows representing the top five stocks per industry and their strength of relation to PC1 and PC2.\nFinance & Real Estate and Transport & Utilities have the longest arrows, explaining much of the overall movement in returns and linking strongly with PC1 reflecting systematic market behaviour. PC2 contrasts consumer sectors (Retail, Services) with financial and resource-based ones (Finance, Mining).\n\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\nFigure 8 shows the 15 stocks that contribute most to the first two components. Finance & Real Estate stand out, with several firms loading strongly on both PC1 and PC2. Most finance stocks load positively on PC1, moving with the market, while some load negatively on PC2, showing variation within the sector.\nOverall, Finance & Real Estate are the most influential, followed by Construction and Transport & Utilities.\nLimitations\nPCA assumes linear relationships and uncorrelated components, which may simplify real market behaviour. Therefore, interpretation depends mainly on economic reasoning, although it remains a useful starting point before applying Factor Analysis.\nFactor Analysis\nTo better understand how industries are connected, the analysis extends to Factor Analysis (FA). FA identifies a smaller set of underlying factors that explain how stocks move together and shows the economic or sector forces driving those relationships.\n\n\n\n\nTable 3: FA model comparison (chi-square test p-values)\n\n\n\n\n\n\n\nk\nrotation\np_value\n\n\n\n\nobjective...1\n2\nvarimax\n0\n\n\nobjective...2\n3\nvarimax\n0\n\n\nobjective...3\n2\npromax\n0\n\n\nobjective...4\n3\npromax\n0\n\n\n\n\n\n\n\n\nFrom Table 3, the chi-square test rejected the null hypothesis for all models, with p-values near zero. Given the dataset’s high dimensionality, this is expected. Instead, the PCA scree plot (elbow after the second component) and the interpretability of rotated loadings guided the choice of a three-factor model. Adding more factors offered little new economic insight, while three factors captured clear distinctions between industries (e.g., Mining on Factor 2 and Construction on Factor 1), making it the most informative representation of systematic variation.\nIndustry-level loading summaries (signed and absolute), by factor\n\n\n\n\nTable 4: Industry loadings summary — 3 factors (varimax rotation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean Loadings\n\n\nAbsolute Mean Loadings\n\n\n\nIndustry\nF1\nF2\nF3\nF1_ab\nF2_ab\nF3_ab\n\n\n\n\nConstruction\n0.43\n0.29\n0.154\n0.43\n0.29\n0.173\n\n\nFinance_RealEstate\n0.37\n0.36\n0.261\n0.40\n0.36\n0.287\n\n\nManufacturing\n0.34\n0.26\n0.049\n0.34\n0.26\n0.100\n\n\nMining\n0.17\n0.42\n0.078\n0.27\n0.42\n0.078\n\n\nRetail\n0.40\n0.13\n0.047\n0.40\n0.13\n0.059\n\n\nServices\n0.29\n0.20\n0.090\n0.29\n0.20\n0.110\n\n\nTransport_Utilities\n0.42\n0.29\n0.102\n0.42\n0.29\n0.114\n\n\nWholesale\n0.33\n0.33\n0.034\n0.33\n0.33\n0.071\n\n\n\n\n\n\n\n\n\nGreatest degree of loading on the first latent factor\n\nConstruction shows the highest average loading on Factor 1 (mean = 0.43, absmean ≈ 0.43), according to the summary tables and heatmaps. This suggests that the first common factor drives returns in the construction sector the most. The largest loading shifts toward Mining (absmean = 0.42) for Factor 2, indicating that this factor captures variation unique to resource-based industries. Finance & Real Estate and Wholesale have the highest loadings on Factor 3, showing that once past the first factor, the main industry drivers change and relationships grow more complex\nIdentify “largest loading” industry for Factor 1 (and others)\n\n\n\n\nTable 5: Top industries by loading across factors\n\n\n\n\n\n\nFactor\nDescription\nIndustry\nMean_loading\n\n\n\n\nFactor 1\nLargest degree of loading (by abs(mean))\nConstruction\n0.434\n\n\nFactor 1\nStrongest positive mover (same direction)\nConstruction\n0.434\n\n\nFactor 1\nStrongest negative mover (opposite direction)\nMining\n0.168\n\n\nFactor 2\nLargest degree of loading (by abs(mean))\nMining\n0.416\n\n\nFactor 2\nStrongest positive mover (same direction)\nMining\n0.416\n\n\nFactor 2\nStrongest negative mover (opposite direction)\nRetail\n0.131\n\n\nFactor 3\nLargest degree of loading (by abs(mean))\nFinance_RealEstate\n0.261\n\n\nFactor 3\nStrongest positive mover (same direction)\nFinance_RealEstate\n0.261\n\n\nFactor 3\nStrongest negative mover (opposite direction)\nWholesale\n0.034\n\n\n\n\n\n\n\n\nBased on Table 5:\n\nIndustries moving in opposite vs same directions\n\nThe signed means reveal each industry’s direction. Most sectors load positively on Factor 1, following the general direction of this systematic component. This is consistent with Factor 1 being a general “market” force. However, some industries show weaker or opposite tendencies. Retail displays weaker or negative alignment with certain factors, suggesting shocks in these industries may move against the common market component. Mining, has a low positive mean on Factor 1 (0.17) but loads more heavily on Factor 2. This sign variation confirms that industries react differently to systematic influences.\n\n\n\n\n\n\n\n\nFigure 9\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10\n\n\n\n\n\n\nHeterogeneity in factor loadings\n\nThe unequal loading distribution across industries and factors is clear from the heatmaps Figure 9 and Figure 10. The consistently high exposures of Construction, Finance & Real Estate, and Transport & Utilities suggest these sectors are more systematically driven. Manufacturing and Services show more idiosyncratic behaviour with lower mean loadings. In line with commodity-related shocks, Mining shifts from Factor 1 to Factor 2. This heterogeneity highlights opportunities for diversification by holding industries with low or opposite loadings to dominant factors, reflecting that sectors face different underlying risks.\nSystematic Risk Across Industries\nSystematic risk reflects broad market forces that cannot be diversified away. From the factor analysis, Construction (0.43), Transport & Utilities (0.42), and Retail (0.40) show the strongest exposures to Factor 1, the broad market factor. These industries are most affected by market-wide shocks, with Construction standing out due to its cyclical nature and sensitivity to downturns. In contrast, Mining and Services have lower loadings, showing more idiosyncratic, industry-specific volatility.\nThis pattern aligns with the PCA results, where PC1 moved almost closely with the market index during the 2008–2009 crisis. Overall, Construction and Transport & Utilities carry the highest systematic risk, while Services and Manufacturing are more influenced by firm- or sector-specific variation.\nInvestor Recommendation: Five Stocks That Move With the Market\nFor an investor seeking exposure to stocks that move with the market, the best choices are those that load strongly and positively on Factor 1. Based on the industry summary tables and heatmaps, Construction, Transport & Utilities, Retail, and Finance & Real Estate show the highest systematic exposures. Within these industries, investors should focus on the stocks with the largest positive loadings.\nOur reasonable recommendations are:\nC54148 (Construction) – consistently high loading on Factor 1.\nE77259 (Transport & Utilities) – strong systematic exposure.\nG77530 (Retail) – among the retail stocks, this one shows high positive co-movement.\nH75274 (Finance & Real Estate) – finance stocks were especially sensitive during crisis periods, reflecting market-wide shocks.\nD75261 (Manufacturing) – while Manufacturing is less dominant than Construction, this stock still shows systematic sensitivity and provides sectoral diversification.\nTogether, these five stocks cover different industries but all show strong co-movement with the market factor. This gives the investor exposure to systematic risk, the part of return variation that cannot be eliminated by diversification, while still spreading the investment across multiple industries.\nAssumptions\nThe factor model assumes that unique errors are normally distributed, common factors are orthogonal, and asset returns can be expressed as linear combinations of idiosyncratic errors and common factors. These assumptions are approximations, since stock returns often deviate from normality, showing heavy tails, skewness, and volatility clustering. Despite this, the model remains useful for identifying systematic risk and explaining industry-level co-movements.\nLimitations\nThe chi-square goodness-of-fit test consistently rejected all specifications, highlighting the difficulty of applying strict statistical tests in large financial panels. Consequently, interpretation relies more on comparative interpretability and economic reasoning than on hypothesis testing. Factor structures may also evolve over time, for example, the 2008–2009 financial crisis likely changed the relative importance of some industries, meaning factors from 2005–2019 may not hold in later periods. Finally, the findings are limited to this dataset and timeframe and may not generalise to other markets.\nReferences\n\nLectures and course materials from ETF5500 - BEX5500\nChatGPT-4 - We used it cautiously to help with code errors and enhancing visualisations."
  },
  {
    "objectID": "posts/scm2/index.html",
    "href": "posts/scm2/index.html",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "",
    "text": "Patagonia utilizes a form of the chase strategy to manage its production and inventory efficiently, aligning closely with its sustainability goals and ensuring that it meets customer demand without overproducing.\n\n\nPatagonia employs Just-in-Time (JIT) inventory management as part of its chase strategy to reduce inventory costs by producing and delivering products only as needed. In their production of Nano Puff jackets, where sales data and real-time demand align production schedules to match seasonal requirements. This approach is crucial during winter when demand fluctuates due to weather and market trends. JIT minimizes excess inventory, reducing waste and environmental impact, while allowing Patagonia to quickly adapt to customer demand changes, thereby avoiding stock-outs and overstock situations and maintaining operational efficiency and customer satisfaction.\n\n\n\nPatagonia’s supply chain is highly responsive, allowing quick adjustments to production based on real-time demand data through close collaboration with suppliers and manufacturers. Patagonia collaborates with suppliers like Tin Shed Ventures, its venture capital fund that invests in sustainable start-ups, ensuring access to innovative and flexible manufacturing processes. During peak periods, production is ramped-up to meet demand, while it is scaled back during off-peak times. This approach effectively manages inventory, reduces carrying costs, and aligns with Patagonia’s sustainability goals by preventing overproduction and waste.\nPatagonia’s Worn Wear Program exemplifies its chase strategy by aligning production with real-time demand and promoting sustainability. The program encourages customers to return used gear, which is then repaired, refurbished, and resold, extending product lifecycle. By tracking returned items and demand for Worn Wear products, Patagonia efficiently allocates resources, such as increasing jacket repairs before fall. This strategy enhances cost efficiency by avoiding excess inventory costs and minimizing raw material use. The Worn Wear Program provides affordable options and effectively manages inventory.\n\n\n\n\nPatagonia’s Utilization Flexibility Strategy employs adaptable manufacturing processes and labor practices to meet fluctuating demand and sustainability objectives. This approach enables Patagonia to quickly respond to changes, optimize resource use, and uphold high standards.\n\n\nPatagonia invests significantly in training its workforce to be multi-skilled and versatile, enabling employees to perform various tasks and roles as required. In Patagonia’s repair centers, technicians undergo training to handle a wide range of repair tasks, including mending zippers and patching holes in different gear types. During peak seasons, this multi-skilled workforce ensures quick turnaround times by managing a wide variety of product repairs. While enhancing flexibility, allows Patagonia to reallocate labor resources as per current needs. Consequently, downtime is reduced, productivity is increased, as employees can seamlessly transition between tasks without requiring specialized training each time.\n\n\n\nPatagonia collaborates closely with supply chain partners capable of swiftly adapting to changing requirements to ensure flexibility and responsiveness. Its partnership with Yulex, a provider of plant-based rubber, enables seamless transition to more sustainable materials. When Patagonia opted to substitute neoprene with Yulex in their wetsuits, supply chain partners adjusted their processes effortlessly to accommodate this change. This adaptability enables integration of new materials and technologies efficiently while minimizing disruptions stemming from material availability fluctuations or shifts in market trends.\nDuring the COVID-19 pandemic, Patagonia effectively utilized its flexibility strategy to handle demand shifts and supply chain disruptions. After retail closures, it redeployed staff to assist with the surge in online orders and prioritized products with available materials, focusing on products like lightweight travel gear. The company transitioned to remote work, implementing systems for collaboration and flexible hours across design, marketing, and administrative functions, served customers, and upheld sustainability by focusing on less resource-intensive products. This approach ensured job security and safety for employees, fostering morale and productivity amidst adversity."
  },
  {
    "objectID": "posts/scm2/index.html#chase-strategy",
    "href": "posts/scm2/index.html#chase-strategy",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "",
    "text": "Patagonia utilizes a form of the chase strategy to manage its production and inventory efficiently, aligning closely with its sustainability goals and ensuring that it meets customer demand without overproducing.\n\n\nPatagonia employs Just-in-Time (JIT) inventory management as part of its chase strategy to reduce inventory costs by producing and delivering products only as needed. In their production of Nano Puff jackets, where sales data and real-time demand align production schedules to match seasonal requirements. This approach is crucial during winter when demand fluctuates due to weather and market trends. JIT minimizes excess inventory, reducing waste and environmental impact, while allowing Patagonia to quickly adapt to customer demand changes, thereby avoiding stock-outs and overstock situations and maintaining operational efficiency and customer satisfaction.\n\n\n\nPatagonia’s supply chain is highly responsive, allowing quick adjustments to production based on real-time demand data through close collaboration with suppliers and manufacturers. Patagonia collaborates with suppliers like Tin Shed Ventures, its venture capital fund that invests in sustainable start-ups, ensuring access to innovative and flexible manufacturing processes. During peak periods, production is ramped-up to meet demand, while it is scaled back during off-peak times. This approach effectively manages inventory, reduces carrying costs, and aligns with Patagonia’s sustainability goals by preventing overproduction and waste.\nPatagonia’s Worn Wear Program exemplifies its chase strategy by aligning production with real-time demand and promoting sustainability. The program encourages customers to return used gear, which is then repaired, refurbished, and resold, extending product lifecycle. By tracking returned items and demand for Worn Wear products, Patagonia efficiently allocates resources, such as increasing jacket repairs before fall. This strategy enhances cost efficiency by avoiding excess inventory costs and minimizing raw material use. The Worn Wear Program provides affordable options and effectively manages inventory."
  },
  {
    "objectID": "posts/scm2/index.html#utilization-flexibility-strategy",
    "href": "posts/scm2/index.html#utilization-flexibility-strategy",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "",
    "text": "Patagonia’s Utilization Flexibility Strategy employs adaptable manufacturing processes and labor practices to meet fluctuating demand and sustainability objectives. This approach enables Patagonia to quickly respond to changes, optimize resource use, and uphold high standards.\n\n\nPatagonia invests significantly in training its workforce to be multi-skilled and versatile, enabling employees to perform various tasks and roles as required. In Patagonia’s repair centers, technicians undergo training to handle a wide range of repair tasks, including mending zippers and patching holes in different gear types. During peak seasons, this multi-skilled workforce ensures quick turnaround times by managing a wide variety of product repairs. While enhancing flexibility, allows Patagonia to reallocate labor resources as per current needs. Consequently, downtime is reduced, productivity is increased, as employees can seamlessly transition between tasks without requiring specialized training each time.\n\n\n\nPatagonia collaborates closely with supply chain partners capable of swiftly adapting to changing requirements to ensure flexibility and responsiveness. Its partnership with Yulex, a provider of plant-based rubber, enables seamless transition to more sustainable materials. When Patagonia opted to substitute neoprene with Yulex in their wetsuits, supply chain partners adjusted their processes effortlessly to accommodate this change. This adaptability enables integration of new materials and technologies efficiently while minimizing disruptions stemming from material availability fluctuations or shifts in market trends.\nDuring the COVID-19 pandemic, Patagonia effectively utilized its flexibility strategy to handle demand shifts and supply chain disruptions. After retail closures, it redeployed staff to assist with the surge in online orders and prioritized products with available materials, focusing on products like lightweight travel gear. The company transitioned to remote work, implementing systems for collaboration and flexible hours across design, marketing, and administrative functions, served customers, and upheld sustainability by focusing on less resource-intensive products. This approach ensured job security and safety for employees, fostering morale and productivity amidst adversity."
  },
  {
    "objectID": "posts/scm2/index.html#enterprise-resource-planning-erp-system",
    "href": "posts/scm2/index.html#enterprise-resource-planning-erp-system",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Enterprise Resource Planning (ERP) System",
    "text": "Enterprise Resource Planning (ERP) System\nPatagonia utilizes SAP ERP (Enterprise Resource Planning) for streamlined inventory management, integrating data from various warehouses and retail locations to optimize stock levels and reduce excess inventory. This real-time visibility informs decisions on restocking and redistribution based on current demand and sales data. The ERP system manages supplier relationships, ensuring timely procurement of raw materials like organic cotton and recycled polyester. Automated purchase orders and supplier performance tracking maintains a consistent flow of materials without delays. This enhances operational efficiency, reducing lead times and minimizes stock-outs."
  },
  {
    "objectID": "posts/scm2/index.html#advanced-forecasting-and-demand-planning-using-salesforce-commerce-cloud",
    "href": "posts/scm2/index.html#advanced-forecasting-and-demand-planning-using-salesforce-commerce-cloud",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Advanced Forecasting and Demand Planning using Salesforce Commerce Cloud",
    "text": "Advanced Forecasting and Demand Planning using Salesforce Commerce Cloud\nPatagonia utilizes Salesforce Commerce Cloud, a cloud-based platform, to predict customer demand accurately and adjust production schedules accordingly, through advanced analytics and forecasting tools. By analyzing historical sales data, market trends, and seasonal patterns, Salesforce Commerce Cloud enables Patagonia to forecast product demands effectively, such as anticipating higher sales for winter gear like insulated jackets and base layers during fall and winter. This allows Patagonia to adapt production schedules, such as increasing production for specific product lines like the R1 TechFace Hoody, to meet projected demand. Advanced forecasting minimizes overproduction and stock-outs, enhancing inventory management efficiency. Enabling Patagonia to respond promptly to market changes, improving customer satisfaction and reducing waste in the supply chain."
  },
  {
    "objectID": "posts/scm2/index.html#risk-management-and-contingency-planning",
    "href": "posts/scm2/index.html#risk-management-and-contingency-planning",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Risk Management and Contingency Planning",
    "text": "Risk Management and Contingency Planning\nPatagonia effectively uses information to identify potential supply chain risks and develop contingency plans. In response to the COVID-19 pandemic, the company carried out risk assessment leveraging real-time sales data and supplier performance metrics. Recognizing the risk of production delays and distribution challenges, Patagonia implemented contingency planning by diversifying its supplier base, partnering with companies like Polartec along with alternative suppliers like Malden Mills and Toray Industries for fleece products and Yulex for plant-based rubber wetsuits, and adjusted production schedules based on supplier availability and capacity. Prioritizing production of versatile products like the Torrentshell 3L Jacket, which can be produced with materials from multiple suppliers. The company also communicated transparently with customers about potential delays in products such as the Down Sweater and Torrentshell 3L Jacket, offering flexible return policies to maintain customer satisfaction."
  },
  {
    "objectID": "posts/scm2/index.html#customer-feedback-and-product-innovation",
    "href": "posts/scm2/index.html#customer-feedback-and-product-innovation",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Customer Feedback and Product Innovation",
    "text": "Customer Feedback and Product Innovation\nPatagonia collects and analyzes customer feedback to inform product development and innovation initiatives, ensuring that its offerings align with customer preferences and market trends. Through its Worn Wear Program, it collects insights on product durability, performance, and design preferences from customers returning worn gear for repair or recycling. When multiple customers reported issues with the Torrentshell 3L Jacket, citing concerns regarding zipper durability and seam integrity, in response, Patagonia collaborated with suppliers like YKK for more durable zippers and Gore-Tex for enhanced seam sealing techniques to enhance these aspects. By integrating specific feedback from customers into its product development process, Patagonia ensures that its offerings are finely tuned to meet the evolving needs and expectations of its customer base."
  },
  {
    "objectID": "posts/scm2/index.html#environmental-disruptions-in-raw-material-supply-chains",
    "href": "posts/scm2/index.html#environmental-disruptions-in-raw-material-supply-chains",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Environmental Disruptions in Raw Material Supply Chains",
    "text": "Environmental Disruptions in Raw Material Supply Chains\nEnvironmental disruptions, such as extreme weather events and changing climate patterns, pose significant challenges to Patagonia’s supply chain for key raw materials. For instance, Patagonia sources Merino wool from suppliers like ZQ Merino in New Zealand, a region that experiences fluctuations in weather patterns, including droughts and heavy rainfall. These conditions have adversely affected sheep farming and the availability of high-quality Merino wool, creating uncertainties in the supply chain and impacting production schedules for popular products like the Merino Air Base Layer. The primary impact of these disruptions is delays in the availability of raw materials leading to production delays, hindering Patagonia’s ability to meet customer demand."
  },
  {
    "objectID": "posts/scm2/index.html#labor-rights-violations-in-manufacturing-facilities",
    "href": "posts/scm2/index.html#labor-rights-violations-in-manufacturing-facilities",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Labor Rights Violations in Manufacturing Facilities",
    "text": "Labor Rights Violations in Manufacturing Facilities\nLabor rights violations in manufacturing facilities present significant ethical and reputation risks to Patagonia’s supply chain, impacting coordination efficiency and brand integrity. Patagonia produces its outdoor apparel in factories worldwide, including in Bangladesh and Vietnam. Recent reports have highlighted issues such as unsafe working conditions and worker exploitation. For instance, the Rana Plaza supplier factory in Bangladesh forced workers to endure long hours in unsafe conditions to meet production targets for items like the Down Sweater. Similarly, Triumph International in Vietnam faced allegations of child labor and wage exploitation, impacting the production of the Torrentshell 3L Jacket. These violations damage Patagonia’s reputation as a socially responsible brand, undermining consumer trust and brand loyalty."
  },
  {
    "objectID": "posts/scm2/index.html#environmental-disruptions-in-raw-material-supply-chains-1",
    "href": "posts/scm2/index.html#environmental-disruptions-in-raw-material-supply-chains-1",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Environmental Disruptions in Raw Material Supply Chains",
    "text": "Environmental Disruptions in Raw Material Supply Chains\n\nStrategy 1: Diversification of Supply Sources\nPatagonia mitigates the risk of environmental disruptions by diversifying its sources of raw materials. By working with multiple suppliers across different regions, Patagonia reduces its reliance on a single supplier or location, thereby minimizing the impact of localized environmental challenges. In addition to sourcing Merino wool from suppliers in New Zealand like ZQ Merino, Patagonia collaborates with suppliers such as Woolmark in Australia and Ovis 21 in Argentina, South America. This diversification strategy ensures continuity of supply and resilience against environmental risks specific to any one region.\n\n\nStrategy 2: Adoption of Sustainable Farming Practices\nPatagonia partners with suppliers such as ZQ Merino to promote sustainable farming practices that enhance resilience to environmental disruptions. By investing in regenerative agriculture, water conservation, and biodiversity preservation, Patagonia and its suppliers mitigate the impact of climate change on raw material production. Patagonia supports initiatives like the Responsible Wool Standard (RWS), ensuring ethical and sustainable practices in wool production. By sourcing RWS-certified wool, Patagonia contributes to ecosystem preservation and supports communities reliant on wool farming for their livelihoods."
  },
  {
    "objectID": "posts/scm2/index.html#labor-rights-violations-in-manufacturing-facilities-1",
    "href": "posts/scm2/index.html#labor-rights-violations-in-manufacturing-facilities-1",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Labor Rights Violations in Manufacturing Facilities",
    "text": "Labor Rights Violations in Manufacturing Facilities\n\nStrategy 1: Supplier Engagement and Capacity Building\nPatagonia engages with its manufacturing partners in countries like Bangladesh and Vietnam through training programs, workshops, and capacity-building initiatives, Patagonia empowers suppliers to uphold ethical labor practices and create safe working environments. For instance, Patagonia collaborates with organizations such as the Fair Labor Association (FLA) to conduct supplier assessments and provide training on labor rights, health, and safety standards. By investing in supplier development, Patagonia strengthens relationships with its manufacturing partners and fosters a culture of ethical manufacturing.\n\n\nStrategy 2: Transparency and Accountability\nPatagonia prioritizes transparency and accountability in its supply chain by conducting regular audits and inspections of manufacturing facilities. It publicly discloses audit findings and corrective actions, holding itself and its suppliers accountable for any violations and committing to continuous improvement. Following the tragic incident of 2013 Rana Plaza collapse in Bangladesh, where over 1,100 garment workers lost their lives due to unsafe working conditions. Patagonia took decisive measures to address them, including terminating contracts with suppliers that failed to meet safety requirements and implementing corrective actions to improve workplace conditions. The company also pledged financial support to affected workers and their families."
  },
  {
    "objectID": "posts/scm2/index.html#implementation-of-3d-printing-technology-for-customized-products",
    "href": "posts/scm2/index.html#implementation-of-3d-printing-technology-for-customized-products",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Implementation of 3D Printing Technology for Customized Products",
    "text": "Implementation of 3D Printing Technology for Customized Products\nLeading companies use 3D printing to customize products and reduce manufacturing lead times. Patagonia could integrate 3D printing into its supply chain to offer personalized items like custom-fit outerwear and accessories. By strategically investing in 3D printing, Patagonia can decentralize manufacturing, shorten lead times, and reduce excess inventory, aligning with its sustainability goals. This technology also enables rapid design iteration and prototyping, allowing Patagonia to innovate and respond swiftly to evolving customer preferences and market trends. Overall, 3D printing would enhance customer satisfaction and reduce the environmental impact of traditional manufacturing."
  },
  {
    "objectID": "posts/scm2/index.html#development-of-supply-chain-resilience-centers",
    "href": "posts/scm2/index.html#development-of-supply-chain-resilience-centers",
    "title": "Patagonia - Part 2 SCM Practices and Strategies",
    "section": "Development of Supply Chain Resilience Centers",
    "text": "Development of Supply Chain Resilience Centers\nLeading companies have established supply chain resilience centers to manage risks like natural disasters, geopolitical instability, and global pandemics. Patagonia can follow suit by setting up similar centers in strategic locations to monitor and address potential disruptions. These centers would function as command hubs, equipped with real-time data analytics, contingency plans, and crisis management protocols, would ensure operational continuity during emergencies. Centralizing resources and expertise would enable Patagonia to proactively manage supply chain risks, enhancing business continuity, stakeholder trust, and long-term sustainability."
  },
  {
    "objectID": "posts/ml1/index.html",
    "href": "posts/ml1/index.html",
    "title": "Introduction - Exploring Machine Learning",
    "section": "",
    "text": "Confusion Matrix\n\n\nCode\n# Reading Data\nd_pred &lt;- read_csv(\"pred_data.csv\")\nhead(d_pred)\n\n\n# A tibble: 6 × 7\n  y     pred1 pred2 bilby1 bilby2 quokka1 quokka2\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 bilby bilby bilby   0.8    0.8     0.2     0.2 \n2 bilby bilby bilby   0.9    0.51    0.1     0.49\n3 bilby bilby bilby   0.9    0.6     0.1     0.4 \n4 bilby bilby bilby   0.7    0.6     0.3     0.4 \n5 bilby bilby bilby   0.8    0.8     0.2     0.2 \n6 bilby bilby bilby   0.51   0.8     0.49    0.2 \n\n\nCode\n# Setting positive class as Bilby\nd_predb &lt;- d_pred |&gt; \n  select(y, pred1, pred2) |&gt; \n  mutate(\n  y = factor(y, levels = c(\"bilby\", \"quokka\")),\n  pred1 = factor(pred1, levels = c(\"bilby\", \"quokka\")),\n  pred2 = factor(pred2, levels = c(\"bilby\", \"quokka\"))\n)\n\n\n\n# Confusion matrix in standard form - model 1\ncm &lt;- d_predb |&gt; count(y, pred1) |&gt;\n  group_by(y) |&gt;\n  mutate(cl_acc = n[pred1==y]/sum(n)) \ncm |&gt;\n  pivot_wider(names_from = pred1, \n              values_from = n) \n\n\n# A tibble: 2 × 4\n# Groups:   y [2]\n  y      cl_acc bilby quokka\n  &lt;fct&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 bilby    0.75    18      6\n2 quokka   0.8      6     24\n\n\nCode\n# Confusion matrix in standard form - model 2\ncm2 &lt;- d_predb |&gt; count(y, pred2) |&gt;\n  group_by(y) |&gt;\n  mutate(cl_acc = n[pred2==y]/sum(n)) \ncm2 |&gt;\n  pivot_wider(names_from = pred2, \n              values_from = n) \n\n\n# A tibble: 2 × 4\n# Groups:   y [2]\n  y      cl_acc bilby quokka\n  &lt;fct&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;\n1 bilby   0.917    22      2\n2 quokka  0.633    11     19\n\n\nModel Metrics\n\n\nCode\n# Metrics - Model 1\n\n# Accuracy\nacc_pred1 &lt;- accuracy(d_predb, y, pred1) |&gt; pull(.estimate)\n\n# Balanced Accuracy\nbacc_pred1 &lt;- bal_accuracy(d_predb, y, pred1) |&gt; pull(.estimate)\n\n# True Positive Rate (TPR)\ntpr_pred1 &lt;- sens(d_predb, y, pred1) |&gt; pull(.estimate)\n\n# True Negative Rate (TNR)\ntnr_pred1 &lt;- specificity(d_predb, y, pred1) |&gt; pull(.estimate)\n\n# Metrics - Model 2\n\n# Accuracy\nacc_pred2 &lt;- accuracy(d_predb, y, pred2) |&gt; pull(.estimate)\n\n# Balanced Accuracy\nbacc_pred2 &lt;- bal_accuracy(d_predb, y, pred2) |&gt; pull(.estimate)\n\n# True Positive Rate (TPR)\ntpr_pred2 &lt;- sens(d_predb, y, pred2) |&gt; pull(.estimate)\n\n# True Negative Rate (TNR)\ntnr_pred2 &lt;- specificity(d_predb, y, pred2) |&gt; pull(.estimate)\n\n# Tibble with metrics for both models\nmetrics_table &lt;- tibble(\n  Metric = c(\"Accuracy\", \"Balanced Accuracy\", \"True Positive Rate (TPR)\", \"True Negative Rate (TNR)\"),\n  Model_1 = round(c(acc_pred1, bacc_pred1, tpr_pred1, tnr_pred1), 4),\n  Model_2 = round(c(acc_pred2, bacc_pred2, tpr_pred2, tnr_pred2), 4)\n)\n\n\nmetrics_table |&gt; \n  kable(caption = \"Model Metrics\", align = \"lcc\") |&gt; \n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n    full_width = F, \n    position = \"center\", \n    font_size = 14, \n    stripe_color = \"gray!20\" \n  )\n\n\n\nModel Metrics\n\n\nMetric\nModel_1\nModel_2\n\n\n\n\nAccuracy\n0.7778\n0.7593\n\n\nBalanced Accuracy\n0.7750\n0.7750\n\n\nTrue Positive Rate (TPR)\n0.7500\n0.9167\n\n\nTrue Negative Rate (TNR)\n0.8000\n0.6333\n\n\n\n\n\na. TRP and TNR\nModel 2 has a higher True Positive Rate (0.9167), making it better at detecting bilbies. Model 1 has a higher True Negative Rate (0.80), meaning it’s better at correctly identifying quokkas. This reflects a trade-off: Model 2 is more sensitive, while Model 1 is more specific.\nb. Accuracy and Balanced Accuracy\nFor both models, accuracy measures overall correctness, considering all predictions, whereas balanced accuracy focuses on individual class performance. It calculates the average of the True Positive Rate (TPR) and True Negative Rate (TNR), which gives equal importance to each class. This is especially useful in cases like ours, where the dataset is imbalanced—there are more quokkas (30) than bilbies (24).\nIn Model 1, the accuracy (77.78%) and balanced accuracy (77.5%) are nearly identical, with only a small difference of 0.28%. This suggests that the model performs evenly across both classes without a strong bias toward the majority class (quokkas). The small gap between the two metrics indicates that Model 1 handles the class imbalance well, making fair predictions for both classes.\nOn the other hand, Model 2 shows a lower accuracy (75.93%) but a higher balanced accuracy (77.5%), with a difference of 1.57%. This suggests that while Model 2’s overall performance is slightly weaker, it does a better job of predicting the minority class (bilbies). The larger difference between accuracy and balanced accuracy points to the model’s ability to focus more on fairness, ensuring better representation for both classes, even though overall accuracy is reduced.\nc. Threshold for Bilby and Quokka\nThe predicted classes in this dataset were determined using a threshold of 0.5. This means that if the probability for bilby was 0.5 or higher, the model predicted the class as bilby, and if the probability for quokka was greater than 0.5, it predicted quokka. While this threshold is a standard choice in binary classification problems, its effectiveness might depend on the balance of the classes and the specific objectives of the model.\nGiven that the dataset is imbalanced, with more quokkas than bilbies, this threshold may not be the best fit. To fine-tune the model, we could experiment with different thresholds, which might improve predictions, especially for the minority class (bilby). ROC curves can give us a clearer picture of how the model performs across different threshold values, helping to identify a better balance between sensitivity and specificity for both classes.\nd. ROC Curves\n\n\nCode\nd_pred$y &lt;- factor(d_pred$y, levels = c(\"bilby\", \"quokka\"))\n\nroc_curve(d_pred, y, bilby1) |&gt; \n  autoplot() +\n  ggtitle(\"ROC Curve for Model 1\")\n\n\n\n\n\n\n\n\n\nCode\nroc_curve(d_pred, y, bilby2) |&gt; \n  autoplot() +\n  ggtitle(\"ROC Curve for Model 2\")\n\n\n\n\n\n\n\n\n\nCode\nmd1 &lt;- roc_auc(d_pred, y, bilby1)\nmd2 &lt;- roc_auc(d_pred, y, bilby2)\n\n# Create a table with ROC AUC values for both models\n\nauc &lt;- tibble(\n  Model = c(\"Model 1\", \"Model 2\"),\n  ROC_AUC = round(c(md1$.estimate, md2$.estimate), 4)\n)\n\nauc |&gt; \n  kable(caption = \"ROC AUC\", align = \"lcc\") |&gt; \n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n    full_width = F, \n    position = \"center\", \n    font_size = 14, \n    stripe_color = \"gray!20\" \n  )\n\n\n\nROC AUC\n\n\nModel\nROC_AUC\n\n\n\n\nModel 1\n0.8354\n\n\nModel 2\n0.7597\n\n\n\n\n\nROC curves were generated for both models using predicted probabilities for the bilby class, treated as the positive class. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) across different thresholds, offering an overall view of model performance.\nModel 1 demonstrates stronger distinction between the classes, with a ROC curve that lies above that of Model 2 across most thresholds. This is confirmed by its higher AUC of 0.8354, indicating that Model 1 has an 83.5% chance of ranking a randomly chosen bilby higher than a randomly chosen quokka. In contrast, Model 2’s AUC is 0.7597, suggesting noticeably weaker separation.\nWhile neither curve shows the ideal shape of a near-vertical rise followed by a flat line along the top (indicating perfect classification), Model 1 is clearly more effective across a range of thresholds. Its curve achieves a better balance between sensitivity and specificity and maintains higher TPR at comparable FPRs. Model 2’s curve, closer to the diagonal line, shows more overlap between the two classes less certainty in distinguishing between them.\nModel 1 consistently outperforms Model 2, both visually and statistically, and is the stronger choice when evaluating overall classification performance based on ROC analysis."
  },
  {
    "objectID": "posts/ml1/index.html#references",
    "href": "posts/ml1/index.html#references",
    "title": "Introduction - Exploring Machine Learning",
    "section": "References",
    "text": "References\n1.Hadley Wickham, Dianne Cook, Heike Hofmann, Andreas Buja (2011). tourr: An R Package for Exploring Multivariate Data with Projections. Journal of Statistical Software, 40(2), 1-18. URL http://www.jstatsoft.org/v40/i02/.\n2.Lecture and tuorial notes and codes snippets from ETC3250/5250, 2025, Monash University\n3.Womens Cricket: https://en.wikipedia.org/wiki/Australia_women%27s_national_cricket_team\n4.OpenAI (2023). ChatGPT (version 3.5) [Large language model]."
  },
  {
    "objectID": "posts/Proj_4_wcd3/index.html",
    "href": "posts/Proj_4_wcd3/index.html",
    "title": "Exploring data methods to uncover how individuals managed anxiety and depression in 2020",
    "section": "",
    "text": "Data DetailsBlog postBehind the Scenes\n\n\n\n\nExploring the prevalence of depression by age, this study aims to analyze and understand the most three commonly used methods by individuals worldwide to cope with anxiety and depression in 2020. For each coping strategy, the aim is to uncover the top five countries that relied on it the most.\n\n\n\nOur World in Data is an online platform that provides accessible and comprehensive data on global development issues. The following datasets are used from the platform for analysis:\n\nHow do people deal with anxiety or depression? 2020\n\n\nThe dataset covers the mental health sections of the Wellcome Global Monitor Survey 2020 conducted in 113 countries and territories worldwide on mental health views and experiences. The full questionnaire of the survey can be found here.\n\nThe dataset consists of survey type data. It was last updated on April 19, 2023.\nDetailed description of the variables is provided in metadata and readme files.\nSource: Wellcome Global Monitor (2021) – processed by Our World in Data\nLicense: Creative Commons Attribution 4.0 International License (CC BY 4.0), this license allows users to share and adapt for any purpose, even commercially, as long as appropriate credit is given, a link to the license is provided, and any changes made are indicated.\n\n\n\n\nBased on the Deon’s Data Science Checklist the following parameters, _Informed consent, Collection bias, Limit PII exposure, Data security, Missing perspectives and Dataset bias in terms of Data Collection, Data Storage and Analysis respectively are considered and addressed appropriately.\nThe share is obtained weighting the answers based on the characteristics of the participant. Data weighting is used to minimize bias in survey estimates and is intended to be used to generate nationally representative estimates within a country.\nData for answers where the demographic group had less than 100 participants are filtered out.\nEmpty answers have been filtered out. Empty answers may appear because the question was not applicable to the respondent or the respondent did not answer the question.\n\n\n\n\n\nThe dataset includes data for the year 2020 only, which restricts us from performing comparative analysis and understand what coping mechanisms dominated before and after COVID 19.\nAnalyzing just one years data is insufficient to understand if the trends in usage of various coping methods remained consistent across the years or if there existed any variations.\nAn age variable, wherein the preference of coping mechanism categorized by age groups would have been more useful to conduct a detailed analysis.\n\n\nDepressive disorders prevalence, by age, World\n\n\nThe dataset gives insight into the share of population with depressive disorders. This is estimated as the total number of cases with depressive disorders relative to the population of a country for various age groups.\nThe dataset consists of both survey and experimental data considering the use of representative surveys and medical data . It was last updated on May 15, 2023.\nDetailed description of the variables is provided in metadata and readme files.\nSource: IHME, Global Burden of Disease (2020) – processed by Our World in Data\nLicense: Creative Commons Attribution 4.0 International License (CC BY 4.0), this license allows users to share and adapt for any purpose, even commercially, as long as appropriate credit is given, a link to the license is provided, and any changes made are indicated.\n\n\n\n\n\nBased on the Deon’s Data Science Checklist the following parameters, _Informed consent, Collection bias, Limit PII exposure, Data security, Missing perspectives and Dataset bias in terms of Data Collection, Data Storage and Analysis respectively are considered and addressed appropriately.\nPersonally identifiable information (PII) is completely anonymized.\nSubjects have given informed consent and collection bias is taken care of. For instance, data is collected from both diagnosed and un-diagnosed individuals.\n\n\n\n\n\nThere is a lag in the data records. Data is available only until 2019.\nA variable representing gender would have made the analysis more interesting and informative.\nShares related to diagnosed and un-diagnosed cases should have been represented separately for better understanding.\n\n\n\n\n\n\nNavigate to Our World in data website.\nClick on Browse by Topic and under Health, select Mental Health.\nUsing the search bar, search for “How do people deal with anxiety or depression around the world? 2020”\nClick on the download button and select to download data in csv format.\nRepeat the previous two steps for downloading dataset 2. In the search bar, instead search for “Depressive disorders prevalence, by age, World, 2019”.\n\n\n\n\n\n\nIn today’s world, given the fast-paced life, societal pressures and economic instability, mental health especially depression has been on the rise and is becoming a global challenge affecting individuals across all ages, backgrounds, and cultures. As of 2019, the share of population in the world , suffering from either diagnosed or un-diagnosed depression from different age groups can be observed below. There is a steady increase in percentage of individuals feeling depressed with increasing age.\n\n\n\n\n\n\n\n\n\nMoreover, the COVID-19 has significantly highlighted the importance of mental health, amplifying feelings of isolation, anxiety, and uncertainty. Despite growing awareness, stigma surrounding mental illness persists, hindering individuals from seeking help and accessing adequate support. Addressing mental health and depression requires multifaceted approaches, encompassing de-stigmatization efforts, accessible and culturally sensitive mental health services, and holistic strategies that prioritize prevention, early intervention, and community support.\nThe blog aims to provide a comprehensive overview of global prevalence of depression and coping strategies being used. It serves as a valuable resource for anyone looking to understand the prevalence of depression among various age groups and the various ways it is being managed globally. By showcasing different coping methods being used around the world, it encourages a more inclusive and empathetic approach to mental health care, ultimately fostering a more supportive and understanding global community.\n\n\n\nIn ordinary circumstances, dealing with mental health conditions is already challenging. However, the additional restrictions and the distressing environment brought about by the COVID-19 pandemic have undoubtedly exacerbated these difficulties. Prolonged periods of isolation have intensified symptoms, making it even more arduous for individuals to cope. In such times, it becomes imperative to understand which coping methods have been most effective for people. This understanding plays a vital role in advancing our comprehension of mental health, enhancing support systems, and nurturing resilience within communities. It would have a direct impact on the following:\n\nImproving Mental Health Interventions: Provide insights into effective coping strategies which can be used to tailor interventions that better support individuals in managing their mental health.\nEnhancing Treatment Approaches: Refine treatment approaches to better address the specific needs based on their coping mechanisms.\nEnhancing Self-Awareness and Resilience: Individuals themselves can benefit from understanding various coping strategies used by others and enhance theirs.\nAdvancing Research: Research into how individuals manage anxiety and depression contributes to the broader understanding of mental health leading to development of new therapeutic interventions.\n\n\n\n\nThe data used for analysis can be found on Our World In Data, an online platform that provides accessible and comprehensive data on global development issues. The two datasets can be searched for as follows:\n\n“How do people deal with anxiety or depression around the world? 2020”\n“Depressive disorders prevalence, by age, World, 2019”\n\nFor more detailed information of the datasets, refer to the data code-book and readme file.\n\n\n\n\n\nWhile coping with anxiety and depression varies greatly among individuals, influenced by factors such as their environment, ethnicity, and available resources, certain coping methods enjoy widespread popularity worldwide. Understanding these universally favored coping mechanisms can shed light on effective strategies for managing mental health challenges across diverse populations. So, what are the top 3 coping methods preferred by the individuals around the world?\n\n\n\nShare of Use of Various Coping Methods\n\n\nMethods\nShare of Use in Percentage\n\n\n\n\nTalked_to_friends_or_family\n79.53809\n\n\nSpent_time_in_nature\n72.01470\n\n\nImproved_healthy_lifestyle_behaviors\n70.84874\n\n\nMade_a_change_to_personal_relationships\n60.42453\n\n\nMade_a_change_to_work_situation\n49.71871\n\n\nTook_prescribed_medication\n49.11862\n\n\nTalked_to_mental_health_professional\n44.11910\n\n\nReligious_spiritual_activities\n42.88649\n\n\n\n\n\nAmong, the eight methods listed above we can conclude that three most used methods to cope with anxiety and depression were:\n\nTalked_to_friends_or_family: With 79.5% of individuals reporting this as a coping method, it underscores the crucial role of social support in managing mental health. Emotional connections and conversations with loved ones provide significant comfort and reduce feelings of isolation.\nSpending Time in Nature: At 72.01%, this method highlights the therapeutic benefits of the natural environment. Time spent outdoors is linked to reduced stress levels and improved mood, indicating the importance of incorporating nature into mental health strategies.\nImproving Healthy Lifestyle Behaviors: With 70.85% of individuals adopting this approach, it emphasizes the impact of healthy habits, such as regular exercise, balanced nutrition, and adequate sleep, on mental well-being. These behaviors contribute to overall physical health, which is closely connected to mental health.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above figure, we can observe that the coping methods of individuals in different countries varied widely, highlighting that there is no one-size-fits-all approach. This underscores the importance of analyzing diverse strategies to spread awareness of the various methods available and to understand which methods are best suited for different demographics. Such analysis is crucial for developing targeted interventions that cater to individuals’ specific needs based on their demographic, ethnic, and socio-economic backgrounds.\n\nFor example, in countries like Ecuador, Peru, and Nicaragua, individuals tend to prefer coping methods such as improving healthy lifestyle behaviors, making changes to personal relationships, and altering work situations. These strategies reflect a focus on personal and social adjustments to manage mental health.\nIn contrast, developed countries like the United Kingdom, Switzerland, and Canada show a preference for seeking professional help through methods such as taking prescribed medication and talking to mental health professionals. This trend indicates a reliance on medical and professional support systems available in these nations.\nInterestingly, the United States, Italy, and Albania show a tendency towards spending time in nature as a preferred coping method. This suggests a recognition of the therapeutic benefits of natural environments in managing stress and enhancing mental well-being.\nOn the other hand, lower-middle-income countries such as Kenya, Nigeria, and Sri Lanka predominantly engage in religious or spiritual activities as their main coping mechanism. This highlights the cultural and spiritual dimensions of mental health management in these regions.\nFinally, talking to friends or family is a common coping strategy in countries like the Philippines, Zimbabwe, and Laos, emphasizing the importance of social support and community connections in these societies.\n\n\n\n\n\nIn conclusion, these findings suggest that effective coping strategies for anxiety and depression often involve a combination of social interaction, engagement with nature, and the adoption of healthy lifestyle practices. These methods collectively contribute to enhancing resilience and promoting mental well-being.\nIt highlights the importance of both professional and non-professional support systems. It shows that while some individuals in developed countries may prefer seeking help from mental health professionals and taking prescribed medication, others in lower-middle-income countries might find solace in religious or spiritual activities. Understanding diverse preferences of coping methods is essential for tailoring mental health interventions to different populations. It also provides valuable insights for researchers aiming to develop more specific and culturally appropriate strategies to support individuals across various demographics and backgrounds.\n\n\n\nThe blog post is particularly interesting to me as it illuminates and spreads awareness about the importance of mental health. It not only discusses the prevalence of depression but also highlights the diverse methods being used around the world to cope with it. This approach captures the essence of the problem while providing a practical guide for people to explore various strategies.\nOne of the most compelling aspects of the post is the way it illustrates how coping methods vary based on demographics, background, and ethnicity. This variation emphasizes the necessity of understanding mental health within different cultural and socio-economic contexts. It demonstrates that while there are numerous ways to manage depression, effective coping often begins with personal-level changes, such as adjusting behaviors, spending time in nature, and reaching out to friends and family.\n\n\n\n\nThibaut, F. (2020). “Anxiety and depression in the context of COVID-19.” International Journal of Psychiatry in Clinical Practice, 24(3), 229-231. DOI: 10.1080/13651501.2020.1760227\nCharlson, F., van Ommeren, M., Flaxman, A., Cornett, J., Whiteford, H., & Saxena, S. (2019).”New WHO prevalence estimates of mental disorders in conflict settings: a systematic review and meta-analysis.” The Lancet, 394(10194), 240-248. DOI: 10.1016/S0140-6736(19)30934-1\nThibaut, F. (2020). “Anxiety and depression in the context of COVID-19.” International Journal of Psychiatry in Clinical Practice, 24(3), 229-231. DOI: 10.1080/13651501.2020.1760227 4.Park, C. L., Russell, B. S., Fendrich, M., Finkelstein-Fox, L., Hutchison, M., & Becker, J. (2020). “Americans’ COVID-19 stress, coping, and adherence to CDC guidelines.” Journal of General Internal Medicine, 35(8), 2296-2303. DOI: 10.1007/s11606-020-05898-9\nWellcome Global Monitor (2021) – processed by Our World in Data. “How do people deal with anxiety or depression around the world? 2020”. Wellcome Global Monitor (2021) [original data].\nIHME, Global Burden of Disease (2020) – processed by Our World in Data. “Depressive disorders prevalence, by age, World”. IHME, Global Burden of Disease (2020) [original data].\n\n\n\n\n\n\n\nOne of the most exhausting aspects of data processing was cleaning the dataset. Manually renaming all the variables, as they were poorly formatted in the raw data, was not only uninteresting but also repetitive and time-consuming. Ensuring accurate and meaningful variable names was crucial for clarity and ease of analysis.\nThe datasets were relatively small, with a limited number of variables. This simplicity made the analysis straightforward and efficient. Additionally, there were almost no missing values, except for one instance, which ensured consistent data without any interruptions or need for extensive imputation methods.\nI would certainly omit the data cleaning and analysis code from the blog post. Instead, the blog would focus on the key insights and findings derived from the analysis, making it more engaging for readers. By highlighting the results and their implications, the blog can effectively communicate the value of the analysis without delving into the technical details of data preparation.\n\n\n\n\n\nConducting the analysis posed no significant challenges; however, the limited availability of data for just one year in the comparison dataset “dealing-with-anxiety-depression” severely constrained the scope of the analysis. While I managed to draw key conclusions, the absence of data spanning pre- and post-COVID periods hindered the ability to analyze any temporal trends or changes over time. Further, not having individual gender and age group variables, limited the analysis to a broader perspective.\nCreating the plots proved to be time-consuming, as achieving the right aesthetics and formatting to present the data in a clear and organized manner required considerable effort. -Initially, I did not anticipate substantial variations in the utilization of different coping methods for anxiety and depression across various countries. I had hoped to group the coping method variables for common countries. However, due to the unexpected diversity in coping strategies, I had to resort to using “facet_wrap” to represent each variable separately.\n\n\n\n\n\nI aim to conduct a more comprehensive analysis by incorporating variables indicating age groups and gender and their utilization of various coping methods. This expanded dataset would enable a deeper understanding of which coping strategies are most effective for different age and gender demographics across diverse countries.\nFurthermore, I plan to enhance the analysis by delving into the prevalence of depression and the corresponding utilization of coping methods within each individual country. By examining these factors on a country-specific level, I aim to uncover nuanced insights into the interplay between mental health trends and coping behaviors within distinct socio-cultural contexts.\nAdditionally, I intend to explore the potential influence of a country’s GDP on the prevalence of depression and the variation in coping method usage. By analyzing these relationships, I seek to discern whether economic factors significantly impact mental health outcomes and coping strategies. This investigation could shed light on the socio-economic determinants of mental health and inform targeted interventions aimed at mitigating disparities."
  },
  {
    "objectID": "posts/Proj_4_wcd3/index.html#how-did-individuals-manage-anxiety-and-depression-in-2020",
    "href": "posts/Proj_4_wcd3/index.html#how-did-individuals-manage-anxiety-and-depression-in-2020",
    "title": "Exploring data methods to uncover how individuals managed anxiety and depression in 2020",
    "section": "",
    "text": "Data DetailsBlog postBehind the Scenes\n\n\n\n\nExploring the prevalence of depression by age, this study aims to analyze and understand the most three commonly used methods by individuals worldwide to cope with anxiety and depression in 2020. For each coping strategy, the aim is to uncover the top five countries that relied on it the most.\n\n\n\nOur World in Data is an online platform that provides accessible and comprehensive data on global development issues. The following datasets are used from the platform for analysis:\n\nHow do people deal with anxiety or depression? 2020\n\n\nThe dataset covers the mental health sections of the Wellcome Global Monitor Survey 2020 conducted in 113 countries and territories worldwide on mental health views and experiences. The full questionnaire of the survey can be found here.\n\nThe dataset consists of survey type data. It was last updated on April 19, 2023.\nDetailed description of the variables is provided in metadata and readme files.\nSource: Wellcome Global Monitor (2021) – processed by Our World in Data\nLicense: Creative Commons Attribution 4.0 International License (CC BY 4.0), this license allows users to share and adapt for any purpose, even commercially, as long as appropriate credit is given, a link to the license is provided, and any changes made are indicated.\n\n\n\n\nBased on the Deon’s Data Science Checklist the following parameters, _Informed consent, Collection bias, Limit PII exposure, Data security, Missing perspectives and Dataset bias in terms of Data Collection, Data Storage and Analysis respectively are considered and addressed appropriately.\nThe share is obtained weighting the answers based on the characteristics of the participant. Data weighting is used to minimize bias in survey estimates and is intended to be used to generate nationally representative estimates within a country.\nData for answers where the demographic group had less than 100 participants are filtered out.\nEmpty answers have been filtered out. Empty answers may appear because the question was not applicable to the respondent or the respondent did not answer the question.\n\n\n\n\n\nThe dataset includes data for the year 2020 only, which restricts us from performing comparative analysis and understand what coping mechanisms dominated before and after COVID 19.\nAnalyzing just one years data is insufficient to understand if the trends in usage of various coping methods remained consistent across the years or if there existed any variations.\nAn age variable, wherein the preference of coping mechanism categorized by age groups would have been more useful to conduct a detailed analysis.\n\n\nDepressive disorders prevalence, by age, World\n\n\nThe dataset gives insight into the share of population with depressive disorders. This is estimated as the total number of cases with depressive disorders relative to the population of a country for various age groups.\nThe dataset consists of both survey and experimental data considering the use of representative surveys and medical data . It was last updated on May 15, 2023.\nDetailed description of the variables is provided in metadata and readme files.\nSource: IHME, Global Burden of Disease (2020) – processed by Our World in Data\nLicense: Creative Commons Attribution 4.0 International License (CC BY 4.0), this license allows users to share and adapt for any purpose, even commercially, as long as appropriate credit is given, a link to the license is provided, and any changes made are indicated.\n\n\n\n\n\nBased on the Deon’s Data Science Checklist the following parameters, _Informed consent, Collection bias, Limit PII exposure, Data security, Missing perspectives and Dataset bias in terms of Data Collection, Data Storage and Analysis respectively are considered and addressed appropriately.\nPersonally identifiable information (PII) is completely anonymized.\nSubjects have given informed consent and collection bias is taken care of. For instance, data is collected from both diagnosed and un-diagnosed individuals.\n\n\n\n\n\nThere is a lag in the data records. Data is available only until 2019.\nA variable representing gender would have made the analysis more interesting and informative.\nShares related to diagnosed and un-diagnosed cases should have been represented separately for better understanding.\n\n\n\n\n\n\nNavigate to Our World in data website.\nClick on Browse by Topic and under Health, select Mental Health.\nUsing the search bar, search for “How do people deal with anxiety or depression around the world? 2020”\nClick on the download button and select to download data in csv format.\nRepeat the previous two steps for downloading dataset 2. In the search bar, instead search for “Depressive disorders prevalence, by age, World, 2019”.\n\n\n\n\n\n\nIn today’s world, given the fast-paced life, societal pressures and economic instability, mental health especially depression has been on the rise and is becoming a global challenge affecting individuals across all ages, backgrounds, and cultures. As of 2019, the share of population in the world , suffering from either diagnosed or un-diagnosed depression from different age groups can be observed below. There is a steady increase in percentage of individuals feeling depressed with increasing age.\n\n\n\n\n\n\n\n\n\nMoreover, the COVID-19 has significantly highlighted the importance of mental health, amplifying feelings of isolation, anxiety, and uncertainty. Despite growing awareness, stigma surrounding mental illness persists, hindering individuals from seeking help and accessing adequate support. Addressing mental health and depression requires multifaceted approaches, encompassing de-stigmatization efforts, accessible and culturally sensitive mental health services, and holistic strategies that prioritize prevention, early intervention, and community support.\nThe blog aims to provide a comprehensive overview of global prevalence of depression and coping strategies being used. It serves as a valuable resource for anyone looking to understand the prevalence of depression among various age groups and the various ways it is being managed globally. By showcasing different coping methods being used around the world, it encourages a more inclusive and empathetic approach to mental health care, ultimately fostering a more supportive and understanding global community.\n\n\n\nIn ordinary circumstances, dealing with mental health conditions is already challenging. However, the additional restrictions and the distressing environment brought about by the COVID-19 pandemic have undoubtedly exacerbated these difficulties. Prolonged periods of isolation have intensified symptoms, making it even more arduous for individuals to cope. In such times, it becomes imperative to understand which coping methods have been most effective for people. This understanding plays a vital role in advancing our comprehension of mental health, enhancing support systems, and nurturing resilience within communities. It would have a direct impact on the following:\n\nImproving Mental Health Interventions: Provide insights into effective coping strategies which can be used to tailor interventions that better support individuals in managing their mental health.\nEnhancing Treatment Approaches: Refine treatment approaches to better address the specific needs based on their coping mechanisms.\nEnhancing Self-Awareness and Resilience: Individuals themselves can benefit from understanding various coping strategies used by others and enhance theirs.\nAdvancing Research: Research into how individuals manage anxiety and depression contributes to the broader understanding of mental health leading to development of new therapeutic interventions.\n\n\n\n\nThe data used for analysis can be found on Our World In Data, an online platform that provides accessible and comprehensive data on global development issues. The two datasets can be searched for as follows:\n\n“How do people deal with anxiety or depression around the world? 2020”\n“Depressive disorders prevalence, by age, World, 2019”\n\nFor more detailed information of the datasets, refer to the data code-book and readme file.\n\n\n\n\n\nWhile coping with anxiety and depression varies greatly among individuals, influenced by factors such as their environment, ethnicity, and available resources, certain coping methods enjoy widespread popularity worldwide. Understanding these universally favored coping mechanisms can shed light on effective strategies for managing mental health challenges across diverse populations. So, what are the top 3 coping methods preferred by the individuals around the world?\n\n\n\nShare of Use of Various Coping Methods\n\n\nMethods\nShare of Use in Percentage\n\n\n\n\nTalked_to_friends_or_family\n79.53809\n\n\nSpent_time_in_nature\n72.01470\n\n\nImproved_healthy_lifestyle_behaviors\n70.84874\n\n\nMade_a_change_to_personal_relationships\n60.42453\n\n\nMade_a_change_to_work_situation\n49.71871\n\n\nTook_prescribed_medication\n49.11862\n\n\nTalked_to_mental_health_professional\n44.11910\n\n\nReligious_spiritual_activities\n42.88649\n\n\n\n\n\nAmong, the eight methods listed above we can conclude that three most used methods to cope with anxiety and depression were:\n\nTalked_to_friends_or_family: With 79.5% of individuals reporting this as a coping method, it underscores the crucial role of social support in managing mental health. Emotional connections and conversations with loved ones provide significant comfort and reduce feelings of isolation.\nSpending Time in Nature: At 72.01%, this method highlights the therapeutic benefits of the natural environment. Time spent outdoors is linked to reduced stress levels and improved mood, indicating the importance of incorporating nature into mental health strategies.\nImproving Healthy Lifestyle Behaviors: With 70.85% of individuals adopting this approach, it emphasizes the impact of healthy habits, such as regular exercise, balanced nutrition, and adequate sleep, on mental well-being. These behaviors contribute to overall physical health, which is closely connected to mental health.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the above figure, we can observe that the coping methods of individuals in different countries varied widely, highlighting that there is no one-size-fits-all approach. This underscores the importance of analyzing diverse strategies to spread awareness of the various methods available and to understand which methods are best suited for different demographics. Such analysis is crucial for developing targeted interventions that cater to individuals’ specific needs based on their demographic, ethnic, and socio-economic backgrounds.\n\nFor example, in countries like Ecuador, Peru, and Nicaragua, individuals tend to prefer coping methods such as improving healthy lifestyle behaviors, making changes to personal relationships, and altering work situations. These strategies reflect a focus on personal and social adjustments to manage mental health.\nIn contrast, developed countries like the United Kingdom, Switzerland, and Canada show a preference for seeking professional help through methods such as taking prescribed medication and talking to mental health professionals. This trend indicates a reliance on medical and professional support systems available in these nations.\nInterestingly, the United States, Italy, and Albania show a tendency towards spending time in nature as a preferred coping method. This suggests a recognition of the therapeutic benefits of natural environments in managing stress and enhancing mental well-being.\nOn the other hand, lower-middle-income countries such as Kenya, Nigeria, and Sri Lanka predominantly engage in religious or spiritual activities as their main coping mechanism. This highlights the cultural and spiritual dimensions of mental health management in these regions.\nFinally, talking to friends or family is a common coping strategy in countries like the Philippines, Zimbabwe, and Laos, emphasizing the importance of social support and community connections in these societies.\n\n\n\n\n\nIn conclusion, these findings suggest that effective coping strategies for anxiety and depression often involve a combination of social interaction, engagement with nature, and the adoption of healthy lifestyle practices. These methods collectively contribute to enhancing resilience and promoting mental well-being.\nIt highlights the importance of both professional and non-professional support systems. It shows that while some individuals in developed countries may prefer seeking help from mental health professionals and taking prescribed medication, others in lower-middle-income countries might find solace in religious or spiritual activities. Understanding diverse preferences of coping methods is essential for tailoring mental health interventions to different populations. It also provides valuable insights for researchers aiming to develop more specific and culturally appropriate strategies to support individuals across various demographics and backgrounds.\n\n\n\nThe blog post is particularly interesting to me as it illuminates and spreads awareness about the importance of mental health. It not only discusses the prevalence of depression but also highlights the diverse methods being used around the world to cope with it. This approach captures the essence of the problem while providing a practical guide for people to explore various strategies.\nOne of the most compelling aspects of the post is the way it illustrates how coping methods vary based on demographics, background, and ethnicity. This variation emphasizes the necessity of understanding mental health within different cultural and socio-economic contexts. It demonstrates that while there are numerous ways to manage depression, effective coping often begins with personal-level changes, such as adjusting behaviors, spending time in nature, and reaching out to friends and family.\n\n\n\n\nThibaut, F. (2020). “Anxiety and depression in the context of COVID-19.” International Journal of Psychiatry in Clinical Practice, 24(3), 229-231. DOI: 10.1080/13651501.2020.1760227\nCharlson, F., van Ommeren, M., Flaxman, A., Cornett, J., Whiteford, H., & Saxena, S. (2019).”New WHO prevalence estimates of mental disorders in conflict settings: a systematic review and meta-analysis.” The Lancet, 394(10194), 240-248. DOI: 10.1016/S0140-6736(19)30934-1\nThibaut, F. (2020). “Anxiety and depression in the context of COVID-19.” International Journal of Psychiatry in Clinical Practice, 24(3), 229-231. DOI: 10.1080/13651501.2020.1760227 4.Park, C. L., Russell, B. S., Fendrich, M., Finkelstein-Fox, L., Hutchison, M., & Becker, J. (2020). “Americans’ COVID-19 stress, coping, and adherence to CDC guidelines.” Journal of General Internal Medicine, 35(8), 2296-2303. DOI: 10.1007/s11606-020-05898-9\nWellcome Global Monitor (2021) – processed by Our World in Data. “How do people deal with anxiety or depression around the world? 2020”. Wellcome Global Monitor (2021) [original data].\nIHME, Global Burden of Disease (2020) – processed by Our World in Data. “Depressive disorders prevalence, by age, World”. IHME, Global Burden of Disease (2020) [original data].\n\n\n\n\n\n\n\nOne of the most exhausting aspects of data processing was cleaning the dataset. Manually renaming all the variables, as they were poorly formatted in the raw data, was not only uninteresting but also repetitive and time-consuming. Ensuring accurate and meaningful variable names was crucial for clarity and ease of analysis.\nThe datasets were relatively small, with a limited number of variables. This simplicity made the analysis straightforward and efficient. Additionally, there were almost no missing values, except for one instance, which ensured consistent data without any interruptions or need for extensive imputation methods.\nI would certainly omit the data cleaning and analysis code from the blog post. Instead, the blog would focus on the key insights and findings derived from the analysis, making it more engaging for readers. By highlighting the results and their implications, the blog can effectively communicate the value of the analysis without delving into the technical details of data preparation.\n\n\n\n\n\nConducting the analysis posed no significant challenges; however, the limited availability of data for just one year in the comparison dataset “dealing-with-anxiety-depression” severely constrained the scope of the analysis. While I managed to draw key conclusions, the absence of data spanning pre- and post-COVID periods hindered the ability to analyze any temporal trends or changes over time. Further, not having individual gender and age group variables, limited the analysis to a broader perspective.\nCreating the plots proved to be time-consuming, as achieving the right aesthetics and formatting to present the data in a clear and organized manner required considerable effort. -Initially, I did not anticipate substantial variations in the utilization of different coping methods for anxiety and depression across various countries. I had hoped to group the coping method variables for common countries. However, due to the unexpected diversity in coping strategies, I had to resort to using “facet_wrap” to represent each variable separately.\n\n\n\n\n\nI aim to conduct a more comprehensive analysis by incorporating variables indicating age groups and gender and their utilization of various coping methods. This expanded dataset would enable a deeper understanding of which coping strategies are most effective for different age and gender demographics across diverse countries.\nFurthermore, I plan to enhance the analysis by delving into the prevalence of depression and the corresponding utilization of coping methods within each individual country. By examining these factors on a country-specific level, I aim to uncover nuanced insights into the interplay between mental health trends and coping behaviors within distinct socio-cultural contexts.\nAdditionally, I intend to explore the potential influence of a country’s GDP on the prevalence of depression and the variation in coping method usage. By analyzing these relationships, I seek to discern whether economic factors significantly impact mental health outcomes and coping strategies. This investigation could shed light on the socio-economic determinants of mental health and inform targeted interventions aimed at mitigating disparities."
  },
  {
    "objectID": "posts/copingmethodsapp/index.html",
    "href": "posts/copingmethodsapp/index.html",
    "title": "Coping Methods App Documentation",
    "section": "",
    "text": "Coping Methods App\nThe Coping Methods App is an interactive Shiny application included with this package. It enables users to explore mental-health–related data through visualisations such as:\n\ndepression trends across age groups\n\nyear-to-year changes\n\ncomparison of coping strategies between countries\n\nBecause the app runs locally, this page provides static screenshots, key insights, and clear explanations to help readers understand the functionality before launching it.\nTo run the app:\n\nlibrary(copingmethods)\nlaunch_copingmethods()\n\n\n\n\n1. Depression by Age — Country View\n\n\nWhat this visualisation shows\nThis tab displays depression percentages across age groups for a selected country/entity and year.\nThe screenshot below shows Afghanistan (1990).\nThe bar chart visualises depression levels across age groups, while the table on the right provides:\n\ndepression (%) for each age group\n\ndifferences relative to the comparison year\n\n\n\n\n\n\nFigure 1: Depression percentages by age group for Afghanistan in 1990.\n\n\nKey insights\n\n\n\n\n\n\nTip\n\n\n\n\nDepression increases steadily across age groups in the displayed dataset.\n\nThe table provides precise numerical values alongside the visual chart.\n\nThis view makes it easy to compare age-specific depression profiles across countries and years.\n\n\n\n\n\n\n\n\n\n2. Year-to-Year Comparison (Trend Analysis)\n\n\nWhat this visualisation shows\nThis tab compares how depression percentages have changed over time relative to a baseline year.\nThe screenshot below shows America (IHME GBD), 2009, compared against 1990.\nThe plot includes:\n\ndepression (%) by age group in 2009\n\na yellow dashed reference line showing the world average depression level\n\na table that lists:\n\ndepression in 1990\n\ndepression in 2009\n\nthe percentage differences\n\n\n\n\n\n\n\nFigure 2: Depression percentages in America (IHME GBD) for 2009 compared with 1990.\n\n\nKey insights\n\n\n\n\n\n\nTip\n\n\n\n\nSeveral age groups show increases or decreases between 1990 and 2009.\n\nThe “Standardised” row provides an overall summary measure of change.\n\nThe global average line allows quick comparison between America and global levels.\n\n\n\n\n\n\n\n\n\n3. Coping Methods Comparison — Between Countries\n\n\nWhat this visualisation shows\nThis tab compares coping strategies used in two selected countries.\nThe screenshot below compares Australia and India.\nThe chart illustrates coping strategies such as:\n\ntalking to friends or family\n\nlifestyle improvements\n\nspending time in nature\n\nseeking professional help\n\nmedication use\n\nwork or social adjustments\n\n\n\n\n\n\nFigure 3: Comparison of coping methods between Australia and India.\n\n\nKey insights\n\n\n\n\n\n\nTip\n\n\n\n\nAustralia shows greater use of professional services and lifestyle-based coping methods.\n\nIndia shows stronger use of community and family-based support strategies.\n\nThe ranked bar chart clearly highlights cultural differences in coping preferences.\n\n\n\n\n\n\n\n\n\nFull Vignette Documentation\nFor a complete walkthrough of the app, dataset, functions, and development process, refer to:\nFull App Overview Documentation\n\n\n\nRun the App Locally\n\nlibrary(copingmethods)\nlaunch_copingmethods()\n\nLaunching this command will open the interactive Shiny app in your browser.\n\n\n\nWhy This Page Exists\nBecause hosting Shiny apps online requires paid servers,\nthis documentation page ensures the app is still fully understandable by providing:\n\nstatic screenshots\n\nclear explanations\n\nhighlighted insights\n\na faithful visual representation of the interface\n\nThis allows assessors, collaborators, and website viewers to understand the app without needing to run it immediately."
  },
  {
    "objectID": "posts/Proj_3_wcd2/index.html",
    "href": "posts/Proj_3_wcd2/index.html",
    "title": "Data deidentification and modification",
    "section": "",
    "text": "Code\nloanData &lt;- readRDS(here::here(\"posts/Proj_3_wcd2/raw_data/loanData.rds\"))"
  },
  {
    "objectID": "posts/Proj_3_wcd2/index.html#data-collection",
    "href": "posts/Proj_3_wcd2/index.html#data-collection",
    "title": "Data deidentification and modification",
    "section": "Data Collection",
    "text": "Data Collection\n\nInformed Consent\n\n\nYes, the individual are fully informed about the data collection, its purpose, sharing and associated risks. Further, they have clear comprehension of its usage and have voluntarily agreed to participate.\nContact Information of the data custodian has been made available for seeking further information on data collection and its processing.\nThe consent records are well documented and updated regularly. Lastly, legal compliance governing the use and sharing of financial and personal data have been taken into consideration.\n\n\nCollection Bias\n\n\nYes, collection bias is considered and a diverse set of protocols are in place to mitigate collection bias, namely:\nVaried data sources and sampling methods are used. To reduce bias, incorporation of data from multiple demographics, regions and markets, avoiding reliance on a single source or method.\nConducting regular audits and reviews to identify and address any emerging bias.\nUse of statistical adjustments, such as weighting, applied to data to correct for either under-underrepresented or over-represented groups.\n\n\nLimit PII exposure\n\n\nYes, steps are taken to limit the exposure of personally identifiable information in the data.\nData Minimization: Only data that is required and adds value to the analysis is collected. Any irrelevant information that could potentially increase the privacy risks is avoided.\nAnonymization: Altering PII’s so that individuals can’t be readily identified. For example, ZIP3 codes are provided, instead of storing the exact addresses of the property.\nData Governance Policies - Clear policies and procedures are established for data handling to ensure consistent and secure data practices."
  },
  {
    "objectID": "posts/Proj_3_wcd2/index.html#data-storage",
    "href": "posts/Proj_3_wcd2/index.html#data-storage",
    "title": "Data deidentification and modification",
    "section": "Data Storage",
    "text": "Data Storage\n\nData Security\n\n\nSignificant emphasis on data security to safeguard sensitive information is imposed. Data security concerns are handled diligently, following are a few:\nData is encrypted to keep it safe, whether it’s stored or being transmitted. A further layer of protection is added, in case of any interceptions using decryption keys.\nRegularly updating software and infrastructure to address any known vulnerabilities. Applying security patches and ensuring software is update date with security updates.\nStrictly controlled access to data with authentication mechanisms.\n\n\nRight to be Forgotten\n\n\nYes, provisions are in place to respect individual’s privacy and the right to have their personal information removed from the database upon request.\nProcedures and mechanisms allow individuals to submit such requests and appropriate steps will be taken to erase their information.\n\n\nData Retention Plan\n\n\nTo minimize privacy risks, and complying with regulatory requirements there exists a structured data retention plan to govern the duration for which data is kept. This plan specifies timelines and criteria for data deletion once no longer required for its intended use."
  },
  {
    "objectID": "posts/Proj_3_wcd2/index.html#identification-and-removal-of-direct-identifiers",
    "href": "posts/Proj_3_wcd2/index.html#identification-and-removal-of-direct-identifiers",
    "title": "Data deidentification and modification",
    "section": "Identification and Removal of Direct Identifiers",
    "text": "Identification and Removal of Direct Identifiers\n\nThe direct identifiers identified in the dataset are as follows:\n\n\nLoan ID\nLast name\nFirst name\n\n\nLoan ID is a distinct identifier which directly links to a specific borrowers financial information. Associated with borrowers and their loan details makes it a direct identifier.\nBoth last name and first name are directly associated with individuals and can be easily used to identify a specific individual within the dataset, following which all the loan data related to the borrower can be identified.\n\n\n\nCode\nrelease_dataset &lt;- loanData |&gt;\n                   select(-loan_id, -last_name, -first_name)"
  },
  {
    "objectID": "posts/Proj_3_wcd2/index.html#de-identification-strategy",
    "href": "posts/Proj_3_wcd2/index.html#de-identification-strategy",
    "title": "Data deidentification and modification",
    "section": "De-identification Strategy",
    "text": "De-identification Strategy\n\nThe de- identification strategy considers the modification of the following variables: seller, orig_rt, orig_amt, dti, cscore_b, frst_date, last_activity_date, last_rt, income, age.\n\n\nRemoval of Quasi-Identifiers\n\n\nCode\nrelease_dataset &lt;- release_dataset |&gt;\n                   select(-no_depend, -cscore_c, -last_upb, -num_bo, -zip_3)\n\n\n\nThe following variables have been excluded because they are not essential for understanding the credit performance of mortgage loans. Additionally, they are not directly relevant and could potentially be linked to the borrower.\nno_depend - Number of dependents ; last_upb - Last upstanding balance ; num_bo - Number of borrowers ; cscore_c - Credit score of co-borrower\nZip3 : After careful consideration of the variables state, MSA, and zip3, decision to remove zip3 is made. Zip3 offers more granular geographic information compared to MSA. Combining state and MSA provides a broader representation of geographical areas, reducing concerns of reidentification.\n\n\n\nAggregating the customer age\n\n\nCode\n# Aggregating age into bins of 10 years\nrelease_dataset &lt;- release_dataset |&gt;\n                    mutate(age_group = cut(cus_age, breaks = c(28,38,48))) |&gt; \n                    select(!cus_age)                   \n\n\n\nGrouping age into bins preserves important trends and patterns while reducing the granularity of the data, making it harder to identify individuals when combined with other data. This protects privacy while still allowing analysis of credit performance across age groups.\n\n\n\nPertubation of Income Values\n\n\nCode\n# Using log normal pertubation on income \nlog_income &lt;- log(release_dataset$income)\n\n# Calculating mean and standard deviation of the log-transformed income\nmean_log &lt;- mean(log_income)\nsd_log &lt;- sd(log_income)\n\n\n\n\nCode\n# Log-normal perturbation\nset.seed(123) \n\nperturbed_log_income &lt;- log_income +\n                        rnorm(length(release_dataset$income), \n                               mean = 0, sd = 0.1 * sd_log)\n\nperturbed_income &lt;- exp(perturbed_log_income)\n\n# Unlisting perturbed_income \nunlisted_perturbed_income &lt;- unlist(perturbed_income)\n\n\n\n\nCode\n# Adding new income to the release_dataset \nrelease_dataset$income_main &lt;- unlisted_perturbed_income\n\n# Checking the margin of error\nmargin_of_error &lt;- abs(perturbed_income - release_dataset$income)\n\n\n\nInitial analysis of the income variable revealed challenges with higher income ranges when using a normal distribution, as the absolute magnitude of change was relatively lesser compared to the lower margin incomes. Adjusting the standard deviation to address this issue resulted in negative values for lower income ranges.\nHence, log normal distribution was considered which introduced proportional variability, adding substantial changes for higher incomes as compared to the lower range of incomes.\nUsing log normal distribution preserves statistical meaning for analysis, making it harder to identify individuals by income levels. It also addresses outliers and skewed distributions, enhancing de-identification.\n\n\n\nMasking Dates\n\n\nCode\n# Converting first payment date and last activity date to quarters of the year\nrelease_dataset &lt;- release_dataset |&gt;\n                   mutate(last_activity_date = ymd(last_activity_date), \n                          frst_dte = ymd(frst_dte))\n\n\n# Creating a function format the dates \nget_quarter_year &lt;- function(date) {\n  year &lt;- format(date, \"%Y\")\n  month &lt;- month(date)\n  quarter &lt;- ceiling(month / 3)\n  paste0(year, \"-Q\", quarter)\n}\n\n# Applying function to last activity date\nrelease_dataset$last_activity_quarter &lt;- sapply(release_dataset$last_activity_date,                                                             get_quarter_year)\n\n# Applying function to first payment date\nrelease_dataset$first_date_quarter &lt;- sapply(release_dataset$frst_dte, \n                                             get_quarter_year)\n\nrelease_dataset &lt;- release_dataset |&gt; \n                   select(-last_activity_date,    \n                           -frst_dte)\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\n\nrelease_dataset &lt;- release_dataset |&gt;\n  mutate(\n    last_activity_date = ymd(last_activity_date),\n    frst_dte = ymd(frst_dte),\n    last_activity_quarter = paste0(year(last_activity_date), \"-Q\", quarter(last_activity_date)),\n    first_date_quarter = paste0(year(frst_dte), \"-Q\", quarter(frst_dte))\n  ) |&gt;\n  select(-last_activity_date, -frst_dte)\n\n\n\nDates can reveal specific events and transactions associated with individuals. Converting dates into quarters of the year retains the temporal information while obscuring the exact dates. This strategy preserves the chronological order of events, allowing for analysis of trends and patterns over time and making it more challenging for re-identification.\n\n\n\nRounding borrower credit score\n\n\nCode\n# Generalizing the credit score by rounding it to the nearest 100\nrelease_dataset$cscore_b_gen &lt;- round(release_dataset$cscore_b / 100) * 100\n\nrelease_dataset &lt;- release_dataset |&gt; \n                   select(-cscore_b)\n\n\n\nBorrower credit scores are specific to each borrower and can widely vary, highly increasing the risk of identification.\nRounding the credit scores to the nearest 100, obscures the exact score while still preserving the general information.\nThis maintains the overall distribution and trends of credit scores while making it more difficult to identify specific individuals based solely on their credit score.\n\n\n\nPerturbation of DTI using Uniform Noise\n\n\nCode\n# Adding uniform noise to each DTI value\n\n# Define the range for uniform noise\nnoise_min &lt;- -5  \nnoise_max &lt;- 5   \n\n# Add uniform noise to each DTI value\nrelease_dataset$dti_p &lt;- release_dataset$dti +\n                         runif(length(release_dataset$dti), \n                               min = noise_min, max = noise_max) \n\nrelease_dataset &lt;- release_dataset |&gt; \n                   select(-dti)\n\n\n\nDebt to income ratio calculated by dividing the total monthly debt expense by the total monthly income of the borrower, provides insight into the borrowers finances and can be easily identified as variables such as income are available.\nBy adding uniform noise, it makes it harder to discern precise values while still ensuring overall distribution of DTI values remains consistent.\nIt introduces randomness to the data, while retaining its statistical properties.\n\n\n\nPertubation of Original Loan Amount\n\n\nCode\n# Scaling and adding noise \n\n# Random scaling factor between 0.9 and 1.1\n# Random Gaussian noise with mean 0 and standard deviation 1000\n\nscaling_factor &lt;- 0.9 + runif(length(release_dataset$orig_amt), \n                              min = 0, max = 0.2)  \n\nnoise &lt;- rnorm(length(release_dataset$orig_amt), \n               mean = 0, sd = 1000)  \n\n# Scaling loan amounts and add noise\nrelease_dataset$loan_amount &lt;- release_dataset$orig_amt * scaling_factor + \n                               noise\n\n# Round scaled loan amounts (optional)\nrelease_dataset$loan_amount &lt;- round(release_dataset$loan_amount)\n\nrelease_dataset &lt;- release_dataset |&gt; \n                   select(-orig_amt)\n\n\n\nOriginal loan amount is a distinct value unique to each borrower. This uniqueness can make it relatively easy to cross-reference with other datasets.\nScaling normalizes the loan amounts to a standard range and maintains a relative distribution, making it harder to link the values back to the original amounts directly. Further, rounding the scaled loan amounts to reduce precision. Lastly, adding noise to to introduce random variation in loan amounts.\nThis approach ensures that the original loan amounts are sufficiently anonymized, reducing the risk of re-identification. It further strikes a balance between protecting borrower privacy and providing valuable insights into loan performance.\n\n\n\nAggregating the Original Interest Rate & Last Rate of Interest\n\n\nCode\n# Creating bins and mapping values to bins\nrelease_dataset &lt;- release_dataset |&gt;\n                    mutate(interest_rate = cut(orig_rt, breaks = c(0,2,4,6,8)), \n                          last_rate = cut(last_rt, c(0,2,4,6,8))) |&gt;     \n                    select(-orig_rt, -last_rt)   \n\n\n\nThe loan rate is a specific financial detail and without de-identification, the loan rate, when combined with other available data (such as loan amount, date, and borrower demographics), can significantly increase the risk of re-identifying individual borrowers.\nBinning groups loan rates into broader categories, which reduces the precision of the data while still preserving important patterns and trends. It masks individual loan rates within a range, enhancing privacy while allowing for meaningful analysis.\n\n\n\nCode\n# Anonymize seller\nunique_sellers &lt;- unique(release_dataset$seller)\nanonymized_sellers &lt;- paste(\"Seller\", seq_along(unique_sellers))\n\n# Mapping anonymized sellers names to original seller names.\nseller_mapping &lt;- setNames(anonymized_sellers, unique_sellers)\n\nrelease_dataset$seller_anon &lt;- seller_mapping[release_dataset$seller]\n\nrelease_dataset &lt;- release_dataset |&gt; \n                   select(-seller)\n\n\n\nWhile the seller can provide valuable insights, it can also act as an indirect identifier. Knowing the seller can sometimes lead to identification of specific loans or groups of loans further bearing implications for competition and business confidentiality.\nAnonymizing ensures that the seller’s identity cannot be reconstructed or traced back."
  },
  {
    "objectID": "posts/comm_data/index.html",
    "href": "posts/comm_data/index.html",
    "title": "Breaking Down Different Media Formats",
    "section": "",
    "text": "Australians say AI shouldn’t produce political news, but its OK for sport\nThe article studied here is from The Conversation, titled “Australians say AI shouldn’t produce political news, but its OK for sport: new research”, available online via The Conversation Website.\nThe article is best described as “online news media” due to its exclusive online publication utilizing digital format with interactive features, engaging readers through online channels. It highlights academic research and expert views, which sets it apart from traditional print media, despite the traditional journalistic standards it upholds.\nThe article summarizes that based on the Digital News Report 2024, Australians generally prefer news created by humans over that produced by AI, although their acceptance depends on the type of news. People are more open to AI-generated content in sports and entertainment than in political or crime news. Trust in news sources and familiarity with AI are crucial factors in shaping public attitudes, highlighting the importance for news organizations to openly disclose their use of AI to preserve audience trust.\nWhile the main background information is presented in the first three paragraphs. The article’s background offers crucial context by outlining the growing role of AI in news media and and highlighting the key question about public attitudes toward this shift. This background is crucial appreciating the study’s findings, which explore how Australians perceive AI-generated news in different domains. This context helps explain why the research into public opinion on AI’s role in news is significant and how it reflects broader concerns about media authenticity and quality.\nThe lede in this article is in the title “Australians say AI shouldn’t produce political news, but it’s OK for sport”. It presents the prime findings of the article and captures interest by emphasizing a significant insight from the research. This is an unusual placement for lede, as it is usually placed in the opening paragraph but it’s increasingly typical for articles that are published exclusively online, where publishers aim for clicks mainly through their headlines.\nThe summary of the article can be found after the background in the fourth paragraph.\n\n“The online survey of 2,003 adult Australians finds they are much more comfortable with sport (31%) and entertainment (26%) news being produced mainly by AI than with politics (19%) and crime (21%) news being produced the same way (see the chart below).”\n\nThe summary effectively reveals that Australians are generally more comfortable with AI-generated news in areas like sports (31%) and entertainment (26%) than in politics (19%) and crime (21%), highlighting how public acceptance of AI in news production varies by topic. By presenting specific data, it effectively highlights the research’s key result in a concise manner.\nThe main result of the article is present in the upper middle section of the article:\n\n“Overall, 59% say they feel uneasy about news produced by AI with human oversight. This is compared to 28% who say they are uncomfortable about news that is mainly produced by human journalists with AI assistance.”\n\nThe study’s main finding reveals that 59% of people are uneasy with AI-produced news overseen by humans, while 28% are uncomfortable with news primarily created by human journalists with AI assistance. This key finding is placed in the middle of the article keeping readers engaged by ensuring they see the key finding early on, even if they don’t finish the entire piece. This allows for further context and analysis to be built around the result, prompting readers to consider its importance as they continue reading.\n\n\nDear Mona, What’s The Most Common Name In America?\nThe article studied here is from FiveThirtyEight (ABC News), titled “Dear Mona, What’s The Most Common Name In America?”, available online via the ABC News Website.\nThis piece is best categorized as a “blog post” based on its online publication and format. Its digital format includes interactive graphics and a conversational tone, distinguishing it from print media. The article’s use of hyperlinks and a comment section highlights its role as an online blog intended for interactive engagement with readers.\nThe article investigates which full names are most prevalent in the U.S., focusing on whether “John Smith” is the top name. The authors use data from the Social Security Administration for first names and the Census Bureau for surnames, adjusting for factors like mortality and immigration. Their analysis shows that simple probability calculations are misleading, revealing that “James Smith” is more common than “John Smith” when demographic factors are taken into account.\nThe background of the article, provided in the initial four paragraphs, explores the challenge of determining the most common full names in the U.S. It starts by questioning whether “John Smith” is truly the most frequent name and discusses the complications of integrating first and last name databases. The authors describe their approach, which includes adjusting Social Security Administration data for mortality and factoring in names of immigrants. This context is essential as it highlights the complexities involved in analyzing name frequency and explains the need for advanced methodologies, preparing the reader for the detailed findings and conclusions presented in the article.\nThe lede in this article is in the opening question “Is John Smith really the most common name?”:\n\n“What are the most common first- and last-name combinations in the United States? Is John Smith really the most common name?”\n\nThe lede effectively introduces the main question of the article, focusing on determining the most common name combinations in the U.S., particularly whether “John Smith” is the most prevalent. It captures the reader’s interest by addressing a widespread curiosity about name popularity and sets the stage for discussion that follows. The lede is distinctively positioned as a standalone question right at the beginning of the article, separate from the main text. This unusual format employs a direct question to immediately capture the reader’s interest and outline the article’s key focus.\nThe summary of the article is positioned early, in the second paragraph.\n\n“To get you an answer, my colleague Andrew Flowers and I tried a more sophisticated technique that reached a different conclusion: We think the most common name in America might very well be James Smith”\n\nThis provides a concise and straightforward summary of the article’s findings. The author promptly introduces the main conclusion, questioning the common belief that “John Smith” is the most prevalent name and proposing that “James Smith” might actually take that spot. This early summary offers readers a quick glimpse into the results of the article’s exploration. Presenting the key conclusion at the start grabs readers’ attention and sets a clear direction for the article. It helps readers grasp the main point early, enabling them to better understand and engage with the detailed analysis and evidence presented later. This method effectively frames the article’s investigation and aligns readers’ expectations with the forthcoming detailed findings.\nThe main result can be found later in the article, in the second last paragraph.\n\n“As a result, “Michael Smith” dropped down from the most likely name in America to second place and was bypassed by “James Smith.” There were more dramatic changes, too. For example, according to independent probabilities, “Maria Garcia” was expected to be the 354th most common full name (and “Maria Smith” would rank as No. 74). But according to Hartman’s data set, “Maria” and “Garcia” correlate nearly 700 percent more than you’d expect. That means “Maria Garcia” skyrockets to the 15th most common name combination overall. (We’ve posted all the data used in our analysis to our GitHub page.)”\n\nThe finding that “James Smith” might be the most common name in the U.S., rather than “John Smith,” is presented after a thorough explanation of the methodology. This positioning helps establish a strong basis for its findings. This sequence allows readers to grasp the thorough analysis supporting the result, making the conclusion more credible and impactful. Revealing the result after explaining the process ensures that the conclusion is seen as well-supported and logically derived.\n\n\nHow did Australia’s housing market get so bad, and is it all negative gearing’s fault?\nThe video studied here is from The Guardian, titled “How did Australia’s housing market get so bad, and is it all negative gearing’s fault?”, available online via Youtube on the Guardian Australia’s channel.\nThe video “How did Australia’s housing market get so bad, and is it all negative gearing’s fault?” is a form of “digital media”, specifically an explanatory or educational video. Found on platforms like YouTube, it uses visuals such as charts and graphics along with spoken narration to educate viewers on complex subjects like the housing market and negative gearing, setting it apart from traditional print or blog content.\nThe video examines how the reduction in capital gains tax in 1999, coupled with negative gearing, has intensified the crisis in Australia’s housing market by encouraging investors to take on greater risks and push property prices higher. It shows how these policy changes have led to a vicious cycle where house prices continually rise, benefiting investors while negatively impacting potential home buyers and the broader economy.\nThe background of this video is provided from (0:06 – 1:27). It features a chart (0:06 – 0:20) comparing Australian disposable income with housing prices, which demonstrates the widening gap between earnings and property costs, and a discussion of negative gearing (0:20 – 1:27), explaining how investors can offset property investment losses against their taxable income. This information is essential for understanding the housing market crisis, as the chart shows the extent of the affordability problem, while the negative gearing explanation reveals how this practice contributes to rising property prices and worsens the crisis. Together, these elements provide a solid basis for analyzing the current issues in the housing market.\nThe lede in the video is present at the following time stamp: (0:00 - 0:06). The lede introduces the video with an engaging statement: “You know what’s cool starting a video with a chart to show how cooked the Australian housing market is,” setting up the viewer’s expectations by focusing on the dramatic state of the housing market with a compelling visual. This approach is relevant as it captures attention and establishes the central theme of the video, highlighting the severity of the crisis and piquing interest in the underlying issues and policies contributing to the current situation. In contrast, Print media and blog posts typically use a straightforward written lede that introduces the topic concisely, relying solely on text to engage the reader.\nThe summary of the video can be found in the segment (3:11 – 4:04). The summary outlines how the 1999 cut in capital gains tax, combined with negative gearing, has worsened issues in Australia’s housing market. It describes how these factors have created a cycle of rising property prices that encourages more investment and intensifies the affordability crisis. The summary, located near the end of the video, consolidates earlier information and provides an overview of how specific policies have led to the current crisis. This placement helps viewers grasp the combined impact of the factors before the video presents the main takeaway and broader implications.\nThe main result appears at the end of the video (4:04 – 4:58). The main result identifies the 1999 reduction in capital gains tax, coupled with negative gearing, as a major factor worsening the Australian housing market crisis. Positioned at the end of the video, this section highlights the culmination of the factors discussed and their significant effect on the housing market. By placing it here, the video reinforces the central finding after a thorough analysis and summary, ensuring viewers grasp the primary cause and its wider implications.\n\n\nCombined Reflection\n\nVideo\n\nThe format focuses on visual engagement and dynamic presentation. Videos use visuals and audio to simplify complex information, making it more engaging for viewers by incorporating charts, graphs, and narration. This mix of visuals, voice-over, and text creates an interactive, multi-sensory experience that often conveys information more effectively than text. This approach captures and maintains audience interest through interactive elements, clarifies abstract ideas, and provides a brief, easily digestible summary, which is perfect for viewers who want a quick explanation without getting into lengthy texts. Its effectiveness is captured in engagement, clarity and time efficiency.\n\nPrint Media\n\nIn Print Media, the format designs the articles to provide a deep dive into topics, offering comprehensive analysis and rich context. This format is ideal for detailed reporting and in-depth discussions, as it organizes information systematically, following a clear journalistic structure. It allows readers to thoroughly explore subjects, making it suitable for those looking for a more profound understanding. Moreover, print articles act as a lasting resource that can be revisited, catering to readers who value detailed, well-thought-out arguments over concise summaries.\n\nBlog Post\n\nBlogs are designed for accessibility by integrating text with visuals like info-graphics and charts, making the content both engaging and easy to understand. This format effectively serves both quick reads and detailed content, utilizing interactive features like hyperlinks and multimedia to boost engagement and offer additional context. By balancing text and visuals, blogs capture readers’ interest and present data in a more accessible way. They are designed for rapid consumption, catering to those who favor concise yet informative content, while their flexible, conversational style makes complex subjects more relatable for a broader audience."
  },
  {
    "objectID": "posts/assign04-cuscus/index.html",
    "href": "posts/assign04-cuscus/index.html",
    "title": "Do different sensors at the same location report the same values?",
    "section": "",
    "text": "Pollutants with the strongest evidence for public health concern include particulate matter (PM), sulfur dioxide (SO2), ozone (O3), nitrogen dioxide (NO2) and carbon monoxide (CO)\n\nParticulate Matter (pm25, pm10): consists of tiny particles in the air that can be inhaled. pm25 refers to particles smaller than 2.5 micrometers, while pm10 includes larger particles up to 10 micrometers. These particles originate from various sources, such as traffic emissions, industrial activities, construction sites, and natural sources like pollen and dust. Because they are small enough to penetrate deep into the lungs and even enter the bloodstream, high concentrations of pm25, pm10 are associated with poor air quality and can lead to serious health issues, including respiratory and cardiovascular problems.\nSulfur dioxide (so2) is a colorless gas that is readily soluble in water. It mainly comes from the combustion of fossil fuels for domestic heating, industries and power generation. Exposure to SO2 is associated with asthma hospital admissions and emergency room visits.\nOzone (o3) is a key ingredient in smog, created when sunlight reacts with pollutants like volatile organic compounds, carbon monoxide, and nitrogen oxides from vehicles and industries. Because it forms in sunlight, ozone levels are often highest on sunny days. Additionally, some household devices, like portable air cleaners, can also produce ozone. Exposure to excessive ozone can cause problems breathing, trigger asthma, reduce lung function and lead to lung disease.\nNitrogen dioxide (no2) is a reddish-brown gas that is soluble in water, and a strong oxidant. It mainly comes from burning fuels in heating, transportation, industry, and power generation, as well as from household appliances like furnaces and gas stoves. Exposure to NO₂ can irritate the airways and worsen respiratory diseases. It also contributes to ozone formation, linking it to asthma and other respiratory issues.\nCarbon monoxide (co) is a colorless, odorless gas produced from the incomplete burning of fuels like wood, petrol, coal, and natural gas. The main source of CO in the air is motor vehicles. Carbon monoxide diffuses across the lung tissues and into the bloodstream, making it difficult for the body’s cells to bind to oxygen. This lack of oxygen damages tissues and cells. Exposure to carbon monoxide can cause difficulties breathing, exhaustion, dizziness, and other flu-like symptoms. Exposure to high levels of carbon monoxide can be deadly.\n\nDo different sensors at the same location report the same values?\nThe question being assessed focuses on analyzing if sensors placed within close proximity report same values. Analyzing data from these varied suburban and urban sensors helps to determine if air quality measurements are consistent across these distinct environments or if local geographic and environmental factors lead to significant differences in reported values. Such insights will clarify how air quality varies across coastal and urban settings, helping to assess whether sensor readings can be directly compared or if adjustments are needed for accurate regional air quality assessments. This can improve the relevance of public health responses and policies specific to each area’s unique conditions.\nExpectations:\n\nSensor data from locations like Melbourne are expected to show higher pollution levels compared to areas like Geelong South, which is closer to the coast.\nA positive correlation is anticipated between pollutant levels, where an increase in one pollutant is likely to correspond with increases in others.\nWeekday and weekend trends in pollutant levels are expected to differ, potentially reflecting variations in daily activities and emissions.\n\nLimitations:\n\nNot all locations have sensors that measure every pollutant. Most sensors capture data for only two pollutants, with exceptions like Alphington and Geelong South, which measure a broader range.\nThere may be significant missing data for certain pollutants, which could affect analysis outcomes.\nThe varying scales and units of pollutants may complicate direct comparisons.\nAs the data originates from third-party organizations and government agencies, there could be occasional concerns about data accuracy and consistency."
  },
  {
    "objectID": "posts/assign04-cuscus/index.html#introduction",
    "href": "posts/assign04-cuscus/index.html#introduction",
    "title": "Do different sensors at the same location report the same values?",
    "section": "",
    "text": "Pollutants with the strongest evidence for public health concern include particulate matter (PM), sulfur dioxide (SO2), ozone (O3), nitrogen dioxide (NO2) and carbon monoxide (CO)\n\nParticulate Matter (pm25, pm10): consists of tiny particles in the air that can be inhaled. pm25 refers to particles smaller than 2.5 micrometers, while pm10 includes larger particles up to 10 micrometers. These particles originate from various sources, such as traffic emissions, industrial activities, construction sites, and natural sources like pollen and dust. Because they are small enough to penetrate deep into the lungs and even enter the bloodstream, high concentrations of pm25, pm10 are associated with poor air quality and can lead to serious health issues, including respiratory and cardiovascular problems.\nSulfur dioxide (so2) is a colorless gas that is readily soluble in water. It mainly comes from the combustion of fossil fuels for domestic heating, industries and power generation. Exposure to SO2 is associated with asthma hospital admissions and emergency room visits.\nOzone (o3) is a key ingredient in smog, created when sunlight reacts with pollutants like volatile organic compounds, carbon monoxide, and nitrogen oxides from vehicles and industries. Because it forms in sunlight, ozone levels are often highest on sunny days. Additionally, some household devices, like portable air cleaners, can also produce ozone. Exposure to excessive ozone can cause problems breathing, trigger asthma, reduce lung function and lead to lung disease.\nNitrogen dioxide (no2) is a reddish-brown gas that is soluble in water, and a strong oxidant. It mainly comes from burning fuels in heating, transportation, industry, and power generation, as well as from household appliances like furnaces and gas stoves. Exposure to NO₂ can irritate the airways and worsen respiratory diseases. It also contributes to ozone formation, linking it to asthma and other respiratory issues.\nCarbon monoxide (co) is a colorless, odorless gas produced from the incomplete burning of fuels like wood, petrol, coal, and natural gas. The main source of CO in the air is motor vehicles. Carbon monoxide diffuses across the lung tissues and into the bloodstream, making it difficult for the body’s cells to bind to oxygen. This lack of oxygen damages tissues and cells. Exposure to carbon monoxide can cause difficulties breathing, exhaustion, dizziness, and other flu-like symptoms. Exposure to high levels of carbon monoxide can be deadly.\n\nDo different sensors at the same location report the same values?\nThe question being assessed focuses on analyzing if sensors placed within close proximity report same values. Analyzing data from these varied suburban and urban sensors helps to determine if air quality measurements are consistent across these distinct environments or if local geographic and environmental factors lead to significant differences in reported values. Such insights will clarify how air quality varies across coastal and urban settings, helping to assess whether sensor readings can be directly compared or if adjustments are needed for accurate regional air quality assessments. This can improve the relevance of public health responses and policies specific to each area’s unique conditions.\nExpectations:\n\nSensor data from locations like Melbourne are expected to show higher pollution levels compared to areas like Geelong South, which is closer to the coast.\nA positive correlation is anticipated between pollutant levels, where an increase in one pollutant is likely to correspond with increases in others.\nWeekday and weekend trends in pollutant levels are expected to differ, potentially reflecting variations in daily activities and emissions.\n\nLimitations:\n\nNot all locations have sensors that measure every pollutant. Most sensors capture data for only two pollutants, with exceptions like Alphington and Geelong South, which measure a broader range.\nThere may be significant missing data for certain pollutants, which could affect analysis outcomes.\nThe varying scales and units of pollutants may complicate direct comparisons.\nAs the data originates from third-party organizations and government agencies, there could be occasional concerns about data accuracy and consistency."
  },
  {
    "objectID": "posts/assign04-cuscus/index.html#data-description",
    "href": "posts/assign04-cuscus/index.html#data-description",
    "title": "Do different sensors at the same location report the same values?",
    "section": "Data description",
    "text": "Data description\nData Source\nOpenAQ is an open-source platform that aggregates real-time air quality data from governmental and research sources worldwide. Using sensors managed by environmental agencies, OpenAQ collects pollutant measurements such as PM2.5, ozone, and nitrogen dioxide. The data is standardized and made accessible on their platform, enabling researchers, policymakers, and the public to monitor air quality, track trends, and support efforts to enhance environmental health globally.\nLicense\nOpenAQ provides air quality data aggregated from multiple sources, each with its own licensing terms. While some data might be available under open licenses such as Creative Commons (e.g., CC0), others may require attribution, restrict commercial use, or have specific conditions as determined by the original providers. OpenAQ includes these license details to ensure users comply with any necessary terms when accessing or using the data.\nData\nThis dataset captures air quality measurements across various locations in Australia, focusing on pollutant levels such as particulate matter (e.g., PM10, PM2.5) over time. It provides spatial and temporal data, including the concentration of pollutants, the geographic location of sampling sites, and time-stamps for when each measurement was recorded. The data allows for a detailed analysis of air quality trends, helping to assess pollution levels and their potential impacts on public health. By studying these pollutant concentrations over time and location, it becomes possible to identify pollution hot spots, track seasonal variations, and evaluate overall compliance with air quality standards.\nThe dataset contains 36,326 rows and 9 columns. The variables in the dataset are as follows:\n\n\nCode\ndata_description &lt;- data.frame(\n  Variable = c(\"location_id\", \"location\", \"parameter\", \"value\", \"date_utc\", \"unit\", \"lat\", \"long\", \"country\"),\n  Description = c(\n    \"Unique identifier for each location.\",\n    \"Name of the location where measurements were taken.\",\n    \"Type of pollutant measured: PM10, PM25, SO2, NO2, CO, O3\",\n    \"Recorded value of the pollutant.\",\n    \"Timestamp in UTC for the recorded measurement\",\n    \"Measurement unit, primarily µg/m³ and ppm\",\n    \"Latitude co-ordinates for the location\",\n    \"Longitude co-ordinates for the location\",\n    \"Country code (AU for all records)\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nkable(data_description, caption = \"Description of Data Variables\")\n\n\n\nDescription of Data Variables\n\n\nVariable\nDescription\n\n\n\n\nlocation_id\nUnique identifier for each location.\n\n\nlocation\nName of the location where measurements were taken.\n\n\nparameter\nType of pollutant measured: PM10, PM25, SO2, NO2, CO, O3\n\n\nvalue\nRecorded value of the pollutant.\n\n\ndate_utc\nTimestamp in UTC for the recorded measurement\n\n\nunit\nMeasurement unit, primarily µg/m³ and ppm\n\n\nlat\nLatitude co-ordinates for the location\n\n\nlong\nLongitude co-ordinates for the location\n\n\ncountry\nCountry code (AU for all records)\n\n\n\n\n\nData Checking and Cleaning Methods\n\nData Type Validation: - It is necessary to ensure that each column has the correct data type. For example, date_utc should be in datetime format for time series analysis.\nHandling Missing or Null values: Although no null values are detected, there may still be missing or erroneous data entries. It’s essential to review columns like value for potential issues like zero or extreme values that may indicate faulty measurements. Further, we might be able to discover missing values once the data is transformed into tidy form.\nDuplicate Records: Given that location_id and date_utc might uniquely identify each record, checking duplicates based on these columns to prevent redundant data.\nOutlier Detection: The value column could contain outliers. Outliers could be identified using statistical methods like IQR analysis. This is particularly important in environmental data to avoid skewed analyses.\nUnit Consistency: Ensure all entries use consistent units for accurate analysis.\nLocation Summary: Check the number of observations at each location to determine if it is consistent for the selected time period.\nParameter Distribution: Check for consistency in number of parameters measured at each location.\n\n\n\n\nCode\n# download data and save as csv file\n#install.packages(\"devtools\")\n#devtools::install_github(\"numbats/airpurifyr\")\n#set_openaq_api_key(\"\")\n\n#melbourne &lt;- get_measurements_for_location(country = \"AU\", \n#                                           city = \"Melbourne\",\n#                                           date_from =as.Date(\"2024-07-01\"), \n#                                           date_to = as.Date(\"2024-09-01\"))\n\n#geelong &lt;- get_measurements_for_location(country = \"AU\", \n#                                       city = \"Geelong\",\n#                                       date_from = as.Date(\"2024-07-01\"), \n#                                       date_to = as.Date(\"2024-09-01\"))\n\n\n#write.csv(melbourne, file = \"melbourne.csv\", row.names = FALSE)\n#write.csv(geelong, file = \"geelong.csv\", row.names = FALSE)\n\n\n#combine data\n#options(scipen = 999)\n#original_df &lt;- bind_rows(melbourne, geelong)\n#write.csv(original_df, file = \"data/original_df.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/assign04-cuscus/index.html#initial-data-analysis",
    "href": "posts/assign04-cuscus/index.html#initial-data-analysis",
    "title": "Do different sensors at the same location report the same values?",
    "section": "Initial data analysis",
    "text": "Initial data analysis\n\n\nLoad data\n\n\nCode\n#Load data\noriginal_df &lt;- read_csv(\"data/original_df.csv\")\n\n\nPopulation\n\nThe population of interest in this context would be all sensors deployed at the same location that measure the same type of pollutants over a given period of time.\n\n\n\nData Screening\nEvaluating Data Type Composition\n\n\nCode\nvis_dat(original_df)\n\n\n\n\n\n\n\n\n\n\nUnderstanding the composition of variable types in the dataset using ‘vis_dat’. Here, the dataset primarily comprises of character type variables, numeric variables, POSIXct and POSIXt variables used to efficiently represent date-time objects.\nBased on observing the dataset, it appears that not all sensors measure all pollutants, hence once in tidy format we will be able to explore the missing values.\n\nChecking for Duplicate Rows\n\n\nCode\n# Detecting duplicate rows\nsum(duplicated(original_df))\n\n\n[1] 0\n\n\nChecking Unit Consistency\n\n\nCode\nunique(original_df$unit)\n\n\n[1] \"µg/m³\" \"ppm\"  \n\n\nEvaluating the structure of Data\n\nglimpse(original_df)\n\nRows: 36,326\nColumns: 9\n$ location_id &lt;dbl&gt; 10494, 10494, 10494, 10494, 10494, 104…\n$ location    &lt;chr&gt; \"Alphington\", \"Alphington\", \"Alphingto…\n$ parameter   &lt;chr&gt; \"pm25\", \"pm25\", \"pm25\", \"pm25\", \"pm25\"…\n$ value       &lt;dbl&gt; 2.68, 5.50, 3.76, 1.61, 1.88, 3.80, 5.…\n$ date_utc    &lt;dttm&gt; 2024-09-01 00:00:00, 2024-08-31 23:00…\n$ unit        &lt;chr&gt; \"µg/m³\", \"µg/m³\", \"µg/m³\", \"µg/m³\", \"µ…\n$ lat         &lt;dbl&gt; -38, -38, -38, -38, -38, -38, -38, -38…\n$ long        &lt;dbl&gt; 145, 145, 145, 145, 145, 145, 145, 145…\n$ country     &lt;chr&gt; \"AU\", \"AU\", \"AU\", \"AU\", \"AU\", \"AU\", \"A…\n\n\n\nThe glimpse function provides an overview of a dataset by displaying each variable’s name, its data type, and a sample of the initial values. This allows for a quick examination of the dataset’s structure, making it easier to confirm data types and gain an immediate understanding of the data.\n\nCreating a location Summary\n\n\nCode\nlocation_summary &lt;- original_df |&gt;\n  group_by(location) |&gt;\n  summarise(n = n())\n\nkable(location_summary, \n      caption = \"Location Summary\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nLocation Summary\n\n\nlocation\nn\n\n\n\n\nAlphington\n8507\n\n\nBox Hill\n1445\n\n\nBrighton\n1391\n\n\nBrooklyn\n2817\n\n\nDandenong\n5697\n\n\nFootscray\n2492\n\n\nGeelong South\n7829\n\n\nMacleod\n1437\n\n\nMelbourne CBD\n1474\n\n\nMooroolbark\n3237\n\n\n\n\n\nCode\n# Number of parameters in each location\nparameter_count &lt;- original_df |&gt;\n  group_by(location) |&gt; \n  summarise(no_parameters = n_distinct(parameter))\n\nkable(parameter_count, \n      caption = \"Number of Parameters in each Location\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nNumber of Parameters in each Location\n\n\nlocation\nno_parameters\n\n\n\n\nAlphington\n6\n\n\nBox Hill\n1\n\n\nBrighton\n1\n\n\nBrooklyn\n2\n\n\nDandenong\n4\n\n\nFootscray\n2\n\n\nGeelong South\n6\n\n\nMacleod\n1\n\n\nMelbourne CBD\n1\n\n\nMooroolbark\n3\n\n\n\n\n\n\nBased on the location summary, we observe inconsistencies in sensor readings across the dataset’s time period. Alphington and Geelong South each have approximately 8,000 observations, while Dandenong records around 6,000, with other locations having significantly fewer observations. This suggests that not all sensors capture data for every pollutant. Further, on exploring the number of parameters detected at each sensor location we can confirm that only Alphington and Geelong South detect all six pollutants. Hence, filtering the data to include locations with sufficient observations for all 6 pollutants.\n\n\n\nCode\n#Filter data of \"Alphington\" and \"Geelong South\" in July and August:\nalp_gee &lt;- original_df |&gt;\n  filter(location %in% c(\"Alphington\", \"Geelong South\"), \n         date_utc != as.POSIXct(\"2024-09-01 00:00:00\", tz = \"UTC\")) |&gt;\n  arrange(date_utc)\n\n\n\n\nCode\n#Converting data to wider form to explore missingness \ndata_wider &lt;- alp_gee |&gt; \n             select(-unit)  |&gt;\n           pivot_wider(names_from = parameter,\n                       values_from = value)\n\n\n\n\nMissing Data Evaluation\n\n\nCode\n# Calculating the missing percentage for each location\nmissing_summary &lt;- data_wider |&gt; \n  summarise(across(everything(), ~ sum(is.na(.)) / n() *100)) |&gt; \n  select( -location, -location_id, -date_utc, -lat, -long, -country) |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"missing_percentage\") \n\n#Plotting the missing percentage for each variable\nggplot(missing_summary, \n       aes(x = reorder(variable,-missing_percentage), \n           y = missing_percentage))  +\n  geom_segment(aes(xend = variable, yend = 0), color = \"blue\") +\n  geom_point(size = 2, color = \"black\") +\n  coord_flip() +\n  labs(title = \"Missing Data Percentage\",\n       x = \"Pollutant\",\n       y = \"Missing Percentage (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBased on the above plot, we can infer that the data is relatively complete with minimal missing values except for no2, which shows around 26% missing values. Further, exploring the missing data by location.\n\nExploring Missing Data by Location\n\n\nCode\n# Calculating the missing percentage for each location\nmissing_summary &lt;- data_wider |&gt; \n  filter(location == \"Alphington\") |&gt;\n  summarise(across(everything(), ~ sum(is.na(.)) / n() *100)) |&gt; \n  select( -location, -location_id, -date_utc, -lat, -long, -country) |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"missing_percentage\") \n\n#Plotting the missing percentage for each variable\np1 &lt;- ggplot(missing_summary, \n       aes(x = reorder(variable,-missing_percentage), \n           y = missing_percentage))  +\n  geom_segment(aes(xend = variable, yend = 0), color = \"blue\") +\n  geom_point(size = 2, color = \"black\") +\n  coord_flip() +\n  labs(title = \"Missing Data Percentage - Alphington\",\n       x = \"Pollutant\",\n       y = \"Missing Percentage (%)\") +\n  theme_minimal()\n\n# Calculating the missing percentage for each location\nmissing_summary &lt;- data_wider |&gt; \n  filter(location == \"Geelong South\") |&gt;\n  summarise(across(everything(), ~ sum(is.na(.)) / n() *100)) |&gt; \n  select( -location, -location_id, -date_utc, -lat, -long, -country) |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"missing_percentage\") \n\n#Plotting the missing percentage for each variable\np2 &lt;- ggplot(missing_summary, \n       aes(x = reorder(variable,-missing_percentage), \n           y = missing_percentage))  +\n  geom_segment(aes(xend = variable, yend = 0), color = \"blue\") +\n  geom_point(size = 2, color = \"black\") +\n  coord_flip() +\n  labs(title = \"Missing Data Percentage - Geelong South\",\n       x = \"Pollutant\",\n       y = \"Missing Percentage (%)\") +\n  theme_minimal()\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\nThe data from Alphington is nearly complete, with less than 5% of values missing. In contrast, Geelong South has relatively complete data for most pollutants, with missing values ranging between 0% and 5%. However, for pm10, approximately 10% of data is missing, and for no2, there is a significant gap with around 45% of data missing.\n\nSpecific Date Analysis of Missing Data\n\n\nCode\nalp_gee_ts &lt;- data_wider |&gt;\n  as_tsibble(key = location, index = date_utc)\n\nalp &lt;- alp_gee_ts|&gt; \n  filter(location == \"Alphington\")\n\ncount_gaps(alp)\n\n\n# A tibble: 4 × 4\n  location   .from               .to                    .n\n  &lt;chr&gt;      &lt;dttm&gt;              &lt;dttm&gt;              &lt;int&gt;\n1 Alphington 2024-07-03 13:00:00 2024-07-03 13:00:00     1\n2 Alphington 2024-07-15 05:00:00 2024-07-15 05:00:00     1\n3 Alphington 2024-07-18 22:00:00 2024-07-19 07:00:00    10\n4 Alphington 2024-08-15 10:00:00 2024-08-15 10:00:00     1\n\n\nCode\ngee &lt;- alp_gee_ts |&gt; \n  filter(location == \"Geelong South\") \n\ncount_gaps(gee)\n\n\n# A tibble: 4 × 4\n  location     .from               .to                    .n\n  &lt;chr&gt;        &lt;dttm&gt;              &lt;dttm&gt;              &lt;int&gt;\n1 Geelong Sou… 2024-07-03 13:00:00 2024-07-03 13:00:00     1\n2 Geelong Sou… 2024-07-18 22:00:00 2024-07-19 07:00:00    10\n3 Geelong Sou… 2024-08-15 10:00:00 2024-08-15 10:00:00     1\n4 Geelong Sou… 2024-08-22 01:00:00 2024-08-22 03:00:00     3\n\n\nBetween July and August 2024, both Alphington and Geelong South experienced several instances of missing data, with some overlaps. Both locations had a one-hour gap on July 3rd at 13:00 and a 10-hour gap from July 18th at 22:00 to July 19th at 07:00. Additionally, Alphington had gaps on July 15th at 05:00 and August 15th at 10:00, while Geelong South also missed data on August 15th at 10:00. Geelong South experienced a unique three-hour gap on August 22nd from 01:00 to 03:00.\nAnalysis of Possible Factors Contributing to Missing Data\nBased on information from the OpenAQ documentation and GitHub resources, potential causes of data gaps across different sensor locations may include:\n\nMonitoring stations typically measure specific pollutants based on local priorities, regulations, and resources, leading to incomplete data coverage for certain pollutants across regions.\nOpenAQ uses both reference monitors and low-cost sensors. While low-cost sensors expand coverage, they may inconsistently measure pollutants like PM10 due to calibration and environmental factors, resulting in data gaps.\nOpenAQ collects data from various government and third-party sources, each with different reporting practices. Changes in data-sharing agreements, or provider disruptions can cause temporary data gaps for certain pollutants.\nTechnical disruptions, such as routine sensor maintenance, calibration, network outages, power failures, and equipment malfunctions, can temporarily pause data transmission, leading to missing data.\n\nWhile the above offer a general overview of data handling and reasons for data gaps on the OpenAQ platform. For specific details on missing data, consulting the relevant data provider or monitoring station is often necessary, as they may have insights into the cause.\n\n\nDetecting Outliers\n\n\nCode\nggplot(alp_gee |&gt; filter(parameter == c(\"pm10\", \"pm25\")) , aes(x = value)) +      geom_boxplot() + \n  facet_wrap(~parameter ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFrom the boxplots we can deduce the following:\n\npm10 has a wider spread and greater variability compared to pm25. Both pollutants have a narrow IQR, indicating low variability within the central 50% of values, with most data concentrated near the median. However, both also have numerous outliers, which are more extreme in pm10 The spike in pm10 levels may result from events like dust storms, construction, or excessive road dust, while the elevated pm25 levels likely stem from combustion sources such as vehicle exhaust during peak traffic hours.\n\n\n\nCode\nggplot(alp_gee |&gt; filter(!(parameter %in% c(\"pm10\", \"pm25\"))), aes(x = value)) + \n       geom_boxplot() + \n  facet_wrap(~parameter, scales = \"free_x\" ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThe co boxplot shows a narrow IQR, indicating low variability within the central 50% of data. The short whiskers and numerous outliers (up to about 1.5 ppm) suggest a concentrated distribution around the median but substantial variability in extreme values, suggesting occasional spikes in co concentration.\nThe no2 boxplot reveals a narrow IQR, signifying low variability within the central 50% of data. The whiskers extend just beyond the box, with outliers reaching up to 0.06 ppm, indicating occasional peaks in no2 concentrations and moderate variability in extreme values, though not as pronounced as for co.\nThe o3 boxplot shows a relatively wide IQR, indicating moderate variability within the central 50% of the data. The median is slightly off-center, and the whiskers are slightly unequal, suggesting a slight asymmetry in the distribution. The absence of outliers indicates that o3 values are spread consistently within the range, with no extreme deviations, leading to a stable variability overall.\nThe so2 boxplot shows a narrow IQR, similar to co and no2, signifying a low variability in the central 50% of values. The short whiskers and presence of outliers up to 0.008 ppm suggest a concentrated distribution around the median, with occasional high values adding to overall variability.\nThe outliers in each of the pollutant may be attributed to factors like emissions from combustion, peak traffic periods, seasonal variations (holiday, non-holiday period), weather conditions like high humidity, natural events like wildfires or could possibly be anomalies in measurements. These would provide additional perspective while doing further analysis and best to retain them.\n\nSummary mean/median of each measurements\n\n\nCode\noptions(scipen = 999)\n\nskim_summary &lt;- data_wider |&gt; \n  skim(pm25, pm10, so2, o3, no2, co) \n\nskim_summary &lt;- skim_summary |&gt; \n  select(-c(n_missing, numeric.hist, complete_rate))\n  \n\nskim_summary\n\n\n\nData summary\n\n\nName\ndata_wider\n\n\nNumber of rows\n2946\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\nskim_variable\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\npm25\n6.82\n6.96\n0.00\n2.67\n4.96\n8.60\n63.01\n\n\npm10\n19.83\n17.85\n0.00\n9.66\n14.88\n23.32\n188.09\n\n\nso2\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n\n\no3\n0.02\n0.01\n0.00\n0.01\n0.02\n0.03\n0.04\n\n\nno2\n0.01\n0.01\n0.00\n0.00\n0.01\n0.01\n0.07\n\n\nco\n0.21\n0.18\n0.04\n0.11\n0.14\n0.22\n1.55\n\n\n\n\n\nOverall, pm25 has an average concentration of 6.82 µg/m³, with a standard deviation of 6.96, indicating variability in measurements. The minimum value for pm25 is 0.00 µg/m³, while the maximum reaches 63.01 µg/m³. Additionally, pm10 shows a highest mean concentration at 19.83 µg/m³ and a wide range, with a maximum of 188.09 µg/m³. The other pollutants, such as so2, o3, no2, and co, show lower mean concentrations. Based on the max and min (p0 and p100 respectively) values for each pollutant, the values are with probable range."
  },
  {
    "objectID": "posts/assign04-cuscus/index.html#exploratory-data-analysis",
    "href": "posts/assign04-cuscus/index.html#exploratory-data-analysis",
    "title": "Do different sensors at the same location report the same values?",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nDistribution and Correlations\n\n\nCode\nggscatmat(data_wider, columns=7:12)\n\n\n\n\n\n\n\n\n\nAll five sensors pm25, pm10, so2, no2, and co exhibit right-skewed distributions, while o3 shows a multimodal distribution. Additionally, there is a strong positive correlation between pm25 and co, with a correlation coefficient of 0.79, and a moderate positive correlation between no2 and co, with a coefficient of 0.65. While there are negative relationships between o3 and no2, o3 and co with correlations are -0.78 and -0.7.\n\n\nGeneral Analysis - Overview\n\n\nCode\n#overview\nalp_gee |&gt; filter(parameter %in% c(\"pm25\", \"pm10\")) |&gt;\n  ggplot(aes(x=date_utc, y=value, color= parameter)) + \n  geom_point(alpha = 0.4) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverall, based on the hourly concentrations of pm10 and pm25during July and August 2024 in Alphington and Geelong South, it is evident that PM₁₀ levels are almost higher than pm25.\nNotably, pm10 concentrations exceed 75 µg/m³ at times, with several instances reaching or surpassing 150 µg/m³, indicating significant variability. In contrast, pm25 concentrations remain below 75 µg/m³ and exhibit less fluctuation, with a narrower range of variation compared to pm10.\n\n\nCode\nalp_gee |&gt; filter(!(parameter %in% c(\"pm25\", \"pm10\"))) |&gt;\n  ggplot(aes(x=date_utc, y=value, color= parameter)) + \n  geom_point(alpha = 0.6) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAmong the four parameters co, no2, o3 and so2 measured during July and August 2024 in Alphington and Geelong South, co stands out with the highest concentration and significant fluctuations, ranging from around 0.1 to as high as 1.5 ppm. This wide range indicates notable variability in co levels over time.\nIn contrast, no2, o3 and so2 concentrations are considerably lower, all remaining below 0.1 ppm throughout the period. Despite their lower levels, these parameters will be analyzed in more detail below to assess whether they show any noticeable fluctuations.\n\n\nDetailed Parameter Analysis\n\n1. pm25\n\n\nCode\nalp_gee |&gt; filter(parameter == \"pm25\") |&gt;\n  ggplot(aes(x=date_utc, y=value, color=location)) + \n  geom_point(alpha = 0.6) +\n  facet_wrap(~location) +\n  labs(title = \"Hourly pm2.5 Concentration\",\n       y = \"pm2.5\")+\n  scale_color_discrete(guide = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\npm25 concentrations in both Alphington and Geelong South show similar levels on certain days, around 10 µg/m³.\nBoth locations have peaks at the beginning of July and August, though the highest concentrations in Alphington are generally higher than in Geelong South, reaching approximately 60 µg/m³ compared to around 40 µg/m³.\nA more detailed look at the concentration levels in both locations is provided in the calendar plot below.\n\n\n\nCode\naggregate_data &lt;- data_wider |&gt;\n  mutate(date = as.Date(date_utc)) |&gt;\n  group_by(date, location) |&gt;\n  summarise(across(c(\"pm25\", \"pm10\", \"so2\", \"o3\", \"no2\", \"co\"), mean, na.rm = TRUE)) \n\n\n\n\nCode\naggregate_data |&gt;  \n  ggplot(aes(x = date, y = pm25, color = location)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Daily Average pm2.5 Concentration\",\n       x = \"Date\",\n       y = \"Average Concentration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSimilar to the hourly concentrations, the daily average pm25 concentrations in both locations reveal a clearer trend, with peaks at the beginning of each month followed by a gradual decrease.\nAdditionally, the average concentration in Alphington is consistently higher than in Geelong South.\n\n\n\nCode\ncalendar &lt;-  alp_gee |&gt;\n  mutate(date = as.Date(date_utc),\n         hour = hour(date_utc)) |&gt;\n  arrange(date)\n \n#\"pm25\" \"pm10\" \"so2\"  \"o3\"   \"no2\"  \"co\"\n# \"Alphington\" and \"Geelong South\" \n\ncalendar |&gt; filter(parameter == \"pm25\") |&gt;\nggplot() +   \n    geom_line(aes(x=hour, y=value,color=location)) +\n    facet_calendar(~date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5)+\n  ggtitle(\"Calendar of pm2.5 Concentration for Alphington & Geelong South\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nThere is no clear seasonal pattern in pm25 concentrations. However, during the first week of July and August, the concentrations fluctuate more widely compared to the rest.\nNotably, on July 4, July 31, and August 3, pm25 levels in Alphington are significantly higher than in Geelong South, ranging between 20 and 60 µg/m³, while Geelong South remains around or below 20 µg/m³.\nIn contrast, on August 1, Geelong South shows higher concentrations than in Alphington, with a peak of 40 µg/m³ compared to Alphington’s peak of around 20 µg/m³.\n\n\n\n2. pm10\n\n\nCode\nalp_gee |&gt; filter(parameter == \"pm10\") |&gt;\n  ggplot(aes(x=date_utc, y=value, color=location)) + \n  geom_point(alpha = 0.6) +\n  facet_wrap(~location) +\n  labs(title = \"Hourly pm10 Concentration\",\n       y = \"pm10\")+\n  scale_color_discrete(guide = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThere is missing data for Geelong South at the beginning of July. While pm25 concentrations are generally higher in Alphington, pm10 levels in Geelong South fluctuate more widely. Geelong South experiences peaks above 100 µg/m³, whereas the highest levels in Alphington are around 60 µg/m³.\n\n\nCode\naggregate_data |&gt;  \n  ggplot(aes(x = date, y = pm10, color = location)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Daily Average pm10 Concentration\",\n       x = \"Date\",\n       y = \"Average Concentration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThere are different trends in the average daily pm10 concentrations between the 2 locations at the beginning of July.\nIn Geelong South, concentrations increase from the start of the month, while in Alphington they decrease.\nHowever, after mid-July, both locations follow a similar trend. One reason for this difference may be the missing data at the beginning of July in Geelong South.\n\n\n\nCode\ncalendar |&gt; filter(parameter == \"pm10\") |&gt;\nggplot() +   \n    geom_line(aes(x=hour, y=value,color=location)) +\n    facet_calendar(~date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5)+\n  ggtitle(\"Calendar of pm10 Concentration for Alphington & Geelong South\")+\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nThere is no clear seasonal pattern in pm10 concentrations. Throughout July and August, levels in Geelong South are generally higher than those in Alphington.\nSpecifically, at the beginning of August, concentrations in Geelong South fluctuate over a wider range compared to Alphington. Additionally, there are some days when the highest levels occur around midnight.\nMoreover, on August 18 and 22, for instance, the concentrations peak around 100 µg/m³, while daytime levels are significantly lower, averaging around 25 µg/m³ or below.\n\n\n\n3. so2\n\n\nCode\nalp_gee |&gt; filter(parameter == \"so2\") |&gt;\n  ggplot(aes(x=date_utc, y=value, color=location)) + \n  geom_point(alpha = 0.6) +\n  facet_wrap(~location)+\n  labs(title = \"Hourly so2 Concentration\",\n       y = \"so2\")+\n  scale_color_discrete(guide = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nMost so2 concentrations in both locations fall within the range of 0 to 0.001 ppm.\nThe highest levels in Geelong South exceed those in Alphington, with concentrations reaching above 0.002 ppm, while Alphington’s highest levels are at 0.002 ppm.\n\n\n\nCode\naggregate_data |&gt;  \n  ggplot(aes(x = date, y = so2, color = location)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Daily Average so2 Concentration\",\n       x = \"Date\",\n       y = \"Average Concentration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThere are distinct trends in the average daily so2 concentrations between the two locations.\nGeelong experiences peaks at the beginning of both July and August, while levels in Alphington show less fluctuation and gradually decrease after mid-August.\n\n\n\nCode\ncalendar |&gt; filter(parameter == \"so2\") |&gt;\nggplot() +   \n    geom_line(aes(x=hour, y=value,color=location)) +\n    facet_calendar(~date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5)+\n  ggtitle(\"Calendar of so2 Concentration for Alphington & Geelong South\")+\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nThere is no clear seasonal pattern in the calendar plot, and the so2 concentrations fluctuate within a narrow range on most days in both locations.\nHowever, a few days show significant differences between the two locations, indicating short-term increases in concentration.\nFor instance, on July 3 and August 12, so2 levels in Geelong South peak much higher than in Alphington. On July 3, the peak occurs around midnight, reaching approximately 0.005 ppm, while on August 12, the concentration peaks in the early morning at around 0.008 ppm.\n\n\n\n4. o3\n\n\nCode\nalp_gee |&gt; filter(parameter == \"o3\") |&gt;\n  ggplot(aes(x=date_utc, y=value, color=location)) + \n  geom_point(alpha = 0.6) +\n  facet_wrap(~location)+\n  labs(title = \"Hourly o3 Concentration\",\n       y = \"o3\")+\n  scale_color_discrete(guide = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\no3 concentrations at both Alphington and Geelong South range from approximately 0.003 ppm to over 0.03 ppm, indicating relatively low ozone levels overall.\nThe scatterplot succinctly captures the hourly fluctuations within each day. This could to attributed to daily cycles of traffic patterns or weather conditions.\nBoth Alphington and Geelong South have similar distributions and ranges of o3, suggesting comparable air quality with respect to ozone in both locations.\nThe data points are within a tight range, with no extreme spikes, indicating stable ozone levels that are not prone to sudden increases.\n\n\n\nCode\naggregate_data |&gt;  \n  ggplot(aes(x = date, y = o3, color = location)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Daily Average o3 Concentration\",\n       x = \"Date\",\n       y = \"Average Concentration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThe above plot shows the daily average concentration for Alphington and Geelong South, with both locations displaying a similar trend.\no3 levels drop to their lowest point in July, then rise, dip slightly around mid-August, and increase again toward the end of August.\nOn average, Geelong South generally has higher o3 concentrations compared to Alphington, particularly in early July and from mid-August onward.\nDespite minor differences, the parallel LOESS curves suggest that both locations experience similar seasonal or environmental influences on o3 levels.\n\n\n\nCode\ncalendar |&gt; filter(parameter == \"o3\") |&gt;\nggplot() +   \n    geom_line(aes(x=hour, y=value,color=location)) +\n    facet_calendar(~date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5)+\n  ggtitle(\"Calendar of o3 Concentration for Alphington & Geelong South\")+\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nMost days, both locations follow a daily cycle, where o3 levels rise in the morning, peaking at midday and declining by evening.\nGeelong South consistently shows higher peaks compared to Alphington, suggesting elevated o3 levels overall.\nJuly 3rd, 4th, 5th, and 31st: Geelong South has sharp peaks with minimal variation in Alphington, likely due to localized factors.\nThere is an unusual increasing trend on July 19th, instead of the usual decreasing trend in the evenings for both Alphington and Geelong South.\nJuly 15, August 26, 30, 31: The o3 levels seem to be consistently high at around 0.03 ppm with minimal variations throughout the day.\nWeekend peaks are frequently as high as those on weekdays, and the overall patterns remain largely consistent across all days, indicating that ozone levels are influenced by sustained factors, regardless of the day of the week.\n\n\n\n5. no2\n\n\nCode\nalp_gee |&gt; filter(parameter == \"no2\") |&gt;\n  ggplot(aes(x=date_utc, y=value, color=location)) + \n  geom_point(alpha = 0.6) +\n  facet_wrap(~location)+\n  labs(title = \"Hourly no2 Concentration\",\n       y = \"no2\")+\n  scale_color_discrete(guide = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThere is data missing for Geelong South for a significant part of August.\nThe variations of no2 at both Alphington and Geelong South are similar, ranging from 0.00 ppm to 0.04 ppm for most time although the highest concentrations in Alphington are generally lower than in Geelong South, reaching approximately 0.04 ppm compared to over 0.06 ppm.\nno2 concentrations remain stable over time, with no clear seasonal or time-dependent trends.\nThere are daily variations, but no consistent pattern of peaks or troughs is apparent in the scatterplot.\n\n\n\nCode\naggregate_data |&gt;  \n  ggplot(aes(x = date, y = no2, color = location)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Daily Average no2 Concentration\",\n       x = \"Date\",\n       y = \"Average Concentration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThe average levels of no2 concentration appears to follow a similar trend at both Alphington and Geelong South. Both peak at the start of July with concentration over 0.015 ppm and dip mid July before rising again in early August before declining again.\nAverage no2 levels are slightly higher at Alphington until late July, likely due to its proximity to the city and increased traffic, which could account for the elevated no2 levels compared to Geelong South.\nWith data missing for much of August at Geelong South, we might expect a similar downward trend as observed at Alphington. However, further investigation is necessary to confirm this assumption.\n\n\n\nCode\ncalendar |&gt; filter(parameter == \"no2\") |&gt;\nggplot() +   \n    geom_line(aes(x=hour, y=value,color=location)) +\n    facet_calendar(~date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5)+\n  ggtitle(\"Calendar of no2 Concentration for Alphington & Geelong South\")+\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nThere is no clear seasonal pattern in no2 concentrations, but the first week of July shows wider fluctuations compared to other days.\nNotably, on July 3, no2 levels in Geelong South peak twice, mid-day and late night, reaching around 0.04 ppm and above 0.06 ppm, higher than Alphington’s peak of 0.03 ppm in the morning.\nOn July 4, Geelong South shows a reverse trend, with levels decreasing from around 0.03 ppm early in the day before rising again at night, whereas in Alphington, no2 levels increase and peak later in the morning, then decline from noon.\n\n\n\n6. co\n\n\nCode\nalp_gee |&gt; filter(parameter == \"co\") |&gt;\n  ggplot(aes(x=date_utc, y=value, color=location)) + \n  geom_point(alpha = 0.6) +\n  facet_wrap(~location)+\n  labs(title = \"Hourly co Concentration\",\n       y = \"co\")+\n  scale_color_discrete(guide = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nco levels follow a similar pattern over the observed period at both locations, with higher concentrations at the start of each month that gradually stabilize by mid-month.\nIn Alphington co levels are higher, ranging up to 1.5 ppm, with multiple spikes exceeding 1.0 ppm, particularly in early July and early August. In Geelong co levels range from 0.0 to around 1.2 ppm, with occasional peaks above 0.5 ppm, indicating some variability.\nWhile, Alphington shows more frequent and intense spikes, likely due to urban sources such as traffic. Geelong South has overall lower co levels, with less frequent and smaller peaks, suggesting fewer or less intense sources.\n\n\n\nCode\naggregate_data |&gt;  \n  ggplot(aes(x = date, y = co, color = location)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Daily Average co Concentration\",\n       x = \"Date\",\n       y = \"Average Concentration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBoth locations display a decline in average co levels from early July, reaching a low around late August. The steepest decline is observed in the first half of July, particularly at Alphington.\nThere is a noticeable peak at the start of each month, with co levels higher in early July and early August before tapering off towards the middle and end of the month.\nAlphington starts with a higher co concentration, peaking near 0.6 ppm, which steadily decreases over time. Its levels remain higher than Geelong South’s until late July, when they begin to converge towards end of August.\nGeelong South maintains generally lower co concentrations, peaking around 0.3 ppm in early July and declining more gradually. This trend indicates consistently lower co pollution in comparison to Alphington.\n\n\n\nCode\ncalendar |&gt; filter(parameter == \"co\") |&gt;\nggplot() +   \n    geom_line(aes(x=hour, y=value,color=location)) +\n    facet_calendar(~date) +\n  theme(axis.title = element_blank(),\n        axis.text.y = element_blank(),\n        aspect.ratio = 0.5)+\n  ggtitle(\"Calendar of co Concentration for Alphington & Geelong South\")+\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\nBoth locations show co peaks, mainly in the morning, likely due to traffic emissions. Alphington consistently has higher peaks than Geelong South.\nAlphington frequently surpasses 0.5 ppm, with some peaks over 1.0 ppm. Geelong South generally stays below 0.5 ppm and exhibits fewer fluctuations.\nHigh Spikes in Early July and August, Alphington shows significant spikes on July 2nd to 4th, and August 3rd-4th, reaching over 1.0 ppm, not seen at Geelong South except on July 3rd. Although, most high spikes but in reduced levels for Geelong appear around the same time.\nLow Variability in late August, both locations show reduced co levels and minimal fluctuations, possibly due to improved dispersion conditions. Similar trend can be observed in July, from 19th to 26th, although there is data missing on July 19th.\nWhile weekdays have more pronounced peaks, both locations follow a similar pattern across all days, with Alphington displaying consistently higher co due to its urban environment."
  },
  {
    "objectID": "posts/assign04-cuscus/index.html#summary",
    "href": "posts/assign04-cuscus/index.html#summary",
    "title": "Do different sensors at the same location report the same values?",
    "section": "Summary",
    "text": "Summary\nOverall, the sensors in Alphington and Geelong South do not consistently report the same values:\n\npm25 concentrations in both locations are generally below 20 µg/m³ on most days, which is within the safe threshold of 25 µg/m³. However, Alphington generally records more higher values, with peaks reaching up to 60 µg/m³ compared to Geelong South’s maximum of 40 µg/m³.\npm10 concentrations in both locations mostly remain below 20 µg/m³, which is well under the recommended limit of 50 µg/m³. However, Geelong South frequently experiences higher values, exceeding 50 µg/m³ and even surpassing 100 µg/m³, whereas Alphington’s peak levels reach only around 60 µg/m³.\nso2 concentrations typically range from 0 to 0.001 ppm in both locations, staying within the safe range of 0 to 0.005 ppm. However, Geelong South occasionally experiences spikes with higher levels, reaching between 0.006 and 0.008 ppm.\no3 concentrations at Alphington and Geelong South range from approximately 0 ppm to over 0.03 ppm, with both locations displaying similar trends in average levels. Both remain within the safe limit of 0.07 ppm for 8-hour exposure. However, Geelong South tends to show higher levels, particularly in the morning.\nno2 concentrations typically range from 0.00 ppm to 0.04 ppm in both locations, remaining below the safe limit of 0.08 ppm for one-hour exposure. Only on July 3, Geelong South recorded a high peak of 0.06 ppm at night. Due to missing data for Geelong South in August, trends during this period are uncertain, but both locations exhibited similar daily average values in July.\nco concentrations in both locations peak at the start of each month, with Alphington consistently higher, reaching up to 1-1.5 ppm. Despite these peaks, both locations remain well below the safe limit of 6 ppm for 8-hour exposure.\n\nIn conclusion, although we expect that Alphington will be more polluted than Geelong South due to its proximity to the Melbourne CBD, which is more crowded, Geelong South still faces more frequent and significant spikes in pm10 and so2. This indicates a potential for health risks in that area as well. The consistently higher pollution levels of the other pollutants in Alphington suggest urban influences, likely from traffic and other sources. Further, there doesn’t seem to appear a different trend on weekdays compared to weekends. The variations and patterns are quite similar on all days of the week. In addition, the expectation of positive correlations among pollutants was partially met, with strong positive correlations between pm25 and co (0.79), and moderate positive correlations between no2 and co (0.65). However, unexpected negative correlations were observed between o3 and both no2 (-0.78) and co (-0.7), which diverges from the anticipated trend, suggesting a more complex relationship involving ozone."
  },
  {
    "objectID": "posts/assign04-cuscus/index.html#references",
    "href": "posts/assign04-cuscus/index.html#references",
    "title": "Do different sensors at the same location report the same values?",
    "section": "References",
    "text": "References\n\nOpenAQ. (n.d.). Explore Open Air Quality. Retrieved from https://explore.openaq.org/#1.2/13.8/40\nWorld Health Organization. (n.d.). Types of Pollutants. Retrieved from https://www.who.int/teams/environment-climate-change-and-health/air-quality-and-health/health-impacts/types-of-pollutants\nNSW Government. (n.d.). Air Quality Categories - Health Advice. Retrieved from https://www.airquality.nsw.gov.au/health-advice/air-quality-categories\nOpenAQ. (2022). Open Air Quality Data Global Landscape 2022. Retrieved from https://documents.openaq.org/reports/Open+Air+Quality+Data+Global+Landscape+2022.pdf\nOpenAQ. (2020). Open Air Quality Data Global State of Play 2020. Retrieved from https://documents.openaq.org/reports/Open+Air+Quality+Data+Global+State+of+Play+2020.pdf\nClean Air Fund. (n.d.). Countries Failing to Report Data on Air Pollution. Retrieved from https://www.cleanairfund.org/news-item/countries-failing-data-on-air-pollution/"
  },
  {
    "objectID": "posts/assign02-fix-praj0022/index.html#becoming-tukey",
    "href": "posts/assign02-fix-praj0022/index.html#becoming-tukey",
    "title": "Diving Deeper into Data Exploration: Olympic Data",
    "section": "Becoming Tukey",
    "text": "Becoming Tukey\na. Top 10 Countries by Gold Medal Count\n\n\nCode\n# Web scraping the wikipedia medal tally page \nwiki_link &lt;- \"https://en.wikipedia.org/wiki/2024_Summer_Olympics_medal_table\"\nwiki_page &lt;- read_html(wiki_link)\n\n#Reading all tables in the page\ntables &lt;- html_nodes(wiki_page, \"table\")\n\n#Selecting the olympic medal table \nmedal_table &lt;- tables[[4]]\n\n#Converting to dataframe \nmedal_tally &lt;- html_table(medal_table, fill = TRUE)\n\n#Cleaning the data scrapped\nmedal_tally &lt;- medal_tally |&gt;\n  mutate(NOC = str_replace_all(NOC, \"\\\\*|[‡]\", \"\"))\n\ntop_10 &lt;- medal_tally |&gt; \n        select(Rank, NOC, Gold) |&gt;\n        head(10)\n\nkable(top_10, caption = \"Top 10 Countries by Gold Medal Count\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nTop 10 Countries by Gold Medal Count\n\n\nRank\nNOC\nGold\n\n\n\n\n1\nUnited States\n40\n\n\n2\nChina\n40\n\n\n3\nJapan\n20\n\n\n4\nAustralia\n18\n\n\n5\nFrance\n16\n\n\n6\nNetherlands\n15\n\n\n7\nGreat Britain\n14\n\n\n8\nSouth Korea\n13\n\n\n9\nItaly\n12\n\n\n10\nGermany\n12\n\n\n\n\n\nb. Top 10 Countires Based on Gold Medals Per Capita\n\n\nCode\npopulation &lt;- read_csv(\"data/population-and-demography.csv\")\n\n\n\n\nCode\n#Cleaning the population data \npopulation &lt;- population |&gt; \n             filter(Year == \"2023\") |&gt; \n              drop_na()\n\n\n\n\nCode\n# Generating the country codes for medal data \n# Manually set code for Kosovo based on code present in population data as it was not generating using 'countrycode' package\n\nmedal_tally &lt;- medal_tally |&gt;\n  mutate(Code = case_when(\n    NOC == \"Kosovo\" ~ \"OWID_KOS\",  \n    TRUE ~ countrycode(NOC, origin = \"country.name\", destination = \"iso3c\")\n  ))\n\n\n\n\nCode\noptions(scipen = 999)\n\n# Coming the medal tally and population data \ncombined_data &lt;- left_join(medal_tally, \n                           population, \n                           by = \"Code\")\n\n# Cleaning combined data and converting population into millions\ncombined_data &lt;- combined_data |&gt; \n        select(NOC, Gold, Silver, Bronze, Total, `Population - Sex: all - Age: all - Variant: estimates`) |&gt;\n        mutate(`Population (Millions)`  = `Population - Sex: all - Age: all - Variant: estimates` / 1000000) |&gt;                   \n        select(-`Population - Sex: all - Age: all - Variant: estimates`)\n\n# Calculating medals per capita\ncombined_data_top &lt;- combined_data |&gt; \n  mutate(Medals_Per_Capita = Gold / `Population (Millions)`) |&gt; \n  arrange(desc(Medals_Per_Capita)) |&gt; \n  mutate(Rank = row_number()) |&gt; \n  head(10)\n\n\nkable(combined_data_top, caption = \"Top 10 Countries Based on Gold Medals Per Capita\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) \n\n\n\nTop 10 Countries Based on Gold Medals Per Capita\n\n\nNOC\nGold\nSilver\nBronze\nTotal\nPopulation (Millions)\nMedals_Per_Capita\nRank\n\n\n\n\nDominica\n1\n0\n0\n1\n0.07\n15.03\n1\n\n\nSaint Lucia\n1\n1\n0\n2\n0.18\n5.58\n2\n\n\nNew Zealand\n10\n7\n3\n20\n5.17\n1.93\n3\n\n\nBahrain\n2\n1\n1\n4\n1.57\n1.27\n4\n\n\nSlovenia\n2\n1\n0\n3\n2.12\n0.94\n5\n\n\nNetherlands\n15\n7\n12\n34\n18.09\n0.83\n6\n\n\nGeorgia\n3\n3\n1\n7\n3.81\n0.79\n7\n\n\nIreland\n4\n0\n3\n7\n5.20\n0.77\n8\n\n\nNorway\n4\n1\n3\n8\n5.52\n0.72\n9\n\n\nAustralia\n18\n19\n16\n53\n26.45\n0.68\n10\n\n\n\n\n\nc. Five New Ways of Ranking the Olympic Countries\n\n\nCode\n# Web scraping the wikipedia page to get count of participants from each country \nparticipant_link &lt;- \"https://simple.wikipedia.org/wiki/2024_Summer_Olympics#Competing_nations\"\nwiki_part &lt;- read_html(participant_link)\n\ntables_part &lt;- html_nodes(wiki_part, \"table\")\n\n#Selecting the participant count table \nparticipant_table &lt;- tables_part[[5]]\n\n#Converting to dataframe \nparticipant_tally &lt;- html_table(participant_table, fill = TRUE)\n\n#Cleaning the scrapped data \nparticipant_tally &lt;- participant_tally |&gt;\n  mutate(Country = str_replace_all(Country, \"\\\\*|[‡]\", \"\"))\n\n\n\n\nCode\n#Joining the participant tally with existing data\njoin_part &lt;- left_join(combined_data, \n                       participant_tally,\n                      by = c(\"NOC\" = \"Country\")) |&gt; \n              select(-Ranking)\n\n\n\n\nCode\n# Reading the GPD per capita data \ngdp_per_capita &lt;- read_csv(\"data/gdp-per-capita-worldbank.csv\")\n\n\n\n\nCode\ngdp_per_capita &lt;- gdp_per_capita |&gt; \n  filter(Year == \"2022\") |&gt; \n  rename(`GDP_Per_Capita ($)` = `GDP per capita, PPP (constant 2017 international $)`)\n\n\n\n\nCode\n#Combining the GDP data other olymic data\nfull_data &lt;- left_join(join_part, \n                       gdp_per_capita,\n                      by = c(\"NOC\" = \"Entity\")) |&gt; \n            select(-Code, -Year)\n\n\n\nMethod 1: Medals Per Total Athletes\n\n\nCode\nmethod_1 &lt;- full_data |&gt;\n  mutate(medal_efficiency = as.numeric(Total) / Athletes) |&gt;\n  arrange(desc(medal_efficiency)) |&gt;\n  select(-`Population (Millions)`, -`GDP_Per_Capita ($)`) |&gt;\n  head(5)\n\n# Top countries by Medal Efficiency Index\nkable(method_1, \n      caption = \"Top 5 Countires Based on Medals Per Total Athletes\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) \n\n\n\nTop 5 Countires Based on Medals Per Total Athletes\n\n\nNOC\nGold\nSilver\nBronze\nTotal\nAthletes\nmedal_efficiency\n\n\n\n\nSaint Lucia\n1\n1\n0\n2\n4\n0.50\n\n\nKyrgyzstan\n0\n2\n4\n6\n16\n0.38\n\n\nNorth Korea\n0\n2\n4\n6\n16\n0.38\n\n\nGrenada\n0\n0\n2\n2\n6\n0.33\n\n\nIran\n3\n6\n3\n12\n40\n0.30\n\n\n\n\n\n\nThis metric evaluates how effectively countries convert the number of athletes they send to the Olympics into actual medals won. It measures the success of a nation by comparing the number of medals (gold, silver, and bronze) achieved relative to the total number of athletes they have participating. A higher value in this index indicates that a country is more successful at converting its athletic representation into medals, reflecting greater efficiency in their Olympic performance. This method highlights how well countries manage to achieve medal success with the athletes they send, emphasizing performance efficiency.\n\n\n\nMethod 2: Total Medals Per Capita\n\n\nCode\nmethod_2 &lt;- full_data |&gt;\n  mutate(population_adjusted_medals = as.numeric(Total) / `Population (Millions)`) |&gt;\n  arrange(desc(population_adjusted_medals)) |&gt; \n  select(-Athletes, -`GDP_Per_Capita ($)`) |&gt;\n  head(5)\n\n# Top 5 countries by medals relative to population\nkable(method_2, \n      caption = \"Top 5 Countires Based on Total Medals Per Capita\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) \n\n\n\nTop 5 Countires Based on Total Medals Per Capita\n\n\nNOC\nGold\nSilver\nBronze\nTotal\nPopulation (Millions)\npopulation_adjusted_medals\n\n\n\n\nGrenada\n0\n0\n2\n2\n0.12\n17.1\n\n\nDominica\n1\n0\n0\n1\n0.07\n15.0\n\n\nSaint Lucia\n1\n1\n0\n2\n0.18\n11.2\n\n\nNew Zealand\n10\n7\n3\n20\n5.17\n3.9\n\n\nBahrain\n2\n1\n1\n4\n1.57\n2.5\n\n\n\n\n\n\nThis metric normalizes the total count of medals won (gold, silver, and bronze) by factoring in the population size of each country. It calculates the total number of medals per population (millions), allowing for a comparison that adjusts for the size of the population. This approach offers a per capita assessment of Olympic success, providing a more equitable measure of performance between countries of varying sizes, rather than focusing only on the count of gold medals or gold medals per capita.\n\n\n\nMethod 3: Medals per unit of GDP Per Capita\n\n\nCode\nmethod_3 &lt;- full_data |&gt;\n  mutate(medals_per_gdp_unit = (as.numeric(Total) / `GDP_Per_Capita ($)`) * 100) |&gt;\n  arrange(desc(medals_per_gdp_unit)) |&gt;\n select(-Athletes, -`Population (Millions)`) |&gt; \n  head(5)\n\n# Top 5 countries by medals relative to GDP per capita\nkable(method_3, \n      caption = \"Top 5 Countires Based on Medals per unit of GDP Per  Capita\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) \n\n\n\nTop 5 Countires Based on Medals per unit of GDP Per Capita\n\n\nNOC\nGold\nSilver\nBronze\nTotal\nGDP_Per_Capita ($)\nmedals_per_gdp_unit\n\n\n\n\nChina\n40\n27\n24\n91\n18188\n0.50\n\n\nKenya\n4\n2\n5\n11\n4882\n0.23\n\n\nUnited States\n40\n44\n42\n126\n64623\n0.19\n\n\nEthiopia\n1\n3\n0\n4\n2381\n0.17\n\n\nUzbekistan\n8\n2\n3\n13\n8073\n0.16\n\n\n\n\n\n\nThis metric evaluates how efficiently countries convert their economic resources, measured as GDP per capita, into Olympic medals. By determining the number of medals achieved per unit of GDP per capita and expressing this as a percentage, it sheds light on how effectively a country uses its financial means to succeed in the Olympics. A higher percentage indicates a greater ability to convert economic investment into athletic success. This approach provides insight into how well countries of different economic standings perform, highlighting the effectiveness of their economic resources in securing medals.\n\n\n\nMethod 4: Weighted Medal Count Per Athletes\n\n\nCode\nmethod_4 &lt;- full_data |&gt;\n  mutate(weighted_medal_count = (Gold * 3 + Silver * 2 + Bronze * 1) / Athletes)|&gt;\n  arrange(desc(weighted_medal_count)) |&gt;\n  select(-`GDP_Per_Capita ($)`, -`Population (Millions)`) |&gt; \n  head(5)\n\n# Top 5 countries by Weighted Medal Count Per athletes\nkable(method_4, \n      caption = \"Top 5 Countires Based on Weighted Medal Count Per Athletes\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) \n\n\n\nTop 5 Countires Based on Weighted Medal Count Per Athletes\n\n\nNOC\nGold\nSilver\nBronze\nTotal\nAthletes\nweighted_medal_count\n\n\n\n\nSaint Lucia\n1\n1\n0\n2\n4\n1.25\n\n\nDominica\n1\n0\n0\n1\n4\n0.75\n\n\nIran\n3\n6\n3\n12\n40\n0.60\n\n\nGeorgia\n3\n3\n1\n7\n28\n0.57\n\n\nChina\n40\n27\n24\n91\n388\n0.51\n\n\n\n\n\n\nThis method assigns different weights to each type of medal, with gold receiving a weight of 3, silver a weight of 2, and bronze a weight of 1. These weights reflect the varying prestige of each medal, acknowledging that gold medals are more significant than silver or bronze. The weighted total is then divided by the number of athletes from each country, providing a measure of how effectively a nation converts its athlete participation into high-value medal achievements. A higher score indicates that a country not only wins medals but does so by prioritizing the most prestigious ones, offering a more nuanced view of Olympic success that factors in both medal quality and athlete efficiency.\n\n\n\nMethod 5: Weighted Medals per GDP and Athletes\n\n\nCode\nmethod_5 &lt;- full_data |&gt;\n  mutate(medal_value_index = ((Gold * 3 + Silver * 2 +Bronze * 1) / (`GDP_Per_Capita ($)` * Athletes)) * 100) |&gt;\n  arrange(desc(medal_value_index)) |&gt;\n  select(-`Population (Millions)`) |&gt; \n  head(5)\n\n# Top 5 countries by Weighted Medal Count Per per GDP and Athletes\nkable(method_5, \n      digits = c(0, 0, 0, 0,0,0,0, 5), \n      caption = \"Top 5 Countires Based on Weighted Medals per GDP and Athletes\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) \n\n\n\nTop 5 Countires Based on Weighted Medals per GDP and Athletes\n\n\nNOC\nGold\nSilver\nBronze\nTotal\nAthletes\nGDP_Per_Capita ($)\nmedal_value_index\n\n\n\n\nEthiopia\n1\n3\n0\n4\n34\n2381\n0.0111\n\n\nKyrgyzstan\n0\n2\n4\n6\n16\n5070\n0.0099\n\n\nUganda\n1\n1\n0\n2\n24\n2280\n0.0091\n\n\nSaint Lucia\n1\n1\n0\n2\n4\n15100\n0.0083\n\n\nPakistan\n1\n0\n0\n1\n7\n5377\n0.0080\n\n\n\n\n\n\nThe Medal Value Index assesses how effectively a country turns its economic resources and athletes into Olympic medals. It weights medals (3 points for gold, 2 for silver, and 1 for bronze), and divides the total by GDP per capita and the number of athletes. This provides insight into a nation’s efficiency in converting resources into medals. A higher percentage suggests that a country excels at winning medals with fewer resources or athletes, while a lower percentage highlights higher resource requirements for the same achievement."
  },
  {
    "objectID": "posts/assign02-fix-praj0022/index.html#chatfield-style-ida",
    "href": "posts/assign02-fix-praj0022/index.html#chatfield-style-ida",
    "title": "Diving Deeper into Data Exploration: Olympic Data",
    "section": "Chatfield-style IDA",
    "text": "Chatfield-style IDA\n\n\nCode\n# Reading the original data \nfuel_data &lt;- read_csv(\"data/world_indicators_data_original.csv\", na = \"..\", n_max = 11935)\n\n\n\nThe population of interest in this context would be all households or individuals in a given region or country that rely on various types of fuels for cooking. This could be within a specific country, region, or globally, depending on the scope of the analysis."
  },
  {
    "objectID": "posts/assign02-fix-praj0022/index.html#checking-the-data-collection",
    "href": "posts/assign02-fix-praj0022/index.html#checking-the-data-collection",
    "title": "Diving Deeper into Data Exploration: Olympic Data",
    "section": "Checking the data collection",
    "text": "Checking the data collection\n\n\nCode\nexperiment_data &lt;- read_csv(\"data/DAT_HexmapPilotData_V1_20191115.csv\")\n\n\n\nAssessing Data Irregularities:\n1. Checking for Missing Values in Variables:\n\n\nCode\n# Checking for missing values \ncolSums(is.na(experiment_data)) \n\n\n    group     trend  location      type    choice    reason \n        0         0         0         0         0         0 \ncertainty      time     order replicate        id    detect \n        0         0         0         0         0         0 \n\n\n2. Checking for Duplicate Rows:\n\n# Detecting duplicate rows\nsum(duplicated(experiment_data))\n\n[1] 0\n\n\n3. Checking for Data Consistency\n\nVerifying the levels in the categorical variables\n\n\n# Checking for unexpected levels in categorical variables\nunique(experiment_data$type)  \n\n[1] \"Geography\" \"Hexagons\" \n\nunique(experiment_data$trend)\n\n[1] \"NW-SE\"        \"three cities\" \"all cities\"  \n\nunique(experiment_data$group) \n\n[1] \"A\" \"B\"\n\n\n\n\nCode\n#Note: Hiding results as the number of variables are high\n\nunique(experiment_data$location) \nunique(experiment_data$certainty)\nunique(experiment_data$reason)\nunique(experiment_data$choice)\nunique(experiment_data$detect)\nunique(experiment_data$order)\nunique(experiment_data$replicate)\n\n\n\nChecking the summary to access the structure and range for each variable.\n\n\n# Note: output is hidden in the report because of high number of variables\n#Checking the summary to confirm the range and detect unusual values\nsummary(experiment_data)\n\n\nChecking the measurements of groups against the six treatment levels.\n\n\n\nCode\nsummary_table &lt;- experiment_data |&gt;\n  count(group, type, trend) |&gt;\n  pivot_wider(names_from = trend, \n              values_from = n, \n              values_fill = list(n = 0)) |&gt;\n  arrange(group, type)\n\n\nkable(summary_table, \n      caption = \"Counts by Group, Type, and Trend\") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"responsive\"))\n\n\n\n\nCounts by Group, Type, and Trend\n\n\ngroup\ntype\nNW-SE\nall cities\nthree cities\n\n\n\n\nA\nGeography\n20\n20\n20\n\n\nA\nHexagons\n20\n20\n20\n\n\nB\nGeography\n22\n22\n22\n\n\nB\nHexagons\n22\n22\n22\n\n\n\n\n\n\n\nHere, we can observe that group B has 2 more measurements as compared to group A for each treatment. There is a slight imbalance and might not have significant effect on the outcome as the primary factors are “plot type” and “trend model”. The paper does not explicitly mention any steps to balance this, suggesting the authors may have considered the imbalance small enough to not affect the results.\n\n\n\nIdentifying which Columns of the Data Match the Factors in the Experiment\nBased on the paper, the two main experimental factors are:\n\nPlot Type: Column in dataset matching the factor: ‘type’ (Geography or Hexagons)\nTrend Model: Column in dataset matching the factor: ‘trend’ (NW-SE, all cities, three cities)\n\n\n\nChecking the Number of Measurements Collected for Each of the Treatments\n\n\nCode\ntype_counts &lt;- experiment_data |&gt;\n  group_by(type) |&gt;\n  summarise(count = n())\n\nggplot(type_counts, aes(x = type, y = count, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Measurements by Plot Type\", \n       x = \"Plot Type\", \n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nAbove figure shows the number of measurements for each type of plot (Geography vs. Hexagons). As observed, they are consistent in count.\n\n\n\nCode\ntrend_counts &lt;- experiment_data |&gt;\n  group_by(trend) |&gt;\n  summarise(count = n())\n\nggplot(trend_counts, aes(x = trend, y = count, fill = trend)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Measurements by Trend Model\", \n       x = \"Trend Model\", \n       y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\nThe above figure, displays the number of measurements for each trend model (NW-SE, all cities, three cities). They are also consistent in measurements.\n\nNumber of Measurements Collected for Each of the Treatments\n\n\nCode\ntype_trend_counts &lt;- experiment_data |&gt;\n  group_by(type, trend) |&gt;\n  summarise(count = n(), .groups = 'drop')\n\nkable(type_trend_counts, \n      caption = \"Distribution of Measurements across all Treatments \") |&gt;\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nDistribution of Measurements across all Treatments\n\n\ntype\ntrend\ncount\n\n\n\n\nGeography\nNW-SE\n42\n\n\nGeography\nall cities\n42\n\n\nGeography\nthree cities\n42\n\n\nHexagons\nNW-SE\n42\n\n\nHexagons\nall cities\n42\n\n\nHexagons\nthree cities\n42\n\n\n\n\n\n\nThe above table illustrates the distribution of measurements across all combinations of plot types and trend types to ensure each treatment has been adequately covered.\n\nBased on the above assessment, summary statistics and plots, we can infer the following:\n\nThere are no gaps/missing values in the variables.\nThere are no duplicate rows.\nThe data types of variables are consistent. The ranges of the variables is within the expected range as described in the paper.\nThere is a slight imbalance in the measurements of Group A and Group B for each treatment.\nThe data shows an even distribution between the two plot types: “Geography” (choropleth map) and “Hexagons” (hexagon tile map). No plot type is significantly underrepresented, which suggest no bias or inconsistency.\nThe data is evenly distributed and follows the expected experimental design across the three trend types: “NW-SE,” “all cities,” and “three cities”. No trend type has significantly fewer measurements.\nIdeally, there is an even distribution of measurements across all combinations of plot types and trend types forming six levels of treatment. All treatments have similar number of observations. Hence, the experimental design was fully covered. There are no gaps in the data collection.\n\nBased on the analysis, plot types, trend types and the combination of both forming the six treatment levels are well represented, without any under-representation or discrepancies in measurements, suggesting that the data collection was successful and aligned with the experimental design. There are no issues in the data collection apart from the slight imbalance in measurements of groups A and B."
  },
  {
    "objectID": "posts/assign02-fix-praj0022/index.html#generative-ai-analysis",
    "href": "posts/assign02-fix-praj0022/index.html#generative-ai-analysis",
    "title": "Diving Deeper into Data Exploration: Olympic Data",
    "section": "Generative AI analysis",
    "text": "Generative AI analysis\n\nChatGPT was useful in understanding how to clean data scrapped from web. I also faced a lot of issues when joining datasets with different naming conventions for countries and was able to resolve such issues using ChatGPT.\nI was able to clarify errors that I was unable to understand. Further, I used ChatGPT to understand the difference between various imputing methods and how to select their parameters. It also helped me in understanding how to set the k value, when using moving averages method.\nI also used ChatGPT to understand how to determine thresholds when it comes to removing variables based on missing percentage. I read about ideal situations to use imputation in and how imputation would affect at various percentages of missing data."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "From Isolation to Connection: How the World Managed Mental Health in 2020",
    "section": "",
    "text": "When COVID-19 hit, the world didn’t just face a health crisis—it also grappled with a mental health crisis. Depression rates surged as people dealt with isolation, financial worries, and uncertainty. So, how did people cope? That’s what we’re digging into here: the different ways people across the globe managed their mental health during the pandemic and what were the most popular coping stategies. By understanding these unique strategies, we can build better mental health support systems that fit different cultural and social contexts.\nDepression isn’t confined to any one group—it impacts people of all ages, backgrounds, and cultures. Recent data shows that depressive symptoms were already on the rise by 2019, particularly affecting older age groups. The COVID-19 pandemic only intensified this trend, with anxiety and depression surging worldwide as observed in Figure 1. According to the World Health Organization (2022), mental health became a critical issue for millions, emphasizing the urgent need for support.\n\n\n\n\n\n\n\n\nFigure 1: Prevalence of Depression by Age Group\n\n\n\n\n\nWhat’s fascinating is how people around the world responded to these mental health challenges. Cultural norms, societal values, and economic realities all influenced the ways people coped with anxiety and depression. Understanding these coping methods matters because it helps tailor mental health services to better address individual needs. It refines therapeutic approaches, making them more responsive to diverse preferences, and empowers people to build resilience by drawing on the coping practices of others. Additionally, these insights contribute to mental health research, paving the way for innovative therapies that make a meaningful difference.\nIt’s clear that a one-size-fits-all approach to mental health won’t work. The more we understand about different coping strategies, the better we can support those who need it most. To explore these coping strategies, I analyzed the 2020 dataset from Our World in Data, “How do people deal with anxiety or depression around the world?”\nThe dataset includes responses from people who experienced anxiety or depression severe enough to disrupt their daily lives for two weeks or more. Key variables include each country’s name (Entity), the country code (Code), and the year of reporting (Year). Most importantly, the dataset details the percentage of people engaging in various coping methods, like spiritual activities, healthy lifestyle changes, altering work or personal relationships, talking to loved ones, taking prescribed medication, spending time in nature, and seeking professional help.\nTo make sense of the data, I cleaned up the dataset by renaming columns to make them clearer and more descriptive. Next, I calculated the average percentage of use for each coping method to find out which strategies were the most common. I used the summarise function for this step and took care of any missing values along the way. Once I had the averages, I transformed the data into a long format to make it easier to work with. From there, I created a summary table to show the share of use for each coping strategy, formatting it with kable so it was neat and easy to read.\nBut I wanted to go beyond averages. To see how coping strategies varied across countries, I reshaped the data into a long format again, where each row represented a country and its corresponding share for each coping method. After grouping by method using group_by function and sorting in descending order, I identified the top 5 countries for each coping strategy using the top_n function.\nI visualized these insights with ggplot, creating a bar plot with the percentage share of each method on the x-axis and the corresponding countries on the y-axis. I used facet_wrap to break down the plot by coping method, presenting methods into separate panels for clarity, with two columns.\nTop Three Coping Methods\nCoping with anxiety and depression varies widely, shaped by factors like environment, cultural background, and access to resources. Yet, despite the differences some strategies stand out as widely adopted across the globe. So, which coping methods were the most commonly used? Let’s explore the top three ways people around the world managed their mental health during the pandemic.\n\n\n\n\nTable 1: Share of Use of Various Coping Methods\n\n\n\n\nShare of Use of Various Coping Methods\n\n\nMethods\nShare of Use in Percentage\n\n\n\n\nTalked to friends or family\n79.54\n\n\nSpent time in nature\n72.01\n\n\nImproved healthy lifestyle behaviors\n70.85\n\n\nMade a change to personal relationships\n60.42\n\n\nMade a change to work situation\n49.72\n\n\nTook prescribed medication\n49.12\n\n\nTalked to mental health professional\n44.12\n\n\nReligious spiritual activities\n42.89\n\n\n\n\n\n\n\n\nAmong the eight strategies analyzed in Table 1, talking to friends or family, spending time in nature, and improving healthy lifestyle behaviors took the top spots. Turns out, 79.5% of people leaned on their social circles, choosing to talk to friends or family when managing anxiety or depression. It makes sense—social support is a tried-and-true way to feel less isolated and more grounded when things get tough. Meanwhile, 72.01% of people found comfort in the great outdoors. Spending time in nature isn’t just a nice escape; it’s scientifically proven to lower stress and boost your mood. Whether it’s a walk in the park or a weekend hike, the fresh air works wonders. Then there’s improving healthy lifestyle behaviors, chosen by 70.85%. Small changes like exercising more, eating well, and getting enough sleep can make a significant difference in how people manage their mental health, highlighting the important link between physical and emotional well-being.\nFor Each Coping Strategy, What Were The Top 5 Countries They Were Most Popular In\nDifferent countries handle mental health in their own unique ways. Just take a look at Figure 2—the coping strategies vary a lot depending on where you are, and that tells us something important about exploring various strategies to customise interventions for diverse demographic, ethnic, and socio-economic contexts (Bell, Williams, & Cooper, 2021).\n\n\n\n\n\n\n\n\nFigure 2: Top Preferences of Coping Methods by Country\n\n\n\n\n\nIn places like Ecuador, Peru, and Nicaragua, people tend to focus on improving lifestyle habits, changing personal relationships, or tweaking their work situations. Basically, it’s all about making adjustments in their personal lives to manage mental health. Now, compare that to the United Kingdom, Switzerland, and Canada, where the approach leans more on professional support. Here, you see people taking prescribed medication or reaching out to mental health professionals. It makes sense, given the healthcare systems in these countries make it easier to access professional help.\nIn the United States, Italy, and Albania, spending time in nature is a popular way to cope with mental health challenges. It’s not just about getting outside—it’s recognizing the stress-relieving benefits of the natural world and how it helps boost mental well-being. Meanwhile, in places like Kenya, Nigeria, and Sri Lanka, religious or spiritual activities are a common go-to. It highlights how, in these lower-middle-income countries, mental health care is often intertwined with cultural and spiritual traditions. And then you’ve got the Philippines, Zimbabwe, and Laos, where people are more likely to talk to friends or family. This shows just how much social support and community connections matter when it comes to managing anxiety and depression.\nSo what’s the takeaway? While talking to friends is one of the most common ways to deal with mental health issues, the coping strategies people use differ a lot from country to country. It all comes down to things like background, ethnicity, and economic conditions. In the end, coping often means a mix of social interaction, time outdoors, and healthy habits—all of which contribute to mental resilience. The results also remind us that mental health support isn’t one-size-fits-all. In wealthier countries, people might lean more on mental health professionals and medication, while in lower-middle-income nations, religious or spiritual practices play a bigger role.\nUnderstanding these differences is key to creating mental health strategies that actually work for people, no matter where they’re from. It’s a big step toward developing culturally relevant interventions that can help individuals across all walks of life.\nReferences\n\nWorld Health Organization. (2022, March 2). COVID-19 pandemic triggers 25% increase in prevalence of anxiety and depression worldwide. World Health Organization. https://www.who.int/news/item/02-03-2022-covid-19-pandemic-triggers-25-increase-in-prevalence-of-anxiety-and-depression-worldwide\nBell, C. A., Williams, J. L., & Cooper, S. E. (2021). Coping with the COVID-19 pandemic: A review of mental health strategies. Journal of Global Health, 11(3), 456-467. https://www.jogh.org/coping-with-the-covid-19-pandemic-a-review-of-mental-health-strategies"
  },
  {
    "objectID": "posts/Proj_2_wcd1/index.html",
    "href": "posts/Proj_2_wcd1/index.html",
    "title": "Exploring Fundamentals of Data",
    "section": "",
    "text": "In this exploration, I develop an understanding of data collection, by utilising open data sources to extract and organise data to solve a problem. This includes the following learning outcomes:\n\nLimitations of various types of data collection\nAccessing different data formats\nOrganising data in an informative and readable manner, to explore how the data can be used.\n\n\n\n\nSwift Parrot (Lathamus discolor)\nImage © Australian Museum / CC BY-NC"
  },
  {
    "objectID": "posts/Proj_2_wcd1/index.html#the-variables-are-curated-based-on-the-motivation",
    "href": "posts/Proj_2_wcd1/index.html#the-variables-are-curated-based-on-the-motivation",
    "title": "Exploring Fundamentals of Data",
    "section": "The variables are curated based on the motivation",
    "text": "The variables are curated based on the motivation\nWhich facilities among Toowoomba, QLD or Geelong, VIC would have the least interaction with the local swift parrot populations\n\ndecimalLatitude: Latitude\ndecimalLongitude: Logitude\neventDate : Date of occurrence\noccurrenceStatus: Status of Occurrence\nscientificName: Scientific Name"
  },
  {
    "objectID": "posts/Proj_2_wcd1/index.html#to-get-started-in-reading-the-data",
    "href": "posts/Proj_2_wcd1/index.html#to-get-started-in-reading-the-data",
    "title": "Exploring Fundamentals of Data",
    "section": "To get started in reading the data:",
    "text": "To get started in reading the data:\n\n\nShow code\nlibrary(tidyverse)\nlibrary(galah)\nlibrary(dplyr)\nairport1 &lt;- tibble(lat = -27.5538, long = 151.7956, name = \"Toowoomba\") \nairport2 &lt;- tibble(lat = -38.0220, long = 144.4768, name = \"Geelong\") \n\ngalah_config(email = \"poojarajendranraju@gmail.com\",\n             download_reason_id = 3, \n             verbose = TRUE)\n\nswift_parrot &lt;- galah_call() %&gt;% \n  galah_identify(\"Lathamus discolor\") %&gt;% galah_filter(year &gt;= 2000) %&gt;% atlas_occurrences() \n\n\n-----\n\n\nShow code\nstorage1_coordinates &lt;- swift_parrot %&gt;%  filter(between(decimalLongitude, min(airport1$long)-1, max(airport1$long)+1),\n         between(decimalLatitude, min(airport1$lat)-1, max(airport1$lat)+1))\n\nstorage2_coordinates &lt;- swift_parrot %&gt;%  filter(between(decimalLongitude, min(airport2$long)-1, max(airport2$long)+1),\n         between(decimalLatitude, min(airport2$lat)-1, max(airport2$lat)+1))\n\ncombined_dataset &lt;- rbind(storage1_coordinates,storage2_coordinates)\n\n\nstorage_filtered &lt;- combined_dataset %&gt;% select(scientificName, decimalLatitude, decimalLongitude, eventDate, occurrenceStatus)\n\nsave( storage_filtered, file = \"data/storage.rda\")\nload(here::here(\"posts/Proj_2_wcd1/data/storage.rda\"))"
  },
  {
    "objectID": "posts/advr3/index.html",
    "href": "posts/advr3/index.html",
    "title": "Global Dietary Trends",
    "section": "",
    "text": "Code\n# Load raw data from targets\nraw_data &lt;- tar_read(raw_data)"
  },
  {
    "objectID": "posts/advr3/index.html#introduction",
    "href": "posts/advr3/index.html#introduction",
    "title": "Global Dietary Trends",
    "section": "Introduction",
    "text": "Introduction\nThis report focuses on the analysis of daily caloric supply derived from carbohydrates, protein, and fat across multiple countries from 1961 to the most recent available year 2022. The objective is to examine global dietary patterns over time, identify regional differences in macronutrient consumption, and explore trends in protein intake from animal-based and plant-based sources.\n\nData\n\n\nCode\n# Glimpse of data\n\nglimpse(raw_data)\n\n\nRows: 12,877\nColumns: 7\n$ Entity                                                          &lt;chr&gt; \"Afgha…\n$ Code                                                            &lt;chr&gt; \"AFG\",…\n$ Year                                                            &lt;int&gt; 1961, …\n$ Daily.calorie.supply.per.person.that.comes.from.animal.protein  &lt;dbl&gt; 52.592…\n$ Daily.calorie.supply.per.person.that.comes.from.vegetal.protein &lt;dbl&gt; 277.46…\n$ Daily.calorie.supply.per.person.from.fat                        &lt;dbl&gt; 328.06…\n$ Daily.calorie.supply.per.person.from.carbohydrates              &lt;dbl&gt; 2256.2…\n\n\nOur World in Data is an online platform that provides accessible and comprehensive data on global development issues. The following dataset is used from the platform for analysis:\nDaily caloric supply derived from carbohydrates, protein and fat: The dataset, provided by Our World in Data, is sourced from the Food and Agriculture Organization (FAO) of the United Nations. It documents the average daily caloric supply per person derived from three primary macronutrients, carbohydrates, protein, and fat, for numerous countries and regions globally. The data spans several decades, offering valuable insights into nutritional trends and shifts over time.\n\nSource: Our World in Data – Daily Caloric Supply Derived from Carbohydrates, Protein, and Fat\nData Provider: Food and Agriculture Organization (FAO), United Nations\nLicense: Creative Commons Attribution 4.0 International License (CC BY 4.0), which allows for sharing and adaptation with proper credit.\nData Collection: Data is collected annually and reflects national food supply estimates, providing an overview of average caloric intake per person per day.\n\n\n\ncaloric_supply Dataset Structure\nThe dataset consists of the following columns:\n\nEntity: The name of the country or region. Code The ISO 3166-1 alpha-3 country code.\nYear: The year of the data observation.\nAnimal Protein (kcal/person/day): Daily per capita caloric supply from animal-based proteins.\nVegetal Protein (kcal/person/day): Daily per capita caloric supply from plant-based proteins.\nFat (kcal/person/day): Daily per capita caloric supply from fats.\nCarbohydrates (kcal/person/day): Daily per capita caloric supply from carbohydrates.\n\nEach row represents the average daily caloric intake for a specific country and year, broken down by macronutrient type.\n\n\nData Limitations\nWhile the dataset provides comprehensive coverage of global dietary patterns, there are some limitations to consider:\n\nMissing Country Codes: Some regions or combined areas (e.g., “Africa,” “Asia”) do not have ISO country codes, which may affect regional analyses.\nSupply vs. Consumption: The data represents the food supply rather than actual consumption, as it does not account for food waste, loss during transport, or spoilage.\nAnnual Aggregation: The dataset provides annual averages, which do not capture seasonal dietary changes.\nLimited Protein Breakdown: The data only distinguishes between animal-based and plant-based protein sources, without further granularity (e.g., beef vs. chicken for animal protein, or grains vs. legumes for vegetal protein).\nLack of Micronutrient Data: Vitamins, minerals, and other micronutrients are not represented in this dataset."
  },
  {
    "objectID": "posts/advr3/index.html#data-wrangling",
    "href": "posts/advr3/index.html#data-wrangling",
    "title": "Global Dietary Trends",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nRenaming Variables\n\n\nCode\n# Load renamed data\nrenamed_data &lt;- tar_read(renamed_data)\n\n# Display structure\nglimpse(renamed_data)\n\n\nRows: 12,877\nColumns: 7\n$ Country         &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanis…\n$ Code            &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\"…\n$ Year            &lt;int&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, …\n$ Animal_Protein  &lt;dbl&gt; 52.59264, 52.41573, 55.23608, 55.75903, 58.14664, 62.7…\n$ Vegetal_Protein &lt;dbl&gt; 277.4622, 270.2443, 244.7503, 269.1064, 268.2374, 245.…\n$ Fat             &lt;dbl&gt; 328.0627, 329.0468, 337.5722, 341.0036, 347.9166, 349.…\n$ Carbohydrates   &lt;dbl&gt; 2256.246, 2183.914, 1986.155, 2206.713, 2201.896, 2005…\n\n\n\n\nMissing Value Analysis\n\n\nCode\n#Load trimmed data\ntrimmed_data &lt;- tar_read(trimmed_data)\n\n#Convert empty strings to NA\nconverted_data &lt;- tar_read(converted_data)\n\n#Detect missing values\nmissing_values &lt;- tar_read(missing_values)\n\n# Display missing value count\nmissing_values\n\n\n        Country            Code            Year  Animal_Protein Vegetal_Protein \n              0            2661               0               0               0 \n            Fat   Carbohydrates \n              0               0 \n\n\n\nIn the entire dataset only variable Code has missing values.\n\nIt currently has 2661 missing codes, which is 4.5% of the total dataset.\n\n\n\nEvaluating and Handling Missing values\n\nAfter evaluating the missing values in the dataset, we can see that only column Code has missing values. The missing values are primarily associated with Country column, which contains regions or combined areas (e.g., “Africa,” “Asia”) that do not have ISO country codes.\nFor the analysis, we will focus on countries with valid ISO codes and exclude regions or combined areas without specific country codes.\nTherefore, the following aggregated regions such as FAO regional groups, income categories, and political regions were removed as part of the data cleaning process. These entries represent regional or economic aggregates that do not correspond to specific, identifiable nations. Including them in the analysis would distort country-level comparisons and mask the true performance of individual nations in terms of macronutrient consumption.\n\n\n\nCode\n# Load cleaned data after removing regions\ncleaned_data &lt;- tar_read(data_cleaned)\n\n#Display missing values after cleaning\npost_clean &lt;- tar_read(missing_post_clean)\npost_clean\n\n\n        Country            Code            Year  Animal_Protein Vegetal_Protein \n              0               0               0               0               0 \n            Fat   Carbohydrates \n              0               0 \n\n\n\n\nDuplicates Analysis\n\n\nCode\n#Duplicate check\nduplicate_count &lt;- tar_read(duplicate_check)\nduplicate_count\n\n\n[1] 0\n\n\n\nThe data cleaning process revealed no duplicate entries in the dataset. This ensures that each observation is unique and accurately represents a specific country and year combination."
  },
  {
    "objectID": "posts/advr3/index.html#exploratory-data-analysis",
    "href": "posts/advr3/index.html#exploratory-data-analysis",
    "title": "Global Dietary Trends",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nGlobal Trends in Macronutrient Consumption\n\n\nCode\n# Load the summarized data by year\nsummary_data &lt;- tar_read(summary_data)\n\n#Plot global trends in macronutrient consumption\nplot_1 &lt;- tar_read(plot_trends)\nplot_1\n\n\n\n\n\n\n\n\n\nThis plot shows the absolute values of daily kilocalorie intake per person for each macronutrient (Animal Protein, Vegetal Protein, Fat, and Carbohydrates) from 1960 to 2020. Helps understand the growth in consumption volume over the years, highlighting the rise in fat intake and the relatively stable intake of proteins.\n\nCarbohydrates: Carbohydrates have consistently been the largest source of daily energy, providing over 1500 kcal per person per day. While relatively stable, there has been a slow but steady increase over time, suggesting that staples like rice, wheat, and maize continue to be dietary mainstays across the world.\nFat Consumption: Fat intake has shown a steady climb since 1961, with sharper increases noticeable after the 1980s. This growth likely reflects shifts towards more processed foods and calorie-dense diets as countries urbanize and food systems globalize. The steady rise in fat consumption suggests changing eating habits that prioritize richer, high-energy foods.\nProtein Intake: Both animal-based and plant-based proteins have seen modest growth, remaining under 500 kcal per person per day. Animal protein consumption has increased gradually, especially in wealthier nations with rising meat consumption. Vegetal protein remains steady, reflecting its role in more plant-based diets, especially in Asia and Africa.\n\n\n\nExploring Top 3 Countries for Animal Protein Consumption by Decade\n\n\nCode\n#Compute Top 3 for Animal Protein \ntop3_animal_protein &lt;- tar_read(top3_animal_protein)\n\n#Plot top 3 countries for Animal Protein by decade\nplot_top3_animal_protein &lt;- tar_read(plot_top3_animal_protein)\nplot_top3_animal_protein\n\n\n\n\n\n\n\n\n\nFrom the Analysis of Animal Protein Consumption by Decade the key observations are as follows:\n\nIceland’s Consistent Top Position Iceland consistently ranks as the highest consumer of animal protein per person across all observed decades. The average daily intake in Iceland was 358.1 kcal in 1960 and steadily increased to 426.1 kcal by 2010, with a slight decrease to 421 kcal in 2020. This trend indicates strong dietary reliance on animal-based food sources that has remained stable over time.\nShifts in Leading Countries Over Time While Iceland’s position remained constant, the second and third spots varied: In the 1960s and 1970s, Australia and New Zealand were prominent, reflecting their strong livestock industries. During the 1980s and 1990s, France emerged as a leading consumer alongside Hong Kong, which continued to rank highly in subsequent decades. By 2020, Mongolia entered the top three for the first time, suggesting increased animal protein consumption in the region.\nChanges in Consumption Levels The plot indicates an overall increase in animal protein consumption among the top-ranking countries over the six-decade span. Iceland’s average intake grew by approximately 68 kcal per person per day from 1960 to 2020, highlighting a steady upward trend. Notably, Hong Kong saw substantial growth, particularly evident from 2010 onwards, indicating rising consumption levels.\n\n\n\nMacronutrient Proportion Analysis\nThe stacked area chart illustrates the shifting proportions of global caloric intake from 1961 to 2022. It visualizes how different macronutrients—Animal Protein, Vegetal Protein, Fat, and Carbohydrates, contribute to the global dietary structure over time.\n\nThe plot represents the proportion of global caloric intake for each macronutrient annually.\nFor each year, the total caloric intake for all countries combined is computed, and the contribution of each macronutrient is expressed as a fraction of this total.\nThis perspective reveals the relative share of each macronutrient in the global diet, highlighting dietary shifts over the decades.\nThe stacked area design effectively captures the long-term trends, such as the steady dominance of carbohydrates, the gradual increase in fat consumption, and the relative stability of protein sources.\n\n\n\nCode\n#Aggregating by macronutrient\nsum_data &lt;- tar_read(sum_data)\n\n#Calculating proportions\ncalorie_prop &lt;- tar_read(calorie_prop)\n\n#Converting to wide format\nlong_prop &lt;- tar_read(long_prop)\n\n#Plot proportions of macronutrients\nplot_proportions &lt;- tar_read(plot_proportions)\nplot_proportions\n\n\n\n\n\n\n\n\n\n\nCarbohydrates Remain Dominant: Carbohydrates consistently account for the largest proportion of global caloric intake. Despite some fluctuations, their share has remained relatively stable over six decades, indicating a strong global reliance on carbohydrate-rich staples like grains and starchy vegetables.\nGradual Rise in Fat Consumption: The proportion of calories derived from fat has shown a noticeable increase, particularly since the 1980s. This growth reflects dietary shifts towards higher-fat food items, potentially driven by increased consumption of processed foods and oils. The rise is gradual but steady, signaling a global trend towards richer, calorie-dense diets.\nStable Share of Animal and Vegetal Protein: The relative contribution of Animal Protein and Vegetal Protein to the total caloric intake has remained largely unchanged. While absolute consumption has grown, their proportional share of the total global caloric intake has not shifted significantly. This indicates that while populations are consuming more food overall, the balance between protein sources remains steady.\n\nConclusion: The stacked area chart effectively captures long-term global dietary patterns. Carbohydrates continue to be the primary source of global energy intake. Fat consumption, while still secondary, is the only macronutrient with a visibly increasing share. Meanwhile, Animal and Vegetal Protein maintain consistent proportions, reflecting stable dietary roles over the decades."
  },
  {
    "objectID": "posts/advr3/index.html#modelling-fat-consumption",
    "href": "posts/advr3/index.html#modelling-fat-consumption",
    "title": "Global Dietary Trends",
    "section": "Modelling Fat Consumption",
    "text": "Modelling Fat Consumption\nThe time series for global fat consumption was created using the annual global average fat consumption data from 1961 to 2022. The summary statistics of the time series are as follows:\n\n\nCode\n# Preparing the data for time series modeling \nfat_ts &lt;- tar_read(fat_ts)\n\n#Summary time series data \nsummary_fat_ts &lt;- tar_read(summary_fat_ts)\nsummary_fat_ts\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  489.0   584.8   660.5   665.9   748.2   854.9 \n\n\n\nMinimum Consumption: 489 kcal per person per day (observed in the early 1960s).\n1st Quartile: 585 kcal per person per day.\nMedian: 661 kcal per person per day.\nMean: 666 kcal per person per day.\n3rd Quartile: 748 kcal per person per day.\nMaximum Consumption: 855 kcal per person per day (observed in recent years).\n\nThese figures reflect a significant rise in global fat consumption per person over time, with notable accelerations in the 1990s and 2010s. The observed upward trend indicates a gradual increase in dietary fat intake worldwide.\n\n\nCode\n#Time series plot\nplot_fat_ts &lt;- tar_read(plot_fat_ts)\nplot_fat_ts\n\n\n\n\n\n\n\n\n\n\nThe time series plot illustrates a steady and relatively linear increase in average fat consumption over the 61-year period. This consistent upward trend suggests a global dietary shift towards higher fat intake, influenced by changes in dietary habits, economic development, and food availability.\nThe slope of the increase indicates a gradual but persistent rise, suggesting that the global trend in fat consumption has not been significantly disrupted by global events during the observed period.\nThe maximum recorded average consumption of 855 kcal per person per day in recent years marks a considerable increase from the early 1960s.\n\n\nARIMA Model Fit\n\n\nCode\n# ARIMA model \nfat_arima &lt;- tar_read(fat_arima)\n\n#Model Summary\nfat_arima_summary &lt;- tar_read(fat_arima_summary)\nfat_arima_summary\n\n\nSeries: fat_ts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      5.9994\ns.e.  0.6371\n\nsigma^2 = 25.17:  log likelihood = -184.43\nAIC=372.87   AICc=373.08   BIC=377.09\n\nTraining set error measures:\n                      ME     RMSE      MAE          MPE      MAPE      MASE\nTraining set 0.007790046 4.935827 3.600444 -0.005469347 0.5343166 0.5723284\n                  ACF1\nTraining set 0.1625095\n\n\nThe ARIMA model selected by auto.arima() for global average fat consumption data is an ARIMA(0,1,0) with drift, indicating that the model captures the trend as a random walk with drift.\nModel Coefficient\n\nDrift: The model estimates an average increase of approximately 6 kcal per year in global average fat consumption. This reflects a gradual and consistent rise in fat intake per person over the decades.\nStandard Error of Drift: The standard deviation of the drift estimate is approximately 0.64, indicating relatively low variability around the average increase.\n\nModel Diagnostics\n\nResidual Variance: The variance of the residuals is around 25.17, indicating moderate variability in prediction errors. Log Likelihood: -184.43, suggesting that the model fits the data reasonably well.\nAIC: 372.87, AICc: 373.08, BIC: 377.09, indicating a well-fitted model without excessive complexity.\n\nError Metrics\n\nMean Error (ME): 0.008 kcal, indicating almost negligible bias.\nRoot Mean Square Error (RMSE): 4.94 kcal, reflecting the average error magnitude.\nMean Absolute Error (MAE): 3.60 kcal, showing that the model’s average absolute deviation from observed values is minimal.\nMean Absolute Percentage Error (MAPE): 0.53%, indicating very high predictive accuracy.\nAutocorrelation at Lag 1 (ACF1): 0.16, suggesting low autocorrelation in residuals.\n\n\n\nForecast\n\nForecast Analysis\n\n\nCode\n# ARIMA Forecasting\nfat_forecast &lt;- tar_read(fat_forecast)\n\n# Plot the forecast\nplot_fat_forecast &lt;- tar_read(plot_fat_forecast)\nplot_fat_forecast\n\n\n\n\n\n\n\n\n\nThe ARIMA model forecasts that global average fat consumption will continue to increase at an approximate rate of 6 kcal per person per day over the next decade (2023–2032).\n\nThe solid black line in the plot represents actual global average fat consumption from 1961 to 2022.\nThe blue line shows forecasted values for the next 10 years, indicating a consistent upward trend.\nThe light blue area marks the 80% confidence interval, and the dark blue area marks the 95% confidence interval. These intervals reflect the range within which future values are likely to fall.\n\n\n\n\nPredicted vs Actual\n\n\nCode\n# Load global trends with predictions\nglobal_trends_predicted &lt;- tar_read(global_trends_predicted)\n\n# Plot Actual vs Predicted\nplot_actual_vs_predicted &lt;- tar_read(plot_actual_vs_predicted)\nplot_actual_vs_predicted\n\n\n\n\n\n\n\n\n\n\nThe plot compares the actual global average fat consumption from 1961 to 2022 with the predicted values from the ARIMA(0,1,0) with drift model.\nThe red line shows observed fat consumption, while the dashed blue line represents predictions.\nThe model accurately captures the upward trend with minimal deviation, demonstrating strong predictive alignment.\nMinor discrepancies are visible during rapid increases but do not impact overall reliability.\nError metrics, RMSE of 4.94, MAE of 3.60, and MAPE of 0.53%, indicate high predictive accuracy.\n\n\n\nResidual Diagnostics\n\n\nCode\n# Residuals\nresidual_diagnostics_test &lt;- tar_read(residual_diagnostics_test)\nresidual_diagnostics_test\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0) with drift\nQ* = 9.3382, df = 10, p-value = 0.5003\n\n\nCode\n# Residual plot\nknitr::include_graphics(tar_read(residual_diagnostics_plot))\n\n\n\n\n\n\n\n\n\nThe ARIMA(0,1,0) with drift model was evaluated using residual diagnostics: - Ljung-Box Test: A p-value of 0.5003 indicates no significant autocorrelation, suggesting the model captured the time-dependent structure well. - Residual Plot: Residuals fluctuate around zero with occasional spikes but remain mostly stable. - ACF Plot: Minimal autocorrelation is observed, with most lags within confidence intervals. - Density Plot: Residuals are approximately normally distributed with slight skew.\nOverall, the diagnostics confirm the model’s reliability for forecasting global fat consumption."
  },
  {
    "objectID": "posts/advr3/index.html#model-output-and-eda-discussion",
    "href": "posts/advr3/index.html#model-output-and-eda-discussion",
    "title": "Global Dietary Trends",
    "section": "Model Output and EDA Discussion",
    "text": "Model Output and EDA Discussion\n\nThe ARIMA(0,1,0) with drift model effectively captures the upward trend in global fat consumption observed in the Global Trends in Macronutrient Consumption (1961 - 2022) plot. The model’s estimated drift of 5.99 kcal per person per year aligns with the steady increase shown in the EDA visualization, demonstrating its reliability in modeling long-term growth.\nThe Proportion of Global Caloric Intake by Macronutrient plot highlights fat’s rising share over time, which the ARIMA model projects into future periods. While carbohydrates remain dominant and protein sources stable, fat’s contribution has grown steadily. The Actual vs. Predicted plot shows strong alignment, with low error metrics (RMSE: 4.94, MAE: 3.60, MAPE: 0.53%), affirming predictive accuracy.\nResidual diagnostics confirm well-behaved residuals with no significant autocorrelation, validating the model’s capacity to capture fat consumption trends effectively.\n\nOverall, the ARIMA model not only matches historical EDA trends but also projects them confidently into the future, demonstrating accuracy and consistency in reflecting global dietary shifts."
  },
  {
    "objectID": "posts/advr3/index.html#conclusion",
    "href": "posts/advr3/index.html#conclusion",
    "title": "Global Dietary Trends",
    "section": "Conclusion",
    "text": "Conclusion\nThe analysis of global macronutrient consumption from 1961 to 2022 reveals clear trends: carbohydrates remain the largest source of calories, while fat consumption has steadily increased. The ARIMA(0,1,0) model effectively captured this upward trend, forecasting continued growth in fat intake over the next decade. Iceland consistently emerged as the top consumer of animal protein across decades, reflecting stable dietary patterns. Overall, the data indicates persistent global dietary habits, with rising fat consumption pointing to broader nutritional shifts and changing food preferences worldwide."
  },
  {
    "objectID": "posts/assign04-bettong/index.html",
    "href": "posts/assign04-bettong/index.html",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "",
    "text": "The purpose of this report is to explore the relationship between air pollution and weather in Australia. Four research questions will be addressed:\n\nDo higher temperatures lead to increase in the formation of ground-level ozone?\nDoes rain and snow wash out particular matter and water-soluble gases from the air?\nDo high pressure systems often lead to stagnant air trapping pollutants?\nDoes sunlight drive photochemical reactions that form secondary pollutants like ozone?\n\nTo answer these questions, data was pulled, cleaned, compiled, and analysed. The following sections will describe the data and then answer the research questions using visual inference and other analytical approaches."
  },
  {
    "objectID": "posts/assign04-bettong/index.html#introduction",
    "href": "posts/assign04-bettong/index.html#introduction",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "",
    "text": "The purpose of this report is to explore the relationship between air pollution and weather in Australia. Four research questions will be addressed:\n\nDo higher temperatures lead to increase in the formation of ground-level ozone?\nDoes rain and snow wash out particular matter and water-soluble gases from the air?\nDo high pressure systems often lead to stagnant air trapping pollutants?\nDoes sunlight drive photochemical reactions that form secondary pollutants like ozone?\n\nTo answer these questions, data was pulled, cleaned, compiled, and analysed. The following sections will describe the data and then answer the research questions using visual inference and other analytical approaches."
  },
  {
    "objectID": "posts/assign04-bettong/index.html#data-description",
    "href": "posts/assign04-bettong/index.html#data-description",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "Data Description",
    "text": "Data Description\n\nData Pull\nData were pulled from two data sources. First, air pollution data was pulled from OpenAQ (OpenAQ 2015) using the airpurifyr R package (Lakshika 2024). OpenAQ is the world’s largest open-source air quality data platform. Second, weather data was pulled from SILO (Long Paddock n.d.) using the weatherOz R package (Pires 2024). SILO is a database of Australian climate data from 1889 to the present. After air pollution and weather data were pulled, the data sets were joined based on date.\nData was pulled from Alphington and Footscray in Melbourne, Victoria. These locations were chosen because they had the most amount of data available over a full year. Our team originally wanted to pull data from both Melbourne and Sydney to be consistent with our analysis between the two cities, but that proved not possible because there was not a full year’s worth available for the same time period. So, our solution was to focus on pulling data from locations that had the most amount of data over the same time period. Ultimately, the best solution was to pull from Alphington and Footscray for 2022.\nEach observation in the air pollution data set was the value of a given air pollutant for a specific hour of a specific day. For the purpose of our analysis, these hourly data records were aggregated to daily records by calculating the average pollution values for each day. The weather data was already at a daily level, so no aggregation was necessary. Data sets were then saved as .rds files.\nThe following code was used to pull the data:\n\n\nCode\n#2022 Alphington\nalphington2022 &lt;- get_measurements_for_location(country = \"AU\", \n    location = \"Alphington\",\n    date_from = as.Date(\"2022-01-01\"),\n    date_to = as.Date(\"2022-12-31\")) %&gt;%\n    mutate(date = as.Date(date_utc)) %&gt;%\n  group_by(date, parameter, location, unit, lat, long, country) %&gt;%\n  mutate(avg_value = mean(value, na.rm = TRUE)) %&gt;%\n  distinct(date, parameter, location, unit, lat, long, country, avg_value) %&gt;%\n  mutate(avg_value = format(avg_value, scientific = FALSE),\n        lat = round(lat, 2), long = round(long, 2)) \n\n#2022 Footscray\nfootscray2022 &lt;- get_measurements_for_location(country = \"AU\", \n    location = \"Footscray\",\n    date_from = as.Date(\"2022-01-01\"),\n    date_to = as.Date(\"2022-12-31\")) %&gt;%\n  mutate(date = as.Date(date_utc)) %&gt;%\n  group_by(date, parameter, location, unit, lat, long, country) %&gt;%\n  mutate(avg_value = mean(value, na.rm = TRUE)) %&gt;%\n  distinct(date, parameter, location, unit, lat, long, country, avg_value) %&gt;%\n  mutate(avg_value = format(avg_value, scientific = FALSE),\n        lat = round(lat, 2), long = round(long, 2)) \n\n#union the data\nair_data &lt;- rbind(alphington2022,footscray2022) |&gt; \n  mutate(date = as.Date(date))\n\n#save air pollution data as RDS\nsaveRDS(air_data, file = \"airdata.rds\")\n\n#pull silo data\nsilo &lt;- get_data_drill(\n  latitude = -38,\n  longitude = 145,\n  start_date = \"20220101\",\n  end_date = \"20221231\",\n  values = c(\n         \"max_temp\",\n         \"min_temp\",\n         \"rain\", \n         \"mslp\",\n         \"vp\",\n         \"vp_deficit\",\n         \"radiation\"\n     )\n ) %&gt;% rename(lat = latitude, long = longitude) \n\n#save silo data as RDS\nsaveRDS(silo, file = \"silo.rds\")\n\n\n\n\nData Cleaning and Joining\nAfter the data was pulled, they were cleaned, made into a tidy format, and joined together. This involved removing unnecessary variables and using a left join that prioritized the air pollution data. The following code was used for this process:\n\n\nCode\nair_data &lt;- readRDS(file = \"airdata.rds\")\nsilo &lt;- readRDS(file = \"silo.rds\")\njoin &lt;- left_join(air_data, silo, by = c(\"date\"))\n\n#filter out unnecessary variables\nair_and_weather_data &lt;- join %&gt;%\n  ungroup() |&gt;   \n  select(-country, -lat.x, -long.x, -long.y, -lat.y, -extracted, -year, \n         -air_tmax_source, -air_tmin_source, -rainfall_source, -elev_m,\n         -mslp_source, -radiation_source, -vp_deficit_source, -vp_source)\n\nwrite.csv(air_and_weather_data, file = \"data/air_and_weather_data.csv\", row.names = FALSE)\n\n\n\n\nData Definitions\nThe dataset contains 14 variables and 4261 observations.\nThe following are the variables used in this analysis:\n\n\n\n\n\n\n\n\nVariable\nDescription\nSource\n\n\n\n\ndate\nDate when the data was recorded\nOpenAQ and SILO\n\n\nparameter\nPollutant parameters (ex: pm25, pm10, so3)\nOpenAQ\n\n\nlocation\nLocation name\nOpenAQ\n\n\nunit\nUnit of measurement\nOpenAQ\n\n\navg_value\nMeasurement of parameter for that day\nOpenAQ\n\n\nmonth\nMonth when the data was recorded\nSILO\n\n\nday\nDay when the data was recorded\nSILO\n\n\nair_tmax\nThe maximum temperature for the day in C\nSILO\n\n\nair_tmin\nThe minimum temperature for the day in C\nSILO\n\n\nmslp\nMean sea level pressure in hPa\nSILO\n\n\nradiation\nSolar exposure in \\(MJ/m^2\\)\nSILO\n\n\nrainfall\nDaily rainfall in mm\nSILO\n\n\nvp\nVapour pressure in hPa\nSILO\n\n\nvp_deficit\nVapour pressure deficit in hPa\nSILO\n\n\n\n\n\nData Screening\n\n\nCode\nvis_dat(air_and_weather_data)\n\n\n\n\n\n\n\n\n\nUnderstanding the composition of variable types in the dataset using ‘vis_dat’. Here, the dataset primarily comprises of character type variables, date type variables, numeric and integer variables.\nChecking for Duplicate Rows\n\n\nCode\n# Detecting duplicate rows\nsum(duplicated(air_and_weather_data))\n\n\n[1] 0\n\n\nThere are no duplicate rows observed in the dataset.\nEvaluating the Structure of Data\n\n\nCode\nglimpse(air_and_weather_data)\n\n\nRows: 4,261\nColumns: 14\n$ date       &lt;date&gt; 2022-12-31, 2022-12-30, 2022-12-29, 20…\n$ parameter  &lt;chr&gt; \"pm25\", \"pm25\", \"pm25\", \"pm25\", \"pm25\",…\n$ location   &lt;chr&gt; \"Alphington\", \"Alphington\", \"Alphington…\n$ unit       &lt;chr&gt; \"µg/m³\", \"µg/m³\", \"µg/m³\", \"µg/m³\", \"µg…\n$ avg_value  &lt;chr&gt; \"9.9\", \"11\", \"5.8\", \"6.2\", \"6.4\", \"12\",…\n$ month      &lt;dbl&gt; 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,…\n$ day        &lt;int&gt; 31, 30, 29, 28, 27, 26, 25, 24, 23, 22,…\n$ air_tmax   &lt;dbl&gt; 29, 26, 20, 26, 38, 29, 30, 22, 22, 24,…\n$ air_tmin   &lt;dbl&gt; 17.2, 12.6, 12.9, 24.5, 16.9, 16.3, 11.…\n$ mslp       &lt;dbl&gt; 1014, 1016, 1019, 1008, 1018, 1017, 101…\n$ radiation  &lt;dbl&gt; 29.8, 32.2, 18.8, 4.7, 31.7, 29.6, 32.2…\n$ rainfall   &lt;dbl&gt; 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.4,…\n$ vp         &lt;dbl&gt; 18.6, 15.3, 10.4, 18.4, 20.4, 17.9, 15.…\n$ vp_deficit &lt;dbl&gt; 15.3, 12.1, 10.3, 15.2, 28.7, 15.8, 17.…\n\n\nAll variables are of appropriate data types apart from avg_value which is currently a chr variable and needs to be converted to dbl.\n\n\nCode\nair_and_weather_data$avg_value &lt;- as.numeric(air_and_weather_data$avg_value)\n\n\n\n\nChecking Missing Data\n\n\nCode\n#Converting data to wider form to explore missingness \ndata_wider &lt;- air_and_weather_data |&gt; \n             select(-unit)  |&gt;\n           pivot_wider(names_from = parameter,\n                       values_from = avg_value)\n#write.csv(data_wider, file = \"data/data_wider.csv\", row.names = FALSE)\n\n# Calculating the missing percentage\nmissing_summary &lt;- data_wider |&gt; \n  summarise(across(everything(), ~ sum(is.na(.)) / n() *100)) |&gt; \n  pivot_longer(cols = everything(),\n               names_to = \"variable\", \n               values_to = \"missing_percentage\") \n\n#Plotting the missing percentage for each variable\nggplot(missing_summary, \n       aes(x = reorder(variable,-missing_percentage), \n           y = missing_percentage))  +\n  geom_segment(aes(xend = variable, yend = 0), color = \"blue\") +\n  geom_point(size = 2, color = \"black\") +\n  coord_flip() +\n  labs(title = \"Missing Data Percentage\",\n       x = \"Pollutant\",\n       y = \"Missing Percentage (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the above plot, we can infer that the data is relatively complete with minimal missing values. Parameters no2, co, pm10 and pm25 have missing values less than 5%.\nSpecific Date Analysis of Missing Data\n\n\nCode\nmissing_specific &lt;- data_wider |&gt;\n  as_tsibble(key = location, index = date)\n\ncount_gaps(missing_specific)\n\n\n# A tibble: 7 × 4\n  location   .from      .to           .n\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;int&gt;\n1 Alphington 2022-02-06 2022-02-06     1\n2 Alphington 2022-03-23 2022-03-24     2\n3 Footscray  2022-02-06 2022-02-06     1\n4 Footscray  2022-03-23 2022-03-24     2\n5 Footscray  2022-09-25 2022-09-25     1\n6 Footscray  2022-11-23 2022-11-23     1\n7 Footscray  2022-12-18 2022-12-18     1\n\n\nBetween February and December 2022, both Alphington and Footscray had overlapping gaps on February 6th and from March 23rd to March 24th, while Footscray experienced unique missing data on the other three days: September 25th, November 23rd, and December 18th."
  },
  {
    "objectID": "posts/assign04-bettong/index.html#relationship-1",
    "href": "posts/assign04-bettong/index.html#relationship-1",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "Relationship 1",
    "text": "Relationship 1\nHigher temperatures can increase the formation of ground-level ozone\nThe current research question explores whether higher temperatures drive the formation of ground-level ozone, a secondary pollutant influenced by atmospheric conditions. This analysis focuses on ozone (O3) levels, examining how variations in daily maximum and minimum temperatures (measured in degrees Celsius) relate to ozone concentrations. The following section investigates the correlation between temperature and ozone, aiming to understand the extent to which warmer conditions may promote ozone formation.\nEvaluating Monthly Temperature Variation\n\n\nCode\ndata_wider$month &lt;- as.factor(data_wider$month)\n\n# Max Daily Temp\np1 &lt;- ggplot(data_wider, aes(x=month, y=air_tmax)) +\n  geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), fill= \"#56B4E9\") +\n  labs(x = \"Month\", y = \"Max Daily Temp (°C)\") + \n  theme_minimal()\n  \n\n# Min Daily Temp\np2 &lt;- ggplot(data_wider, aes(x=month, y=air_tmin)) +\n  geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), fill= \"#56B4E9\") +\n  labs(x = \"Month\", y = \"Min Daily Temp (°C)\") +\n  theme_minimal() \n\np1/p2\n\n\n\n\n\n\n\n\n\n\nThe violin plot provides context on the monthly maximum and minimum temperature distribution for Melbourne. Both maximum and minimum daily temperatures are highest during the summer months (December to February), especially in January, while the winter months (June to August) experience the lowest and most stable temperatures.\nApril and October show moderate temperature variability, serving as transitional months. Overall, summer and early autumn months are warmer, so if higher temperatures increase the formation of ground-level ozone, elevated ozone levels should be more apparent in these months. In contrast, winter months, with their consistently lower temperatures, would likely see reduced ozone formation.\n\nEvaluating Average Variation of Ozone compared to Max and Min Temperature\n\n\nCode\n# Average tmax, tmin, and O3 levels\ntemp_oz_summary &lt;- data_wider %&gt;%\n  group_by(month) %&gt;%\n  summarise(avg_tmax = mean(air_tmax, na.rm = TRUE),\n            avg_tmin = mean(air_tmin, na.rm = TRUE),\n            avg_o3 = mean(o3, na.rm = TRUE))\n\n\nggplot(temp_oz_summary, aes(x = month)) +\n  geom_line(aes(y = avg_tmax, \n                color = \"Max Temperature (°C)\", group = 1), size = 1.2) + \n  geom_line(aes(y = avg_tmin, \n                color = \"Min Temperature (°C)\", group = 1), size = 1.2) +                 geom_line(aes(y = avg_o3 * 1000, \n                color = \"Ozone (O3 in ppm)\", group = 1), size = 1.2) +                    scale_y_continuous(name = \"Temperature (°C)\", \n                     sec.axis = sec_axis(~./1000, name = \"Ozone (ppm)\")) +  \n  labs(x = \"Month\", \n       title = \"Average Max Temp, Min Temp, and Ozone Levels Over 12 Months\") +\n  scale_color_manual(values = c(\"Max Temperature (°C)\" = \"blue\", \n                                \"Min Temperature (°C)\" = \"green\",\n                                \"Ozone (O3 in ppm)\" = \"red\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())  \n\n\n\n\n\n\n\n\n\n\nThese line graphs show the average maximum temperature, minimum temperature, and ozone levels across each month. The maximum and minimum temperatures follow a typical seasonal pattern, as observed in the earlier violin plots.\nOzone levels display a clear seasonal pattern aligned with temperature, peaking during the summer months (December to February) when both maximum and minimum daily temperatures are highest. This supports the expectation that higher temperatures facilitate ground-level ozone formation. Ozone levels remain relatively elevated in March before declining as temperatures start to drop in autumn.\nDuring the winter months (June to August), ozone levels dip to their lowest, particularly in July, which aligns with the coldest temperatures of the year.\nHowever, there are unexpected ozone peaks in June and September, which do not correspond with high temperatures. These anomalies suggest that factors such as high-pressure systems or local emissions may occasionally cause ozone levels to rise independently of temperature, particularly during transitional or atypical weather periods.\n\nExamining Relationships Using Scatterplots\n\n\nCode\n# air_tmax vs. O3\np3 &lt;- ggplot(data_wider, aes(y = o3, x = air_tmax)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"Max Temperature vs Ozone\",\n       y = \"Ozone (ppm)\",\n       x = \"Max Temperature (°C)\") +\n  theme_minimal()\n\n# air_tmin vs. O3\np4 &lt;- ggplot(data_wider, aes(y = o3, x = air_tmin)) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  labs(title = \"Min Temperature vs Ozone\",\n       y = \"Ozone (ppm)\",\n       x = \"Min Temperature (°C)\") +\n  theme_minimal()\n\n\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\n\n\nBased on the scatter plots the following observations can be made:\nThere is a moderately positive relationship between maximum temperature and ozone, indicated by the steeper slope and tighter clustering of points around the trend line. This suggests that as daily maximum temperatures rise, ozone levels tend to increase.\nSimilarly, there is a positive relationship between minimum temperature and ozone, though it appears slightly weaker based on the lower gradient of the trend line and more dispersed points around it. This indicates that while warmer minimum temperatures are also associated with higher ozone levels, the effect is less pronounced than for maximum temperatures.\nOverall, both maximum and minimum temperature trends provide evidence that temperature plays a key role in ozone formation. Further analysis through linear modeling could help quantify these relationships more precisely.\n\nEvaluating How The Daily Temperature Difference Affects Ozone Formation\n\n\nCode\n# Daily temperature range (tmax - tmin)\ndata_wider$temperature_range &lt;- data_wider$air_tmax - data_wider$air_tmin\n\n# Ozone vs Temperature Range\nggplot(data_wider, aes(x = temperature_range, y = o3)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Ozone vs Temperature Range (tmax - tmin)\",\n       x = \"Temperature Range (°C)\", y = \"Ozone (ppm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nThere is a slight tendency for ozone levels to increase as the daily temperature range grows, but the relationship is weak, as indicated by the nearly flat trend line.\nThis minimal increase suggests that while a larger temperature range might have some influence, it is not a strong determinant of ozone formation.\nOverall, absolute temperatures (highs or lows) are more important for ozone formation than the daily difference between maximum and minimum temperatures.\n\n\nLinear Models\n\nLinear Modeling of Ozone with Max Temperature\n\n\n\nCode\nlm_tmax &lt;- lm(o3 ~ air_tmax, data = data_wider)\n\nsummary(lm_tmax)\n\n\n\nCall:\nlm(formula = o3 ~ air_tmax, data = data_wider)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.015001 -0.003620 -0.000515  0.003845  0.016662 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.59e-03   7.37e-04    7.59    1e-13 ***\nair_tmax    4.59e-04   3.58e-05   12.83   &lt;2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0055 on 719 degrees of freedom\nMultiple R-squared:  0.186, Adjusted R-squared:  0.185 \nF-statistic:  165 on 1 and 719 DF,  p-value: &lt;2e-16\n\n\n\nThe linear model shows a statistically significant positive relationship between maximum temperature and ozone levels: for each 1°C increase in maximum temperature, ozone levels increase by an estimated 0.000459 ppm.\nWith extremely low p-values for both the intercept and slope, the model confirms that maximum temperature is a highly significant predictor of ozone levels. Additionally, the F-statistic p-value is far below the 0.01 threshold, indicating very strong overall model significance.\nHowever, the model’s R-squared value (0.186) suggests that maximum temperature explains only 18.6% of the variation in ozone levels, indicating that other factors also influence ozone formation.\n\n\nLinear Modeling of Ozone with Min Temperature\n\n\n\nCode\nlm_tmin &lt;- lm(o3 ~ air_tmin, data = data_wider)\n\nsummary(lm_tmin)\n\n\n\nCall:\nlm(formula = o3 ~ air_tmin, data = data_wider)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.014985 -0.003807 -0.000673  0.003733  0.021601 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.009728   0.000555   17.52   &lt;2e-16 ***\nair_tmin    0.000454   0.000047    9.65   &lt;2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0057 on 719 degrees of freedom\nMultiple R-squared:  0.115, Adjusted R-squared:  0.113 \nF-statistic: 93.2 on 1 and 719 DF,  p-value: &lt;2e-16\n\n\n\nThe linear model shows a statistically significant positive relationship between minimum temperature and ozone levels: for each 1°C increase in minimum temperature, ozone levels increase by an estimated 0.000454 ppm.\nWith extremely low p-values for both the intercept and slope, the model confirms that minimum temperature is a highly significant predictor of ozone levels. Additionally, the F-statistic p-value is far below the 0.01 threshold, indicating very strong overall model significance.\nHowever, the model’s R-squared value (0.115) suggests that minimum temperature explains only 11.5% of the variation in ozone levels, indicating that other factors also influence ozone formation.\n\n\n\nSeasonal Evaluation\nSeasonal Overview\n\n\nCode\ndata_wider &lt;- data_wider %&gt;%\n  mutate(month = factor(month, levels = 1:12, labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n                                                         \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")),\n         season = case_when(\n           month %in% c(\"Jan\", \"Feb\", \"Dec\") ~ \"Summer\",\n           month %in% c(\"Mar\", \"Apr\", \"May\") ~ \"Autumn\",\n           month %in% c(\"Jun\", \"Jul\", \"Aug\") ~ \"Winter\",\n           month %in% c(\"Sep\", \"Oct\", \"Nov\") ~ \"Spring\"\n         ))\n\nggplot(data_wider, aes(x = month)) +\n  geom_smooth(aes(y = air_tmax, group = 1), method = \"loess\", span = 0.7, se = FALSE, color = \"blue\", size = 1.2) +\n  geom_smooth(aes(y = air_tmin, group = 1), method = \"loess\", span = 0.7, se = FALSE, color = \"green\", size = 1.2) +\n  geom_smooth(aes(y = o3 * 1000, group = 1), method = \"loess\", span = 0.7, se = FALSE, color = \"red\", size = 1.2) +\n  facet_wrap(~ season, ncol = 2, scales = \"free_x\") +\n  scale_y_continuous(name = \"Temperature (°C)\", \n                     sec.axis = sec_axis(~./1000, name = \"Ozone (ppm)\")) +\n  labs(title = \"Seasonal Comparison of Temperature and Ozone Levels\",\n       x = \"Month\", y = \"Temperature (°C)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(hjust = 1),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nThe above plot illustrates a strong seasonal pattern where ozone levels closely follow maximum temperature trends.\nSummer - Summer has the highest ozone levels, aligning with the warmest temperatures. Maximum and minimum temperatures are high and stable. Ozone levels peak in January, followed closely by December, and decrease in February.\nAutumn - Autumn shows a gradual decline in both temperatures and ozone levels, contrasting with the elevated levels seen in summer. Both maximum and minimum temperatures decline steadily. Ozone levels also decrease, following the cooling trend.\nWinter - Winter has the lowest temperatures and ozone levels, emphasizing a strong seasonal effect on ozone reduction.Temperatures reach their lowest, with both maximum and minimum values consistently low. Ozone levels are at their lowest, dipping particularly in July.\nSpring - Spring mirrors autumn as a transitional season, but with an upward trend in both temperature and ozone, leading into the high summer levels. Temperatures begin to rise gradually. Ozone levels increase as temperatures warm, reaching relatively high levels by November.\n\nOverall, Ozone levels closely follow seasonal temperature trends, peaking in summer (especially January) and reaching their lowest in winter. Autumn and spring serve as transitional periods, with autumn showing a cooling and decreasing trend in ozone, while spring shows warming and increasing ozone levels.\nPlotting Daily Variations Across Each Month\n\n\nCode\np5 &lt;- ggplot(data_wider, aes(y = o3, x = air_tmax)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE) + \n  facet_wrap(~month, ncol = 4) +\n  scale_y_continuous(\"Ozone (ppm)\", breaks = seq(0, max(data_wider$o3, na.rm = TRUE), by = 0.01)) +\n  ylab(\"Max Daily Temp (°C)\") +\n  theme(aspect.ratio = 0.4)\n\np6 &lt;- ggplot(data_wider, aes(y = o3, x = air_tmin)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(se = FALSE) + \n  facet_wrap(~month, ncol = 4) +\n  scale_y_continuous(\"Ozone (ppm)\", breaks = seq(0, max(data_wider$o3, na.rm = TRUE), by = 0.01)) +\n  ylab(\"Max Daily Temp (°C)\") +\n  theme(aspect.ratio = 0.4)\n\np5 \n\n\n\n\n\n\n\n\n\nCode\np6\n\n\n\n\n\n\n\n\n\n\nAcross all months, there is a generally positive relationship between temperature and ozone levels, with maximum temperature (air_tmax) showing a stronger association with ozone than minimum temperature (air_tmin). As temperatures increase, ozone levels tend to rise, particularly in warmer months with higher maximum temperatures.\nMonthly variations also reflect these seasonal trends. In summer, especially January, there is a strong, consistent positive relationship between temperature and ozone levels, with warmer months correlating closely with higher ozone levels. In autumn, the positive trend remains but becomes more variable, indicating increased scatter in the temperature-ozone relationship. Winter shows a weaker, less consistent pattern, with minimal correlation between temperature and ozone. By spring, the positive relationship between temperature and ozone begins to stabilize again, especially in November, as temperatures rise toward summer.\nSome anomalies are evident, such as an unexpected dip in December’s ozone levels at higher minimum temperatures (air_tmin). Additionally, winter months exhibit a weaker pattern, suggesting that factors beyond temperature may influence ozone levels during these periods, a finding also supported by the linear model.\n\n\n\nPermutation and Lineup Test\nLine up for Max Temperature and O3\n\n\nCode\nset.seed(20190709)\nggplot(lineup(null_permute('air_tmax'), data_wider), \n  aes(x=air_tmax, y=o3)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"o0vr 8ZGZ D3 k5fDGD53 M1\")\n\n\nLine up for Min Temperature and O3\n\n\nCode\nset.seed(32632632)\nggplot(lineup(null_permute('air_tmin'), data_wider), \n  aes(x=air_tmin, y=o3)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"o0vr 8ZGZ D3 k5fDGD53 eV\")\n\n\n\nHere, we used visual inference to assess whether a relationship exists between temperature (min & max) and ozone levels. We applied permutation and randomly shuffled the temperature values and plotted them alongside ozone levels across 19 null plots for each line up.\n\nDuring our visual inference test, participants unanimously identified the real plot in both lineups, demonstrating clear visual relationships between temperature and O3 levels. In the first lineup for maximum temperature and O3, plot 20 was consistently chosen due to its distinct upward trend. Similarly, in the second lineup for minimum temperature and O3, plot 6 stood out as the real plot, with all participants correctly identifying it.\nThese results indicate that both maximum and minimum temperatures have statistically significant and visually distinct relationships with O3 levels. Although maximum temperature may explain more variation in ozone levels, both relationships were equally recognizable, highlighting how both min and max temperatures serve as visually reliable indicators of ozone variation in our analysis.\n\n\n\nConclusion for Relationship 1\nThe analysis provides strong evidence that higher temperatures increase the formation of ground-level ozone (O3). Monthly trends show ozone peaking during warmer months, while scatter plots confirm a positive relationship between temperature and ozone. Linear models reveal that both maximum and minimum temperatures are significant predictors of ozone, with maximum temperature explaining 18.6% and minimum temperature 11.5% of ozone variability. Visual inference tests highlight that the maximum and minimum temperature relationships with O3 are visually identifiable, though maximum temperature provides slightly more insight into variations in ozone levels. Overall, the findings conclude that higher temperatures do indeed drive ground-level ozone formation, particularly during peak daytime temperatures."
  },
  {
    "objectID": "posts/assign04-bettong/index.html#relationship-2",
    "href": "posts/assign04-bettong/index.html#relationship-2",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "Relationship 2",
    "text": "Relationship 2\nRain and snow can “wash out” particulate matter and water-soluble gases from the air\nThe next research question is if rain or snow can wash out particulate matter and water-soluble gases from the air. According to the US National Institute of Health (Zhang 2024), water-soluble gases would include o3, so2, and no2. SILO provides daily rainfall (mm) measurements. The following section will explore the relationship between rainfall and pm25, pm10, o3, so2, and no2.\nEvaluating Monthly Rainfall Variation\n\n\nCode\ndata_wider$month &lt;- as.factor(data_wider$month)\n\nfiltered_rainfall_data_wider &lt;- data_wider %&gt;%\n  filter(rainfall &gt; 0)\n\n\nggplot(filtered_rainfall_data_wider, aes(x = month, y = rainfall)) +\n  geom_boxplot(fill = \"#56B4E9\") +  \n  labs(x = \"Month\", y = \"Rainfall (mm)\") +\n  theme(aspect.ratio = 0.5)\n\n\n\n\n\n\n\n\n\n\nThis boxplot provides context as to how much rainfall occurs for the two locations in Melbourne for every month. Overall, daily rainfall tends to be around 3mm with April and October having the highest variability.\nIn general, the winter and early spring months tend to have more rainfall than the summer and early autumn months. So, if rain is effective in washing out water-soluble air pollutants, one would expect to see less of those air pollutants during those months.\n\nEvaluating Average Variation of Pollutants compared to Rainfall\n\n\nCode\noptions(scipen=99)\n\nrain_avg_lines_table &lt;- data_wider %&gt;%\n  group_by(month) %&gt;%\n  summarise(avg_rainfall = mean(rainfall, na.rm = TRUE),\n            avg_so2 = mean(so2, na.rm = TRUE),\n            avg_o3 = mean(o3, na.rm = TRUE),\n            avg_no2 = mean(no2, na.rm = TRUE),\n            avg_pm25 = mean(pm25, na.rm = TRUE),\n            avg_pm10 = mean(pm10, na.rm = TRUE))\n\nrain_avg_lines_table &lt;- rain_avg_lines_table %&gt;%\n  mutate(month = as.numeric(month))\n\n# Create individual plots for each pollutant\no3_plot &lt;- ggplot(rain_avg_lines_table, aes(x = month, y = avg_o3)) +\n  geom_line(color = \"red\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg O3 Levels\", x = \"Month\", y = \"Avg o3 (ppm)\") +\n  theme_minimal()\n\nso2_plot &lt;- ggplot(rain_avg_lines_table, aes(x = month, y = avg_so2)) +\n  geom_line(color = \"blue\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg SO2 Levels\", x = \"Month\", y = \"Avg so2 (ppm)\") +\n  theme_minimal()\n\nno2_plot &lt;- ggplot(rain_avg_lines_table, aes(x = month, y = avg_no2)) +\n  geom_line(color = \"green\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg NO2 Levels\", x = \"Month\", y = \"Avg NO2 (ppm)\") +\n  theme_minimal()\n\npm25_plot &lt;- ggplot(rain_avg_lines_table, aes(x = month, y = avg_pm25)) +\n  geom_line(color = \"purple\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg PM2.5 Levels\", x = \"Month\", y = \"Avg PM2.5 (µg/m³)\") +\n  theme_minimal()\n\npm10_plot &lt;- ggplot(rain_avg_lines_table, aes(x = month, y = avg_pm10)) +\n  geom_line(color = \"pink\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg PM10 Levels\", x = \"Month\", y = \"Avg pm10 (µg/m³)\") +\n  theme_minimal()\n\nrainfall_plot &lt;-ggplot(rain_avg_lines_table, aes(x = month, y = avg_rainfall)) +\n  geom_line(color = \"lightblue\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg Rainfall\", x = \"Month\", y = \"Rainfall (mm)\") +\n       theme_minimal()\n\n# Arrange the plots horizontally using patchwork\ncombined_plot &lt;- o3_plot + so2_plot + no2_plot + pm25_plot + pm10_plot + rainfall_plot\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n\nThese line graphs show the average pollution and rainfall rates across each month. The average rainfall per month shows fairly stable rainfall with a spike in October and November. In general, if rainfall is to wash out air pollutants, we would expect pm25, pm10, o3, so2, and no2 levels to drop in the early spring and summer months.\nIn these charts, we can see that both no2 and so2 levels decrease from July to December, but it is not entirely clear if there is a relationship. Average o3 levels actually increase from July to December, which is opposite of what one would expect if rainfall were to wash air pollutants.\nAverage pm25 levels decreased from July to October but the steeply increase in November and December. And, average pm10 levels dropped from February to October, then rose in November and December like pm25. Diving deeper into the relationship of rainfall with these pollutants using scatter plots, regressions, and visual inference could provide more clarity.\n\nExamining Relationships Using Scatterplots\n\n\nCode\n# Scatter plot for rainfall vs o3\nscatter_rainfall_o3 &lt;- ggplot(data_wider, aes(y = o3, x = rainfall)) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  labs(title = \"Rainfall (mm) vs O3\",\n       y = \"Ozone (ppm)\",\n       x = \"Rainfall (mm)\") +\n  theme_minimal()\n\n# Scatter plot for rainfall  vs. so2\nscatter_rainfall_so2 &lt;- ggplot(data_wider, aes(y = so2, x = rainfall)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"Rainfall (mm) vs SO2\",\n       y = \"SO2 (ppm)\",\n       x = \"Rainfall (mm)\") +\n  theme_minimal()\n\n# Scatter plot for rainfall vs no2\nscatter_rainfall_no2 &lt;- ggplot(data_wider, aes(y = no2, x = rainfall)) +\n  geom_point(alpha = 0.5, color = \"green\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  labs(title = \"Rainfall vs NO2\",\n       y = \"NO2 (ppm)\",\n       x = \"Rainfall (mm)\") +\n  theme_minimal()\n\n# Scatter plot for rainfall vs pm25\nscatter_rainfall_pm25 &lt;- ggplot(data_wider, aes(y = pm25, x = rainfall)) +\n  geom_point(alpha = 0.5, color = \"purple\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#301934\") +\n  labs(title = \"Rainfall vs PM2.5\",\n       y = \"PM2.5 (µg/m³)\",\n       x = \"Rainfall (mm)\") +\n  theme_minimal()\n\n# Scatter plot for rainfall vs pm10\nscatter_rainfall_pm10 &lt;- ggplot(data_wider, aes(y = pm10, x = rainfall)) +\n  geom_point(alpha = 0.5, color = \"pink\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#e75480\") +\n  labs(title = \"Rainfall vs PM10\",\n       y = \"PM10 (µg/m³)\",\n       x = \"Rainfall (mm)\") +\n  theme_minimal()\n\n# Arrange the two plots side by side for easier comparison\ngrid.arrange(scatter_rainfall_o3, scatter_rainfall_so2, scatter_rainfall_no2, \n             scatter_rainfall_pm25, scatter_rainfall_pm10, ncol = 3)\n\n\n\n\n\n\n\n\n\n\nWhen daily rainfall rates are plotted with each air pollutant, two different stories emerge. There is a slightly positive relationship between rainfall o3. This means that as a location has more rain, there is also an increase in o3. This is contrary to the hypothesis that rain washes out the water-soluble air pollutants.\nHowever, there is a slightly negative relationship between rainfall and so2, no2, pm2.5, and pm10. This means that as a location has more rain, there is a decrease in sulfur dioxide, nitrogen dioxide, pm2.5, and pm10. This is aligned with the hypothesis that rain can wash out particulate matter and water-soluble air pollutants.\nSo, based on the scatter points and above plots, there is some evidence that rain can wash out so2, no2, pm2.5, and pm10 but not o3. Further testing using linear modeling and permutation can provide a more informed answer.\n\n\nLinear Models\n\nLinear Modeling of Rainfall with O3\n\n\n\nCode\n#rainfall affect on o3 - insignificant\nlm_rainfall_o3 &lt;- lm(o3 ~ rainfall, data = data_wider)\nsummary(lm_rainfall_o3)\n\n\n\nCall:\nlm(formula = o3 ~ rainfall, data = data_wider)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.014186 -0.003797 -0.000546  0.003454  0.024454 \n\nCoefficients:\n             Estimate Std. Error t value\n(Intercept) 0.0145461  0.0002455   59.25\nrainfall    0.0000660  0.0000482    1.37\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nrainfall                   0.17    \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0061 on 719 degrees of freedom\nMultiple R-squared:  0.0026,    Adjusted R-squared:  0.00121 \nF-statistic: 1.87 on 1 and 719 DF,  p-value: 0.172\n\n\n\nWith a p-value greater than .05, there is not a statistically significant relationship between rainfall and o3. Rainfall does not have a meaningful effect on the amount of o3 in the air.\n\n\nLinear Modeling of Rainfall with SO2\n\n\n\nCode\n#rainfall affect on so2 - significant\nlm_rainfall_so2 &lt;- lm(so2 ~ rainfall, data = data_wider)\nsummary(lm_rainfall_so2)\n\n\n\nCall:\nlm(formula = so2 ~ rainfall, data = data_wider)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0004482 -0.0002500 -0.0000543  0.0001934  0.0024526 \n\nCoefficients:\n               Estimate  Std. Error t value\n(Intercept)  0.00044816  0.00001308   34.27\nrainfall    -0.00000764  0.00000257   -2.97\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nrainfall                  0.003 ** \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00032 on 719 degrees of freedom\nMultiple R-squared:  0.0122,    Adjusted R-squared:  0.0108 \nF-statistic: 8.84 on 1 and 719 DF,  p-value: 0.00304\n\n\n\nWith a p-value lower than .01, there is a statistically significant relationship between rainfall and so2. When rainfall is zero, the predicted so2 levels are 0.000448 ppm. For every additional mm of rainfall, so2 levels decrease by 0.00000764 ppm.\nThis means that higher rainfall is associated with lower levels of so2. The R² value is .0122, suggesting that only 12% of the variability of so2 is explained by rainfall. This means that there are other factors that contribute to the levels of so2 in the air besides rainfall.\n\n\nLinear Modeling of Rainfall with NO2\n\n\n\nCode\n#rainfall affect on no2 - significant\nlm_rainfall_no2 &lt;- lm(no2 ~ rainfall, data = data_wider)\n\nsummary(lm_rainfall_no2)\n\n\n\nCall:\nlm(formula = no2 ~ rainfall, data = data_wider)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.00793 -0.00373 -0.00113  0.00281  0.01757 \n\nCoefficients:\n              Estimate Std. Error t value\n(Intercept)  0.0094339  0.0001927   48.95\nrainfall    -0.0001084  0.0000377   -2.87\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nrainfall                 0.0042 ** \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0047 on 708 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.0115,    Adjusted R-squared:  0.0101 \nF-statistic: 8.25 on 1 and 708 DF,  p-value: 0.00419\n\n\n\nWith a p-value lower than .01, there is a statistically significant relationship between rainfall and no2. When rainfall is zero, the predicted no2 levels are 0.00943 ppm. For every additional mm of rainfall, no2 levels decreases by 0.000108 ppm.\nThis means that higher rainfall is associated with lower levels of no2. The R² value is .0115, suggesting that only 1% of the variability in no2 is explained by rainfall. This means that there are many other factors that contribute to the levels of so2 in the air besides rainfall.\n\n\nLinear Modeling of Rainfall with PM25\n\n\n\nCode\n#rainfall affect on pm25\nlm_rainfall_pm25 &lt;- lm(pm25 ~ rainfall, data = data_wider)\n\nsummary(lm_rainfall_pm25)\n\n\n\nCall:\nlm(formula = pm25 ~ rainfall, data = data_wider)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.50  -2.38  -0.86   1.34  32.44 \n\nCoefficients:\n            Estimate Std. Error t value\n(Intercept)   6.5566     0.1518   43.19\nrainfall     -0.1319     0.0317   -4.16\n                        Pr(&gt;|t|)    \n(Intercept) &lt; 0.0000000000000002 ***\nrainfall                0.000036 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.7 on 696 degrees of freedom\n  (23 observations deleted due to missingness)\nMultiple R-squared:  0.0243,    Adjusted R-squared:  0.0229 \nF-statistic: 17.3 on 1 and 696 DF,  p-value: 0.0000358\n\n\n\nWith a p-value lower than .01, there is a statistically significant relationship between rainfall and PM2.5. When rainfall is zero, the predicted PM2.5 levels are 6.5566 µg/m³. For every additional mm of rainfall, PM2.5 levels decrease by 0.1319 µg/m³.\nThis means that higher rainfall is associated with lower levels of PM2.5. The R² value is 0.0243, suggesting that only 2.43% of the variability of PM2.5 is explained by rainfall. This means that there are other factors that contribute to the levels of PM2.5 in the air besides rainfall.\n\n\nLinear Modeling of Rainfall with PM10\n\n\n\nCode\n#rainfall affect on pm10\nlm_rainfall_pm10 &lt;- lm(pm10 ~ rainfall, data = data_wider)\n\nsummary(lm_rainfall_pm10)\n\n\n\nCall:\nlm(formula = pm10 ~ rainfall, data = data_wider)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12.64  -5.26  -0.87   3.32  49.13 \n\nCoefficients:\n            Estimate Std. Error t value\n(Intercept)   16.872      0.294   57.39\nrainfall      -0.326      0.057   -5.71\n                        Pr(&gt;|t|)    \n(Intercept) &lt; 0.0000000000000002 ***\nrainfall             0.000000017 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.2 on 700 degrees of freedom\n  (19 observations deleted due to missingness)\nMultiple R-squared:  0.0445,    Adjusted R-squared:  0.0431 \nF-statistic: 32.6 on 1 and 700 DF,  p-value: 0.0000000169\n\n\n\nWith a p-value lower than .01, there is a statistically significant relationship between rainfall and PM10. When rainfall is zero, the predicted PM10 levels are 16.872 µg/m³. For every additional mm of rainfall, PM10 levels decrease by 0.326 µg/m³.\nThis means that higher rainfall is associated with lower levels of PM10. The R² value is 0.0445, suggesting that only 4.45% of the variability of PM10 is explained by rainfall. This means that there are other factors that contribute to the levels of PM10 in the air besides rainfall.\n\nOf the five air pollutants, linear modelling provides the most amount of evidence that rainfall washes out so2. This is because there is a statistically significant relationship between rainfall and so2 and rainfall explains a higher proportion of the variability of so2 compared to no2, pm25, and pm10. There is also a statistically significant relationship with rainfall and no2, pm25, and pm10, but the low R² values show that 1-4% of the variability is explained by rainfall. So, it is less clear if rainfall actually contributes to lower no2, pm25, and pm10 levels or if it is simply just a correlation.\n\n\nPermutation and Lineup Test\nLine up for Rainfall and O3\n\n\nCode\nset.seed(10)\n# Rainfall and O3\nggplot(lineup(null_permute('rainfall'), data_wider), \n       aes(x = rainfall, y = o3)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  #labs(title = \"Rainfall vs O3\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc o2\")\n\n\nLineup for Rainfall and SO2\n\n\nCode\nset.seed(10)\n# Rainfall and SO2\nggplot(lineup(null_permute('rainfall'), data_wider), \n       aes(x = rainfall, y = so2)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  #labs(title = \"Rainfall vs SO2\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc VV\")\n\n\nLineup for Rainfall and NO2\n\n\nCode\nset.seed(10)\n# Rainfall and NO2\nggplot(lineup(null_permute('rainfall'), data_wider), \n       aes(x = rainfall, y = no2)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  #labs(title = \"Rainfall vs NO2\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc o0\")\n\n\nLineup for Rainfall and PM2.5\n\n\nCode\nset.seed(10)\n# Rainfall and PM25\nggplot(lineup(null_permute('rainfall'), data_wider), \n       aes(x = rainfall, y = pm25)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  #labs(title = \"Rainfall vs PM2.5\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc 0N\")\n\n\nLineup for Rainfall and PM10\n\n\nCode\nset.seed(10)\n# Rainfall and PM10\nggplot(lineup(null_permute('rainfall'), data_wider), \n       aes(x = rainfall, y = pm10)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  #labs(title = \"Rainfall vs PM10\") +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc o0\")\n\n\n\nFinally, visual inference can be used to test if there is a relationship between rainfall and air pollution levels. This is done by permuting or shuffling the rainfall values and then plotting them with the o3, so2, no2, pm25, and pm10 levels 19 times.\nIf the original scatter plot is identifiable next to the 19 null plots, then there is evidence that there is a relationship between the rainfall and the air pollutants.\nWhen doing this test with our research team and other peers, it was difficult to differentiate the plots and correctly identify the real plot.\nSo, although the linear modeling suggests that that there are statistically significant relationships between rainfall and no2, so2, pm10, and pm25, these relationships are not as evident in the permutation and nullplot tests.\n\n\n\nConclusion for Relationship 2\nThere is some evidence that rainfall might wash out so2, no2, pm10, and pm25, but not o3. When average rainfall and air pollution levels are plotted over time, we can see a general trend that as rainfall increases that so2, no2, pm25, and pm10 decrease while o3 increases. This is further supported when rainfall and air pollutants are plotted in a scatter plot. Linear modeling shows a statistically significant negative relationship between rainfall and so2, no2, pm25, and pm10. So2 has 12% of its variability explained by rainfall while no2, pm25, and pm10 have 1-4% of their variability explained by rainfall. Finally, it was difficult to correctly identify the correct plot when doing visual inference through the use of permutation and nullplots. This suggests that although there is some evidence of relationships in the linear modeling, it may not be that strong. It is clear that there are other factors that affect the levels of air pollutants in the air besides rainfall."
  },
  {
    "objectID": "posts/assign04-bettong/index.html#relationship-3",
    "href": "posts/assign04-bettong/index.html#relationship-3",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "Relationship 3",
    "text": "Relationship 3\nHigh-pressure systems often lead to stagnant air, trapping pollutants\nThis research question examines whether high-pressure systems, which create stagnant atmospheric conditions, contribute to the buildup of pollutants near the ground. This analysis focuses on various pollutants, including particulate matter (PM2.5 and PM10), ozone (O3), sulfur dioxide (SO2), nitrogen dioxide (NO2), and carbon monoxide (CO), assessing their relationship with mean sea level pressure (MSLP). The following section explores how high-pressure conditions may influence pollutant concentrations, providing insights into the extent to which these atmospheric conditions impact air quality.\nEvaluating Monthly Mean Sea Level Pressure (MSLP) Variation\n\n\nCode\ndata_wider$month &lt;- as.factor(data_wider$month)\n\nggplot(data_wider, aes(x=month, y=mslp)) +\n  geom_boxplot(fill= \"#56B4E9\") +\n  labs(x = \"Month\", y = \"Mean Sea Level Pressure\") +\n  theme(aspect.ratio=0.5)\n\n\n\n\n\n\n\n\n\n\nThe box plot reveals seasonal patterns in mean sea level pressure (MSLP) across the year. July has the highest median MSLP, indicating strong high-pressure conditions in mid-winter that could lead to stagnant air and the trapping of pollutants.\nMay and August show greater variability, with wider ranges and outliers, reflecting fluctuations during transitional months. This variability may result in alternating conditions of pollutant trapping and dispersion.\nIn contrast, summer months, especially January, have lower and more stable MSLP values, suggesting consistent low-pressure conditions that likely facilitate better air circulation and pollutant dispersion.\nOverall, this seasonal pattern implies that winter’s higher and more stable MSLP may contribute to pollutant accumulation, while summer’s lower MSLP aids in clearing the air.\n\nEvaluating Average Variation of Pollutants compared to MSLP\n\n\nCode\noptions(scipen=99)\n\nmslp_avg_lines_table &lt;- data_wider %&gt;%\n  group_by(month) %&gt;%\n  summarise(avg_mslp = mean(mslp, na.rm = TRUE),\n            avg_so2 = mean(so2, na.rm = TRUE),\n            avg_o3 = mean(o3, na.rm = TRUE),\n            avg_no2 = mean(no2, na.rm = TRUE),\n            avg_pm25 = mean(pm25, na.rm = TRUE),\n            avg_pm10 = mean(pm10, na.rm = TRUE),\n            avg_co = mean(co, na.rm = TRUE))\n\nmslp_avg_lines_table &lt;- mslp_avg_lines_table %&gt;%\n  mutate(month = as.numeric(month))\n\n# Create individual plots for each pollutant\no3 &lt;- ggplot(mslp_avg_lines_table, aes(x = month, y = avg_o3)) +\n  geom_line(color = \"red\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg O3 Levels\", x = \"Month\", y = \"Avg o3 (ppm)\") +\n  theme_minimal()\n\nso2 &lt;- ggplot(mslp_avg_lines_table, aes(x = month, y = avg_so2)) +\n  geom_line(color = \"blue\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg SO2 Levels\", x = \"Month\", y = \"Avg so2 (ppm)\") +\n  theme_minimal()\n\nno2 &lt;- ggplot(mslp_avg_lines_table, aes(x = month, y = avg_no2)) +\n  geom_line(color = \"green\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg NO2 Levels\", x = \"Month\", y = \"Avg NO2 (ppm)\") +\n  theme_minimal()\n\npm25 &lt;- ggplot(mslp_avg_lines_table, aes(x = month, y = avg_pm25)) +\n  geom_line(color = \"purple\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg PM2.5 Levels\", x = \"Month\", y = \"Avg PM2.5 (µg/m³)\") +\n  theme_minimal()\n\npm10  &lt;- ggplot(mslp_avg_lines_table, aes(x = month, y = avg_pm10)) +\n  geom_line(color = \"pink\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg PM10 Levels\", x = \"Month\", y = \"Avg pm10 (µg/m³)\") +\n  theme_minimal()\n\nco  &lt;- ggplot(mslp_avg_lines_table, aes(x = month, y = avg_co)) +\n  geom_line(color = \"orange\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg CO Levels\", x = \"Month\", y = \"Avg CO (ppm)\") +\n  theme_minimal()\n\nmslp &lt;-ggplot(mslp_avg_lines_table, aes(x = month, y = avg_mslp)) +\n  geom_line(color = \"lightblue\", size = 1) +\n   scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg MSLP\", x = \"Month\", y = \"MSLP\") +\n       theme_minimal()\n\n\n\ngrid.arrange(o3, so2, no2, pm25, pm10, co, mslp, ncol = 3)\n\n\n\n\n\n\n\n\n\n\nThese line graphs illustrate the average monthly levels of various air pollutants (O3, SO2, NO2, PM2.5, PM10, and CO) alongside mean sea level pressure (MSLP). MSLP follows a seasonal pattern, peaking in winter (May to August) and dipping in summer (December to February). If high MSLP is associated with air stagnation, we might expect pollutant levels to increase during winter and decrease in summer.\nIn these charts, NO2 and CO closely follow MSLP trends, with higher levels in winter and lower levels in summer. O3, however, shows an inverse trend, peaking in summer and declining in winter, likely due to the influence of sunlight in ozone formation.\nPM2.5 exhibits fluctuating peaks throughout autumn and winter, indicating seasonal variability that may be influenced by both MSLP and other factors like seasonal emissions or heating.\nIn contrast, PM10 does not display a consistent winter increase, with lower levels during high-pressure months, suggesting a slight inverse relationship with MSLP, especially in summer.\nSO2 levels generally decline through the year, with some variability. Further analysis, including scatter plots and regression models, could help clarify the effects of seasonal pressure changes on these pollutants.\n\nExamining Relationships Using Scatterplots\n\n\nCode\n# Scatter plot for MSLP vs o3\nscatter_mslp_o3 &lt;- ggplot(data_wider, aes(y = o3, x = mslp)) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  labs(title = \"MSLP vs O3\",\n       y = \"Ozone (ppm)\",\n       x = \"MSLP\") +\n  theme_minimal()\n\n# Scatter plot for MSLP  vs. so2\nscatter_mslp_so2 &lt;- ggplot(data_wider, aes(y = so2, x = mslp)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkblue\") +\n  labs(title = \"MSLP vs SO2\",\n       y = \"SO2 (ppm)\",\n       x = \"MSLP\") +\n  theme_minimal()\n\n# Scatter plot for MSLP vs no2\nscatter_mslp_no2 &lt;- ggplot(data_wider, aes(y = no2, x = mslp)) +\n  geom_point(alpha = 0.5, color = \"green\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  labs(title = \"MSLP vs NO2\",\n       y = \"NO2 (ppm)\",\n       x = \"MSLP\") +\n  theme_minimal()\n\n# Scatter plot for MSLP vs pm25\nscatter_mslp_pm25 &lt;- ggplot(data_wider, aes(y = pm25, x = mslp)) +\n  geom_point(alpha = 0.5, color = \"purple\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#301934\") +\n  labs(title = \"MSLP vs PM2.5\",\n       y = \"PM2.5 (µg/m³)\",\n       x = \"MSLP\") +\n  theme_minimal()\n\n# Scatter plot for MSLP vs pm10\nscatter_mslp_pm10 &lt;- ggplot(data_wider, aes(y = pm10, x = mslp)) +\n  geom_point(alpha = 0.5, color = \"pink\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#e75480\") +\n  labs(title = \"MSLP vs PM10\",\n       y = \"PM10 (µg/m³)\",\n       x = \"MSLP\") +\n  theme_minimal()\n\n# Scatter plot for MSLP vs CO\nscatter_mslp_co &lt;- ggplot(data_wider, aes(y = co, x = mslp)) +\n  geom_point(alpha = 0.5, color = \"orange\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  labs(title = \"MSLP vs CO\",\n       y = \"CO (ppm)\",\n       x = \"MSLP\") +\n  theme_minimal()\n\n\ngrid.arrange(scatter_mslp_o3, scatter_mslp_so2, scatter_mslp_no2, \n             scatter_mslp_pm25, scatter_mslp_pm10, scatter_mslp_co, ncol = 3)\n\n\n\n\n\n\n\n\n\n\nThe scatter plots display the relationships between mean sea level pressure (MSLP) and various air pollutants, offering insights into how high-pressure systems may influence air quality by either trapping pollutants or allowing for dispersion.\nO3 shows a moderately negative relationship with MSLP, with lower ozone levels during high-pressure conditions, likely due to reduced sunlight and limited dispersion under stagnant air. In contrast NO2 and CO both have a positive relationship with MSLP, indicating that higher MSLP can trap these pollutants near ground level, preventing their dispersion and leading to accumulation under stable, high-pressure conditions.\nBoth PM2.5 and PM10 exhibit a weaker but still positive relationship with MSLP, suggesting that high-pressure conditions may contribute to their buildup, though the relationship is weaker than for NO2 and CO. SO2 has minimal correlation with MSLP, indicating its levels are more influenced by localized sources rather than atmospheric pressure patterns.\nOverall, these findings support the that high MSLP conditions, often associated with stagnant air, can lead to the accumulation of pollutants like NO2, CO, PM2.5, and PM10.\n\n\nLinear Models\nLinear Modeling of Mean Sea Level Pressure with O3\n\n\nCode\n#mslp affect on o3 \nlm_mslp_o3 &lt;- lm(o3 ~ mslp, data = data_wider)\nsummary(lm_mslp_o3)\n\n\n\nCall:\nlm(formula = o3 ~ mslp, data = data_wider)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.014428 -0.003827 -0.000261  0.003246  0.024623 \n\nCoefficients:\n              Estimate Std. Error t value\n(Intercept)  0.2985004  0.0265844    11.2\nmslp        -0.0002790  0.0000261   -10.7\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nmslp        &lt;0.0000000000000002 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0057 on 719 degrees of freedom\nMultiple R-squared:  0.137, Adjusted R-squared:  0.136 \nF-statistic:  114 on 1 and 719 DF,  p-value: &lt;0.0000000000000002\n\n\n\nWith a p-value well below 0.01, there is a statistically significant inverse relationship between mean sea level pressure (MSLP) and ozone levels. For each unit increase in MSLP, ozone levels decrease by 0.000279 ppm, suggesting that higher pressure is associated with lower ozone levels.\nThis supports the idea that high-pressure systems, which reduce air dispersal, tend to trap pollutants but do not directly promote ozone formation, as it is more influenced by sunlight and temperature.\nThe R-squared value of 0.137 indicates that MSLP accounts for about 13.7% of the variation in ozone levels, with other factors also contributing.\n\nLinear Modeling of Mean Sea Level Pressure with SO2\n\n\nCode\n#mslp affect on SO2 \nlm_mslp_so2 &lt;- lm(so2 ~ mslp, data = data_wider)\nsummary(lm_mslp_so2)\n\n\n\nCall:\nlm(formula = so2 ~ mslp, data = data_wider)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0004920 -0.0002474 -0.0000573  0.0001889  0.0024102 \n\nCoefficients:\n               Estimate  Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.00435962  0.00152083   -2.87   0.0043 **\nmslp         0.00000471  0.00000149    3.15   0.0017 **\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00032 on 719 degrees of freedom\nMultiple R-squared:  0.0136,    Adjusted R-squared:  0.0123 \nF-statistic: 9.93 on 1 and 719 DF,  p-value: 0.00169\n\n\n\nWith a p-value below 0.01, there is a statistically significant positive relationship between mean sea level pressure (MSLP) and SO2 levels. For each unit increase in MSLP, SO2 levels increase by 0.00000471 ppm, suggesting that higher pressure is associated with slightly elevated SO2 levels.\nThis supports the idea that high-pressure systems may trap pollutants, including SO2, due to reduced air dispersal. However, the R-squared value is only 0.0136, indicating that MSLP explains just 1.36% of the variability in SO2 levels, so other factors also play a significant role in influencing SO2 concentrations.\n\nLinear Modeling of Mean Sea Level Pressure with NO2\n\n\nCode\n#mslp affect on NO2 \nlm_mslp_no2 &lt;- lm(no2 ~ mslp, data = data_wider)\nsummary(lm_mslp_no2)\n\n\n\nCall:\nlm(formula = no2 ~ mslp, data = data_wider)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.008455 -0.003400 -0.000823  0.002809  0.015231 \n\nCoefficients:\n              Estimate Std. Error t value\n(Intercept) -0.2282715  0.0206867   -11.0\nmslp         0.0002334  0.0000203    11.5\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nmslp        &lt;0.0000000000000002 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0044 on 708 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.157, Adjusted R-squared:  0.156 \nF-statistic:  132 on 1 and 708 DF,  p-value: &lt;0.0000000000000002\n\n\n\nWith a p-value much lower than 0.01, there is a statistically significant positive relationship between mean sea level pressure (MSLP) and NO2 levels. For each unit increase in MSLP, NO2 levels increase by 0.0002334 ppm, indicating that higher pressure is associated with slightly elevated NO2 levels.\nThis finding aligns with the notion that high-pressure systems can trap pollutants due to reduced air dispersal, potentially increasing NO2 concentrations.\nThe R-squared value of 0.157 indicates that MSLP explains about 15.7% of the variability in NO2 levels, suggesting that while MSLP has a noticeable influence, other factors also contribute significantly to NO2 concentrations.\n\nLinear Modeling of Mean Sea Level Pressure with PM2.5\n\n\nCode\n#mslp affect on pm25 \nlm_mslp_pm25 &lt;- lm(pm25 ~ mslp, data = data_wider)\nsummary(lm_mslp_pm25)\n\n\n\nCall:\nlm(formula = pm25 ~ mslp, data = data_wider)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6.15  -2.20  -0.74   1.40  33.13 \n\nCoefficients:\n             Estimate Std. Error t value\n(Intercept) -150.7612    16.5793   -9.09\nmslp           0.1544     0.0163    9.47\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nmslp        &lt;0.0000000000000002 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.5 on 696 degrees of freedom\n  (23 observations deleted due to missingness)\nMultiple R-squared:  0.114, Adjusted R-squared:  0.113 \nF-statistic: 89.8 on 1 and 696 DF,  p-value: &lt;0.0000000000000002\n\n\n\nWith a p-value much lower than 0.01, there is a statistically significant positive relationship between mean sea level pressure (MSLP) and PM2.5 levels. For each unit increase in MSLP, PM2.5 levels increase by 0.1544 µg/m³, suggesting that higher pressure is associated with higher PM2.5 concentrations.\nThis aligns with the idea that high-pressure systems can trap particulate matter due to reduced air movement, leading to an accumulation of pollutants like PM2.5.\nThe R-squared value of 0.114 indicates that about 11.4% of the variability in PM2.5 levels is explained by MSLP, implying that while MSLP has an influence, other factors —such as traffic emissions, industrial activities, and seasonal weather variations—also play important roles in shaping PM2.5 concentrations.\n\nLinear Modeling of Mean Sea Level Pressure with PM10\n\n\nCode\n#mslp affect on pm10\nlm_mslp_pm10 &lt;- lm(pm10 ~ mslp, data = data_wider)\nsummary(lm_mslp_pm10)\n\n\n\nCall:\nlm(formula = pm10 ~ mslp, data = data_wider)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15.77  -5.11  -1.07   3.39  50.33 \n\nCoefficients:\n             Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) -197.0526    33.8464   -5.82 0.00000000886 ***\nmslp           0.2097     0.0333    6.30 0.00000000052 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.2 on 700 degrees of freedom\n  (19 observations deleted due to missingness)\nMultiple R-squared:  0.0537,    Adjusted R-squared:  0.0523 \nF-statistic: 39.7 on 1 and 700 DF,  p-value: 0.00000000052\n\n\n\nWith a p-value significantly below 0.01, this model establishes a statistically significant positive relationship between mean sea level pressure (MSLP) and PM10 levels. For each unit increase in MSLP, PM10 levels rise by 0.2097 µg/m³, supporting the concept that high-pressure systems can trap pollutants like PM10 by limiting air dispersal.\nHowever, with an R-squared value of 0.0537, MSLP explains only about 5.4% of the variability in PM10, indicating that other factors—such as local emissions from traffic and industry, seasonal weather patterns, and temperature—likely contribute significantly to PM10 concentrations.\n\nLinear Modeling of Mean Sea Level Pressure with CO\n\n\nCode\n#mslp affect on CO\nlm_mslp_co &lt;- lm(co ~ mslp, data = data_wider)\nsummary(lm_mslp_co)\n\n\n\nCall:\nlm(formula = co ~ mslp, data = data_wider)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.2094 -0.0649 -0.0153  0.0403  0.5774 \n\nCoefficients:\n             Estimate Std. Error t value\n(Intercept) -5.082186   0.457175   -11.1\nmslp         0.005178   0.000449    11.5\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nmslp        &lt;0.0000000000000002 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.097 on 707 degrees of freedom\n  (12 observations deleted due to missingness)\nMultiple R-squared:  0.158, Adjusted R-squared:  0.157 \nF-statistic:  133 on 1 and 707 DF,  p-value: &lt;0.0000000000000002\n\n\n\nThe linear model reveals a statistically significant positive relationship between mean sea level pressure (MSLP) and CO levels, as indicated by a very low p-value (&lt; 0.01). For every unit increase in MSLP, CO levels rise by 0.005178 ppm, suggesting that higher pressures are associated with elevated CO concentrations. This supports that high-pressure systems may trap pollutants like CO, limiting their dispersion.\nHowever, with an R-squared value of 0.158, MSLP explains only 15.8% of the variability. Other factors likely contribute to CO levels, primarily vehicle emissions and industrial activities, which are significant sources of carbon monoxide in the atmosphere.\n\nOverall, MSLP has the strongest effect on CO and NO2 levels, explaining about 15-16% of their variability and suggesting that high-pressure systems can trap these pollutants. While MSLP also significantly influences PM2.5, PM10, and SO2, the lower R-squared values (5-11%) imply that other factors more heavily impact these pollutants.\n\n\nPermutation and Lineup Test\nLine up for Mean Sea Level Pressure and O3\n\n\nCode\nset.seed(362362)\nggplot(lineup(null_permute('mslp'), data_wider), \n  aes(x=mslp, y=o3)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nLine up for Mean Sea Level Pressure and SO2\n\n\nCode\nset.seed(123456)\nggplot(lineup(null_permute('mslp'), data_wider), \n  aes(x=mslp, y=so2)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nLine up for Mean Sea Level Pressure and NO2\n\n\nCode\nset.seed(272923)\nggplot(lineup(null_permute('mslp'), data_wider), \n  aes(x=mslp, y=no2)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nLine up for Mean Sea Level Pressure and PM25\n\n\nCode\nset.seed(754638)\nggplot(lineup(null_permute('mslp'), data_wider), \n  aes(x=mslp, y=pm25)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nLine up for Mean Sea Level Pressure and PM10\n\n\nCode\nset.seed(659812)\nggplot(lineup(null_permute('mslp'), data_wider), \n  aes(x=mslp, y=pm10)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nLine up for Mean Sea Level Pressure and CO\n\n\nCode\nset.seed(129132)\nggplot(lineup(null_permute('mslp'), data_wider), \n  aes(x=mslp, y=co)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\n\nVisual inference through permutation-based lineup tests was useful in assessing the relationship between Mean Sea Level Pressure (MSLP) and various pollutants. By randomly shuffling MSLP values and plotting them against O3, SO2, NO2, PM2.5, PM10, and CO levels 19 times, we aimed to see if the original relationship stood out among the randomized plots.\nWhen conducting this test with peers and our research team, participants consistently identified the real plot for O3, NO2, and CO, indicating that the relationship between MSLP and these pollutants was visually distinct.\nIn contrast, SO2 and PM10 proved challenging, with most participants unable to confidently identify the original plot. For PM2.5, responses were mixed, with some participants able to discern the correct plot while others were not.\nThese findings largely align with the linear models, where pollutants with a relatively higher R-squared value (such as O3, NO2 and CO) had more distinct plots in the lineup tests, supporting the idea that MSLP influences these pollutants more strongly. This suggests that while MSLP affects multiple pollutants, it has the most substantial impact on NO2 and CO in terms of trapping pollutants under high-pressure conditions.\n\n\n\nConclusion for Relationship 3\nThe analysis supports that high-pressure systems, indicated by mean sea level pressure (MSLP), contribute to the accumulation of pollutants like NO2, CO, and, to a lesser extent, PM2.5 and PM10, by limiting air dispersal. Conversely, ozone (O3) shows an inverse relationship with MSLP, likely due to reduced sunlight and mixing under stagnant conditions. Linear modeling reinforces these findings, with NO2 and CO showing the strongest correlations, as indicated by higher R-squared values, suggesting a notable impact of MSLP on their concentrations. Visual inference tests further validated these relationships, as participants consistently identified the real plots for NO2 and CO, aligning with their clear response to MSLP. In conclusion, the findings affirm that high-pressure systems contribute to air stagnation, trapping pollutants and particularly increasing concentrations of NO₂ and CO compared to other pollutants."
  },
  {
    "objectID": "posts/assign04-bettong/index.html#relationship-4",
    "href": "posts/assign04-bettong/index.html#relationship-4",
    "title": "The Relationship Between Weather and Atmospheric Pollutants",
    "section": "Relationship 4",
    "text": "Relationship 4\nSunlight drives photochemical reactions that form secondary pollutants like ozone\nThe last research question is if sunlight drives photochemical reactions that form secondary pollutants like ozone. This analysis will focus on the secondary pollutants of o3 and no2. SILO provides daily solar radiation daily values that are measured in MJ/m2. The following section will explore the relationship between sunlight with o3 and no2.\nEvaluating Monthly Radiation Variation\n\n\nCode\ndata_wider$month &lt;- as.factor(data_wider$month)\n\n# Violin plot for each month\nggplot(data_wider, aes(x=month, y=radiation)) +\n  geom_violin(draw_quantiles=c(0.25, 0.5, 0.75), fill= \"#56B4E9\") +\n  labs(x = \"Month\", y = \"Solar Radiation (MJ/m2)\") +\n  theme(aspect.ratio=0.5)\n\n\n\n\n\n\n\n\n\n\nThe violin plot visualises how sunlight is highest during the summer months and lowest during the winter months in Melbourne. So, if it is true that sunlight drives photochemical reactions that produce secondary pollutants, we would expect to see higher o3 and no2 values during the summer months.\n\nEvaluating Average Variation of Pollutants compared to Radiation\n\n\nCode\n# Summarize the data by month to get the average solar radiation, o3 and no2\nradiation_avg_lines_table &lt;- data_wider %&gt;%\n  group_by(month) %&gt;%\n  summarise(avg_radiation = mean(radiation, na.rm = TRUE),\n            avg_o3 = mean(o3, na.rm = TRUE),\n            avg_no2 = mean(no2, na.rm = TRUE))\n\nradiation_avg_lines_table &lt;- radiation_avg_lines_table %&gt;%\n  mutate(month = as.numeric(month))\n\n# Create individual plots for each pollutant\no3_plot_radiation &lt;- ggplot(radiation_avg_lines_table, aes(x = month, y = avg_o3)) +\n  geom_line(color = \"red\", size = 1) +\n  labs(title = \"Avg O3 Levels\", x = \"Month\", y = \"Avg O3 (ppm)\") +\n  scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  theme_minimal()\n\n\n\nno2_plot_radiation &lt;- ggplot(radiation_avg_lines_table, aes(x = month, y = avg_no2)) +\n  geom_line(color = \"green\", size = 1) +\n  scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n  labs(title = \"Avg NO2 Levels\", x = \"Month\", y = \"Avg NO2 (ppm)\") +\n  theme_minimal()\n\nradiation_plot &lt;-ggplot(radiation_avg_lines_table, aes(x = month, y = avg_radiation)) +\n  geom_line(color = \"lightblue\", size = 1) +\n  labs(title = \"Avg Radiation\", x = \"Month\", y = \"Radiation (mm)\") +\n  scale_x_continuous(breaks = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \n                                               \"J\", \"A\", \"S\", \"O\", \"N\", \"D\")) +\n       theme_minimal()\n\n# Arrange the plots horizontally using patchwork\ncombined_plot_radiation &lt;- o3_plot_radiation + no2_plot_radiation + radiation_plot\n\n# Display the combined plot\ncombined_plot_radiation\n\n\n\n\n\n\n\n\n\n\nThe line graphs show the average o3, no2, and solar radiation levels across the months of 2022. Consistent with the violin plot, the radiation line graph shows how sunlight is stronger in the summer months and dimmer in the winter months.\nThe o3 line graph shows that ozone levels behave similarly, having higher o3 levels in the summer and weaker levels in the winter.\nConversely, no2 has weaker levels in the summer and higher levels in the winter. So, it is possible that sunlight produces more o3.\nMore testing with linear modelling and visual inferencing can be done get a more informed answer.\n\nExamining Relationships Using Scatterplots\n\n\nCode\n# Scatter plot for radiation vs o3\nscatter_radiation_o3 &lt;- ggplot(data_wider, aes(y = o3, x = radiation)) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  labs(title = \"Radiation (mm) vs O3\",\n       y = \"Ozone (ppm)\",\n       x = \"Radiation\") +\n  theme_minimal()\n\n# Scatter plot for radiation vs no2\nscatter_radiation_no2 &lt;- ggplot(data_wider, aes(y = no2, x = radiation)) +\n  geom_point(alpha = 0.5, color = \"green\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  labs(title = \"Radiation vs NO2\",\n       y = \"Nitrogen Dioxide (ppm)\",\n       x = \"Radiation\") +\n  theme_minimal()\n\n# Arrange the two plots side by side for easier comparison\ngrid.arrange(scatter_radiation_o3, scatter_radiation_no2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe scatter plots further reveal the relationship that sunlight has with o3 and no2. As solar radiation increases, o3 increases and no2 decreases. It is possible that sunlight causes secondary pollutants like ozone to be produced while it is also causes pollutants like no2 to be broken down.\n\nLinear Models\n\nLinear Modeling of Sunlight with O3\n\n\n\nCode\n#radiation affect on o3\nlm_radiation_o3 &lt;- lm(o3 ~ radiation, data = data_wider)\n\nsummary(lm_radiation_o3)\n\n\n\nCall:\nlm(formula = o3 ~ radiation, data = data_wider)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.012969 -0.004157 -0.000774  0.003937  0.019579 \n\nCoefficients:\n             Estimate Std. Error t value\n(Intercept) 0.0106526  0.0004495    23.7\nradiation   0.0002766  0.0000272    10.2\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nradiation   &lt;0.0000000000000002 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0057 on 719 degrees of freedom\nMultiple R-squared:  0.125, Adjusted R-squared:  0.124 \nF-statistic:  103 on 1 and 719 DF,  p-value: &lt;0.0000000000000002\n\n\n\nWith a p-value lower than .01, there is a statistically significant relationship between sunlight and o3. When sunlight is zero, the predicted o3 levels are 0.0107 ppm. For every additional unit increase in sunlight, o3 levels increase by 0.000277. This means that more sunlight is associated with more o3.\nThe R² value is .0125, suggesting that 12.5% of the variability of o3 is explained by sunlight. This means that there are other factors that contribute to the levels of o3 in the air besides sunlight.\n\n\nLinear Modeling of Sunlight with NO2\n\n\n\nCode\n#radiation affect on no2\nlm_radiation_no2 &lt;- lm(no2 ~ radiation, data = data_wider)\n\nsummary(lm_radiation_no2)\n\n\n\nCall:\nlm(formula = no2 ~ radiation, data = data_wider)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.00936 -0.00341 -0.00059  0.00299  0.01583 \n\nCoefficients:\n              Estimate Std. Error t value\n(Intercept)  0.0124012  0.0003533    35.1\nradiation   -0.0002207  0.0000216   -10.2\n                       Pr(&gt;|t|)    \n(Intercept) &lt;0.0000000000000002 ***\nradiation   &lt;0.0000000000000002 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0045 on 708 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.128, Adjusted R-squared:  0.127 \nF-statistic:  104 on 1 and 708 DF,  p-value: &lt;0.0000000000000002\n\n\n\nWith a p-value lower than .01, there is a statistically significant relationship between sunlight and no2. When sunlight is zero, the predicted no2 levels are 0.0124ppm. For every additional unit of sunlight, no2 levels decrease by 0.000221ppm. This means that higher sunlight is associated with lower levels of no2.\nThe R² value is .0128, suggesting that 12.8% of the variability in no2 is explained by sunlight. This means that there are many other factors that contribute to the levels of so2 in the air besides sunlight.\n\nLinear modeling gives us more evidence that there is a relationship between sunlight and o3 and no2. More sunlight is associated with more o3 and less no2. Although it is not possible to conclude that sunlight is the primary cause for more production of o3 and the breakdown of no2, there is enough evidence to suggest that it is certainly a contributing factor.\n\n\nPermutation and Lineup Test\nLineup for Sunlight and O3\n\n\nCode\nset.seed(10)\n\n#radiation and o3\nggplot(lineup(null_permute('radiation'), data_wider), \n  aes(x=radiation, y=o3)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc V0\")\n\n\nLineup for Sunlight and NO2\n\n\nCode\n#rainfall and no2\nggplot(lineup(null_permute('radiation'), data_wider), \n  aes(x=radiation, y=no2)) +\n  geom_point(alpha = 0.4) +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nCode\n#decrypt(\"4nZe kSDS Ec whQEDEhc VV\")\n\n\n\nFinally, visual inference can be used to test if there is a relationship between sunlight and o3 and no2. This is done by permuting or shuffling the sunlight values and then plotting them with the o3 and no2 levels 19 times.\nIf the original scatter plot is identifiable next to the 19 null plots, then there is evidence that there is a relationship between the sunlight and the air pollutants.\nWhen doing this test, both both o3 and no2 are identifiable. So, visual inference provides further evidence that there is relationship between sunlight and o3 and no2.\n\n\n\nConclusion for Relationship 4\nThere is evidence that sunlight helps produce more o3 but does not produce more no2. In fact, there is evidence that sunlight helps break down no2. When average sunlight and air pollution levels are plotted over time, we can see a general trend that as sunlight increases that o3 levels increase and and no2 levels decrease. This is further supported when sunlight and air pollutants are plotted in a scatter plot. Linear modeling shows a statistically significant positive relationship between sunlight and o3 and a negative relationship between sunlight and no2. Both o3 and no2 have 12% of their variability explained by sunlight. Finally, visual inference through the use of permutation and nullplots support that there is a relationship between sunlight and o3 and no2.\n\n\nGenerative AI analysis\nThe link to our use of ChatGPT for help on this project is:\n\nTravis - https://chatgpt.com/share/672361c0-59a4-8006-b88a-60fa76c634ba\nPooja - https://chatgpt.com/share/67243b34-4210-8005-afd3-7317fe030407"
  },
  {
    "objectID": "posts/assignment-3-praj0022/index.html",
    "href": "posts/assignment-3-praj0022/index.html",
    "title": "Diving Deeper into Data Exploration: Visualisation",
    "section": "",
    "text": "doubledecker(xtabs(n ~ Dept + Gender + Admit, data = ucba), gp = gpar(fill = c(“grey90”, “orangered”)))\n\n\\(H_0\\) : There is no association between Department, Gender and Admits. That is, the distribution of admits are not influenced by either department or gender.”\n\nTo generate null samples for the doubledecker plot, we employ permutation since the data consists of categorical variables (department, gender, and admission status). This involves randomly rearranging the admission status while leaving the department and gender categories unchanged. This breaks any original associations between admission and the other variables, resulting in new datasets with randomized relationships. By repeating this process several times, we can build a collection of null samples, each with a different random assignment of admission status, illustrating how the data behaves under random circumstances.\n\n\n\n\n\nggplot(landmine, aes(x, y)) + geom_point(alpha=0.6)\n\n\\(H_0\\) : There is no spatial association between the x and y coordinates of the landmines. The distribution of landmines across the field is random, with no significant clustering or pattern in their spatial arrangement.\n\nTo generate null samples for this plot, we use simulation due to the continuous nature of the x and y spatial coordinates. The process involves randomly reassigning new x and y coordinates to each landmine within the boundary of the field, which disrupts any existing spatial patterns or clustering and assumes randomness. By repeating this random reassignment of new coordinates multiple times, we create a series of null samples, each with a different random spatial distribution of landmines. This method allows us to assess whether the observed clustering in the original data is statistically significant or could simply be due to random variation.\n\n\n\n\n\n\nCode\nlandmine &lt;- read_csv(\"data/landmine3.csv\")\n\n\n\nAlternative plots for the data that might help to discover the landmine locations are\n\n\nScatter Plot: Displays the spatial distribution of landmines, helping to identify any potential clustering or spatial patterns.\nHeatmap: Highlights regions with high landmine concentrations by using color gradients to represent density.\nContour Plot: Displays lines representing regions of equal density to highlight areas with varying concentrations of landmines.\nOverlayed Density Plot: Integrates density estimation with spatial data to pinpoint regions where landmine density is particularly high.\nHexbin Map: Groups landmine data into hexagonal bins to provide a clearer view of density patterns and clustering.\nKernel Density Estimation (KDE) Plot: Offers a smoothed view of landmine distribution to uncover patterns in the spatial density of landmines.\n\n\n\nCode\n# 1. Scatter Plot\nggplot(landmine, aes(x = x, y = y)) +\n  geom_point(alpha=0.3) +\n  labs(title = \"Scatter Plot of Coordinates\") \n\n\n\n\n\n\n\n\n\nCode\n# 2. Heatmap\nggplot(landmine, aes(x = x, y = y)) +\n  stat_bin2d(bins = 30) +\n  scale_fill_viridis_c() +\n  labs(title = \"Heatmap of Landmine Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 3. Contour Plot\nggplot(landmine, aes(x = x, y = y)) +\n  geom_density_2d() +\n  labs(title = \"Contour Plot of Landmine Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 4. Overlayed Density Plot\nggplot(landmine, aes(x, y)) + \n  geom_point(alpha = 0.4) +  \n  geom_density2d(color = \"blue\") + \n  stat_density2d(aes(fill = ..level..), geom = \"polygon\", alpha = 0.4) +  \n  scale_fill_gradient(low = \"yellow\", high = \"red\") +  \n  theme_minimal() + \n  labs(title = \"Overlayed Density Plot of Landmine Locations\",\n       x = \"X Coordinate\",\n       y = \"Y Coordinate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 5. Hexbin Plot\nggplot(landmine, aes(x = x, y = y)) +\n  stat_bin_hex(bins = 30) +     \n  scale_fill_viridis() +   \n  labs(title = \"Hexbin Plot of Landmine Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 6. KDE Plot \nggplot(landmine, aes(x, y)) + \n  geom_density_2d_filled() +\n  labs(title = \"KDE of Landmine Locations\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nCan we see any potential locations of landmines? \n\nYes, we can observe potential locations of landmines from the plots generated above.\nLets consider the overlayed density plot: In the plot, the red area represents the region with the highest density of points, indicating that the landmines are likely concentrated in that area.\nObservations:\n\nThe highest density of points (in red) is around the center of the plot. This is the most probable region where landmines are clustered.\nThe dark yellow to orange gradient surrounding the red zone indicates areas with moderately high densities, which are also potential regions with landmine locations, but with lower certainty compared to the red area.\nThe light yellow regions and blue contour lines on the outer parts of the plot show regions of lower density , indicating a lower likelihood of landmines in these areas.\nHence, the landmines are most likely concentrated in the red, high-density region at the center of the plot. There could be other smaller clusters in some of the lighter orange areas, but they are less significant compared to the central cluster. To effectively detect landmines, the red and orange zones where the density is highest should be prioritized.\n\n\nLine up experiment\n\n\nConstructing the lineup for Kernel Density Estimation (KDE) Plot\n\n\n\nCode\nset.seed(20190709)\nggplot(lineup(null_permute('y'), landmine), \n  aes(x=x, y=y)) +\n  geom_density_2d_filled() +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# decrypt(\"o0vr 8ZGZ D3 k5fDGD53 YM\")\n\n\n\nI presented the lineup to 8 of my friends, and all 8 independently identified plot 13 as the most distinctive. Each of them noted that the bright yellow spot at the center of plot 13 made it stand out clearly from the other plots, highlighting it as the most unique in the lineup.\n\nComputing the p-value\nAs true data position is indeed plot 13\n\nx = 8 ; As 8 people chose the data plot\nn = 8 ; Total number of people the lineup was shown to is 8\nTotal number of plots in the lineup = 20\n\nWe suppose that each person has the same ability to identify the data plot. If we let X be the number of people who correctly identified the data plot in the lineup, then X ~ B(8,p). The visual inference p-value is calculated from testing the hypothesis \\(h_0\\): p = 0.05 vs \\(H_1\\): p != 0.05 , and so P(X = 8) is an extremely small value. The visual inference p-value is extremely small so there is strong evidence to reject the null hypothesis. Hence, it strongly suggests that the data plot is distinguishable from the random plots, meaning it likely contains meaningful features that set it apart from the null distribution and is not just a random occurrence.\n\n\nCode\nnullabor::pvisual(8, 8, 20)\n\n\n     x simulated   binom\n[1,] 8         0 3.9e-11\n\n\nThe p-value is \\(3.9e^{-11}\\).\n\n\n\n\n\nCode\nfuel_data &lt;- read_csv(\"data/wdi_valid.csv\")\n\n\n\n\nCode\noptions(scipen = 999)\nfuel &lt;- fuel_data |&gt;\n        filter(year == \"2022\")\n\n\nChecking the Composition of Data types\n\n\nCode\nvis_dat(fuel)\n\n\n\n\n\n\n\n\n\n\nAs we can observe from the above plot, the data consists of character and numeric datatypes.\n\n\nSummary of the distribution of each of the variable\n\nPlotting the histograms of each variable to assess the distribution.\n\n\nCode\ng1 &lt;- ggplot(fuel, aes(x = clean_fuels_all)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng2 &lt;- ggplot(fuel, aes(x = clean_fuels_rural)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng3 &lt;- ggplot(fuel, aes(x = clean_fuels_urban)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng4 &lt;- ggplot(fuel, aes(x = fuel_exports)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng5 &lt;- ggplot(fuel, aes(x = fuel_imports)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng1 + g2 + g3 + g4 + g5 + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nFrom the above plot, we can observe that:\n\nclean_fuels_all\n\n\nThe distribution is not symmetric. It is left skewed as there are more countries with high values close to 100%, fewer countries with low values.\nNo clear outliers.\nWeak multi-modality. There exists a slight peaks around 25% and a proper peak around 100%.\nHeaping around 100%, where many countries report high access to clean fuel.\nWhile uneven, the distribution is continuous with no significant gaps.\nNo implausible values. The values are within possible range.\n\n\nclean_fuels_rural\n\n\nThe distribution is somewhat uniform with a few peaks. It is more balanced with peaks on both ends. There is no notable skewness.\nThere are no outliers.\nIt is bi-modal, with peaks around 15% and at 100% indicating two distinct groups.\nThere are gaps in the 25-50% range, indicating fewer countries with medium levels of rural clean fuel access.\nNo heaping is present. No sharp accumulation of values at any specific point.\nNo implausible values. The values are within possible range.\n\n\nclean_fuel_urban\n\n\nThe distribution is not symmetric. There are more countries with high values close to 100%, fewer countries with low values. The distribution is left skewed with most countries showing high levels of urban access to clean fuels.\nThere are outliers towards 0%. While this may not be considered as an extreme outlier.\nIt is unimodal, with a significant peak at 100%.\nThere are gaps between 0 - 50% range, suggesting fewer countries have mid-range urban access to clean fuels.\nThere is considerate heaping around 100%, indicating that many countries have almost full access to clean fuels in urban areas.\nThere seems to be some discreteness and no implausible values. The values are within possible range.\n\n\nfuel_exports\n\n\nThe distribution is not symmetric. It is right skewed with the tail extending toward the higher values, as the majority of countries report low percentages of fuel exports, with fewer countries showing higher export levels.\nThere are outliers towards 100%.\nIt is unimodal, with a clean peak in the lower range around 0 - 10%.\n\nThere are gaps between the 75 - 90% range, where fewer countries report moderate to high fuel exports.\nThere is some heaping in the lower ranges around 0 - 15%, where many countries have minimal fuel exports.\nNo implausible values. The values are within possible range.\n\n\nfuel_imports\n\n\nThe distribution is mostly symmetric with data centered around the middle. It largely follows a normal distribution with a central peak and balanced tails. There is no skewness.\nThe distribution is unimodel, with a prominent peak around 20%.\nThere are no clear outliers observable in the distribution.\nNo heaping is evident, the values are distributed relatively evenly.\nThe data is spread continuously without any gaps.\nNo implausible values. The values are within possible range.\n\nCreating Boxplots to confirm outliers\n\n\nCode\ndf_long &lt;- fuel |&gt;\n  pivot_longer(\n    cols = -c(country_code, year),     \n    names_to = \"fuel_type\",    \n    values_to = \"percentage\"  \n  )\n\n\n\n\nCode\nggplot(df_long, aes(x = percentage)) +\n  geom_boxplot() +\n  facet_wrap(~ fuel_type) +  \n  labs(title = \"Boxplot of fuel variables\",\n       x = \"Percentage\",\n       y = \"Count\") + \n      theme_minimal()\n\n\n\n\n\n\n\n\n\n\nUsing the histograms, while we were able to observe outliers only for clean_fuels_urban and fuel_exports. From the boxplots we can actually detect outliers in fuel_imports as well.\nTherefore, along with clean_fuels_urban and fuel_exports , fuel_imports variable also has outliers present.\n\n\nSummary of the relationship between each of the pairs of variables\n\n\n\nCode\nfuel |&gt; select(-country_code, -year) |&gt; \n    GGally::ggpairs()\n\n\n\n\n\n\n\n\n\n\nclean_fuels_all and clean_fuels_rural\n\n\nLinear form: There is a clear linear relationship between the two variables.\nPositive trend: As clean_fuels_all increases, so does clean_fuels_rural.\nStrong: Corr:0.950, the relationship is very strong with minimal variation around the trend line.\nOutliers: No noticeable outliers.\nHeteroskedastic: No evidence of heteroskedasticity.\nNo gaps, clusters, or discreteness: The points are tightly packed along the trend line.\n\n\nclean_fuels_all and clean_fuels_urban\n\n\nLinear form: The relationship is strongly linear.\nPositive trend: Both variables increase together.\nStrong: Corr:0.950, the correlation is strong with minimal variation.\nOutliers: No significant outliers.\nHeteroskedastic: No heteroskedasticity.\nNo gaps, clusters, or discreteness: Points closely follow the trend without clustering or gaps.\n\n\nclean_fuels_all and fuel_exports\n\n\nNo trend: There is no visible relationship.\nNo form: The scatterplot is random, showing no linear or nonlinear form.\nCorr:−0.067, the correlation is extremely weak with significant variation.\nOutliers: There are a few outliers at the top, where fuel_exports is much higher than the general spread of the data.\nNo heteroskedasticity: The variation is constant throughout the plot.\nNo gaps, clusters, or discreteness: The points are scattered randomly.\n\n\nclean_fuels_all vs fuel_imports\n\n\nLinear form: A weak linear relationship exists.\nNegative trend: As clean_fuels_all increases, fuel imports tend to decrease slightly.\nWeak: Corr:−0.278, the relationship is weak, with significant variation around the trend.\nHeteroskedastic: No clear signs of heteroskedasticity exists, while there is a slightly more spread at higher levels of clean_fuels_all but the change is minimal.\nOutliers: There is one significant outlier below the trend line.\nNo gaps or clusters: No noticeable clustering.\n\n\nclean_fuels_rural and clean_fuels_urban\n\n\nNonlinear form: The relationship is slightly nonlinear, with a curve.\nPositive trend: Both variables increase together, though not at a constant rate.\nModerate: Corr:0.851, the relationship is moderately strong with noticeable variation.\nOutliers: Some outliers where clean_fuels_rural is lower than expected, despite high urban access.\nHeteroskedastic: Slight heteroskedasticity is visible, as the spread of points increases a bit at higher values.\nNo gaps or clusters: No noticeable clustering.\n\n\nclean_fuels_rural and fuel_exports\n\n\nNo trend: No discernible trend exists between the two variables.\nNo form: The points are scattered randomly, no visible pattern either linear or nonlinear.\nCorr:−0.069, the correlation is very weak with significant variation suggesting no relationship.\nOutliers: There are outliers at the top, where a few countries have very high fuel exports despite varying levels of rural clean fuel access.\nNo gaps, clusters, or discreteness: The points are spread randomly.\nNo heteroskedasticity: The variation is fairly consistent.\n\n\nclean_fuels_rural and fuel_imports\n\n\nLinear form: A weak linear relationship exists.\nNegative trend: As clean_fuels_rural increases, fuel imports tend to decrease slightly.\nWeak: Corr:−0.281, the relationship is weak, with considerable variation around the trend.\nHeteroskedastic: Slight heteroskedasticity is present with more variation in fuel imports at higher values of clean_fuels_rural.\nOutliers: There are ouliers.\nNo gaps or clusters: Points are widely spread.\n\n\nclean_fuels_urban and fuel_exports\n\n\nNo trend: There is no clear trend between clean_fuels_urban and fuel_exports.\nNo form: The scatterplot shows random points.\nCorr:−0.109: the correlation is very weak with significant variation suggesting no relationship.\nOutliers: There are outliers at the top, where a few countries have very high fuel exports despite typical or high urban access to clean fuels.\nNo gaps, or discreteness: The points are randomly scattered.\nClustering: There is significant concentration of points in the lower right corner, where countries have high clean_fuels_urban values but low fuel_exports.\nNo heteroskedasticity: No clear variation in the spread.\n\n\nclean_fuels_urban and fuel_imports\n\n\nLinear form: A very weak linear relationship is present.\nNegative trend: As clean_fuels_urban increases, fuel imports tend to decrease slightly.\nWeak: Corr:−0.161, the relationship is weak with significant variation.\nHeteroskedastic: Slight heteroskedasticity is present, as the spread of fuel_imports increases slightly at higher values of clean_fuels_urban.\nOutliers: There are outliers at the lower end, where some countries have very low fuel imports.\nNo gaps or discreteness.\nClustering: There is considerable concentration of points toward the right side of the plot, where many countries have high urban clean fuel access and moderate to low fuel imports.\n\n\nfuel_exports and fuel_imports\n\n\nNo trend: There is no visible relationship.\nNo form: The scatterplot is random, no visible pattern either linear or nonlinear.\nCorr:−0.039, the correlation is extremely weak with significant variation suggesting no relationship.\nOutliers: There are outliers on the right side where a few countries have high fuel exports.\nNo gaps, or discreteness:\nClustering: There is concentration of points in the lower-left corner, where most countries have low fuel exports and low fuel imports.\nNo heteroskedasticity: There is no significant change in the spread of the points across the plot.\n\n\nDeciding on which variables to transform, and examine the before and after patterns\n\nData Plot before transformation\n\n\nCode\nfuel |&gt; select(-country_code, -year) |&gt; \n    GGally::ggpairs()\n\n\n\n\n\n\n\n\n\n\nPost analysis of the histograms of each variable, given that variables clean_fuels_all, clean_fuels_urban show considerable left skewness, applying square and cube transformations respectively would be appropriate.\nFurther, for fuel_exports which exhibits right skewness, I am applying log transformation.\nclean_fuels_rural seems somewhat uniform and is bi-modal, while fuel_imports appears to be fairly symmetric and close to normal so based on this, I am not applying transformations for both.\n\nApplying the transformations\n\n\nCode\ntransformed &lt;- fuel |&gt; \n       mutate(clean_fuels_all_power_2 = (clean_fuels_all)^2, \n              clean_fuels_rural_nt = clean_fuels_rural, \n              clean_fuels_urban_power_3 = (clean_fuels_urban)^3, \n              fuel_exports_log = log1p(fuel_exports), \n              fuel_imports_nt = fuel_imports)\n\ntransformed |&gt; select(-country_code, -year, -clean_fuels_all, -clean_fuels_rural, -clean_fuels_urban, -fuel_exports, -fuel_imports) |&gt; \n    GGally::ggpairs()\n\n\n\n\n\n\n\n\n\n\nPost transformation we can observe in clean_fuels_all and clean_fuels_urban, the skewness is less visible as compared to before. While the transformation has reduced skewness, the plots are not symmetric and nonlinear.\nThe log transformation on fuel_exports has worked really well. The transformation has significantly reduced the skewness and the plot is now roughly symmetric.\nIn the scatter plots, the transformations have increased variance in few such as clean_fuels_all vs fuel_imports.\nHeteroskedasticity in most scatter plots has slightly reduced and we can observe a reduction in skewness as well. That is, in the plots without transformations, few plots have significant concentration of data points in one side of the plot, post transformation this has reduced.\nThe transformation has helped in increasing linearity and reducing curvature as observed in plots related to clean_fuels_all vs clean_fuels_rural. The same can be observed in few other plots as well.\n\n\nThree expected observations from the data\n\n\nNegative relationship between fuel exports and imports: As countries export more fuel, their need to import fuel typically decreases, leading to a negative relationship between these two variables.\nUrban areas having better access to clean fuels than rural areas: Due to better infrastructure and access to resources, urban areas generally are expected to have significantly higher access to clean fuels compared to rural areas.\nCountries with higher economic development are likely to have greater access to clean fuels: Countries with stronger economies and more resources are expected to show better access to clean fuels, as they can invest in cleaner energy infrastructure and technology.\n\n\nThree things that are most surprising, or unexpected in the data\n\n\nSimultaneous increases in both fuel exports and imports: In some cases, countries like Brazil and Jamaica have similar levels of of both fuel exports and fuel imports. This is unexpected, as one would assume that increased exports would reduce the need for imports, but it may point to the import and export of different types of fuel.\nSimilar levels of clean fuel access in both urban and rural areas: In many countries like Algeria and Belarus the access to clean fuels in rural and urban are at par. Moreover, there are instances where rural areas show slightly higher access to clean fuels than urban areas like in Jamaica, which is unexpected since urban regions generally have better infrastructure.\nMinimal progress in clean fuel access in some countries: Despite global initiatives, some countries like Benin and Ethiopia still show very low access to clean fuels, especially in rural areas. This disparity is surprising, given the overall progress worldwide.\n\n\n\n\n\n\nCode\nwinner &lt;- read_csv(\"data/polls_Sep1_2024.csv\")\n\n\n\nPopulation Categories\n\nChecking the categories in population\n\n\nCode\nunique(winner$population)\n\n\n[1] \"lv\" \"rv\" \"a\" \n\n\n\n“lv” - Likely Voters\n\n\nLikely voters are those considered most probable to cast a ballot in the election. This determination is made based on factors such as their past voting habits, involvement in political matters, and their stated intention to participate. This group represents a narrower, more focused group compared to registered voters, as they have demonstrated a stronger commitment to actually casting a ballot.\n\n\n“rv” - Registered Voters\n\n\nThis group consists of individuals who are registered to vote but might not necessarily participate in the election. Registered voters represent a broader category than likely voters, as they include anyone who has met the legal requirements to vote, regardless of their intention to do so. This group covers a diverse range of people, including regular voters, those who vote occasionally, and others who may not vote at all. It offers a general view of the electorate but may not accurately represent the group that will turn out on Election Day.\n\n\n“a” - Adults\n\n\nThis category includes all adults, regardless of whether they are registered or plan to vote. It is the broadest group, capturing opinions from a wide range of people, many of whom may not participate in the election. While it provides a general view of public sentiment, it includes many non-voters, making it less reflective of the actual electorate.\n\nHow the results differ\n\nLikely Voters: Polls of likely voters are considered the most dependable when predicting election outcomes because they specifically focus on individuals who are highly expected to vote. By narrowing the survey sample to people most likely to show up on Election Day, these polls reduce the uncertainty found in broader population categories, such as registered voters or all adults. As a result, the data from likely voter polls closely aligns with actual voting behavior, making them more accurate in estimating the final results. Additionally, because these voters are more politically engaged, their preferences often reflect more informed and committed choices, further increasing the reliability of the polling outcome.\nRegistered Voters: Polls of registered voters tend to be less accurate in predicting election results because they include people who are eligible to vote but might not turn out on Election Day. This group consists of individuals with varying levels of political involvement, from those highly motivated to vote to those who are less inclined to do so. As a result, these polls offer a broader view of public opinion but often misjudge actual voter turnout. Since registered voter polls capture opinions from people who may not be fully committed to voting, the results can change as the election draws nearer. This makes these polls less reliable when compared to likely voter polls, as the surveyed preferences may not convert into actual votes. While they provide a broader perspective, the chances of their results differing from the final outcome are higher.\nAdults: Polls of all adults provide a broader perspective on public opinion, capturing the views of both voters and non-voters. However, this makes them less accurate for predicting election outcomes, as many respondents may not actually vote. These polls can reflect general societal attitudes or preferences, but they may overrepresent groups that are less likely to participate, such as younger adults or those less politically engaged. As a result, while they offer valuable insights into the overall mood of the public, they are not a reliable measure of how the electorate will behave on Election Day, making them less useful for predicting actual results compared to polls focused on likely or registered voters.\n\n\nPollster Bias\n\nExamining the average results for Trump and Harris by each Pollster\n\n\nCode\n# Average results for Harris and Trump by pollster\npollster_bias &lt;- winner |&gt;\n  group_by(pollster) |&gt;\n  summarise(Harris = mean(Harris, na.rm = TRUE),\n            Trump = mean(Trump, na.rm = TRUE)) |&gt;\n  pivot_longer(cols = c(Harris, Trump), names_to = \"Candidate\", values_to = \"Average_Result\")\n\n\nggplot(pollster_bias, aes(x = pollster, y = Average_Result, fill = Candidate)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +  \n  labs(title = \"Average Results for Harris and Trump by Pollster\",\n       x = \"Pollster\", y = \"Average Result (%)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10)) +  \n  scale_fill_manual(values = c(\"blue\", \"red\")) +\n  theme(legend.position = \"right\") \n\n\n\n\n\n\n\n\n\nFrom the bar plot, it seems that most pollsters report relatively balanced results for Harris and Trump. However, there are a few observations:\n\nPollsters showing larger differences: Some pollsters, such as Change Research and Outward Intelligence, appear to report noticeably higher values for Harris compared to Trump. Conversely, pollsters like Bullfinch and Fabrizio/GBAO seem to report higher results for Trump compared to Harris.\nThe majority of pollsters display bars for Harris and Trump that are quite close in height, which suggests no strong favoritism from most pollsters. However, the few mentioned exceptions with more significant gaps could indicate some level of bias toward one candidate.\n\nExamining the transparency score for each Pollster\n\n\nCode\n#Average transparency score for each pollster\npollster_transparency &lt;- winner |&gt;\n  group_by(pollster) |&gt;\n  summarise(avg_transparency = mean(transparency_score, na.rm = TRUE))\n\nggplot(pollster_transparency, aes(x = pollster, y = avg_transparency, fill = avg_transparency)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Transparency Score by Pollster\",\n       x = \"Pollster\",\n       y = \"Avg Transparency Score\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))  \n\n\n\n\n\n\n\n\n\nFrom the Transparency Plot, we can make the following observations:\n\nPollsters with high transparency score: Pollsters like Marist, Kaplan Strategies, Ipsos, and Emerson, with average transparency scores above 8, tend to be more open about their methodologies and data collection practices. This openness suggests that their results are more reliable and balanced. Typically, pollsters with higher transparency scores are less prone to significant bias, as transparency often aligns with greater credibility in polling.\nPollsters with low transparency score: Pollsters like Big Village, Civics, Clarity, and Change Research, with transparency scores around 4 or lower, often demonstrate less openness regarding their polling methodologies, which can raise concerns about potential bias. Low transparency typically makes it harder to assess the reliability of their results, increasing the likelihood of bias due to the lack of visibility into their methods and data sources.\nPollsters with mid level transparency score: Pollsters with transparency scores between 5 and 7, show some openness about their methodologies. This partial transparency can raise questions about their credibility and potential biases, highlighting the need for greater clarity to build trust in their results.\n\nBased on the average result by pollster and transparency scores we can make the following observations:\n\nChange Research, Bullfinch, and Fabrizio/GBAO all display low transparency scores and noticeable bias—Change Research toward Harris, and both Bullfinch and Fabrizio/GBAO toward Trump. This combination of low transparency and bias strongly suggests that their polling results may not be fully impartial or reliable.\nOutward Intelligence shows moderate transparency, but given their bias toward Harris, their results should still be scrutinized. While their transparency is not as low as Change Research or Bullfinch, the bias is still noticeable.\nSimilar observations can be made for other pollsters showing difference in average result for Trump and Harris.\nHence, Pollsters with low transparency and visible bias in their results are more likely to be influenced by unreliable methods. Change Research, Bullfinch, and Fabrizio/GBAO fit this pattern, suggesting a closer examination in their polling outcomes. Higher transparency generally correlates with more reliable results, so focusing on pollsters with high transparency scores might provide more balanced and credible insights."
  },
  {
    "objectID": "posts/assignment-3-praj0022/index.html#visual-exploration",
    "href": "posts/assignment-3-praj0022/index.html#visual-exploration",
    "title": "Diving Deeper into Data Exploration: Visualisation",
    "section": "",
    "text": "doubledecker(xtabs(n ~ Dept + Gender + Admit, data = ucba), gp = gpar(fill = c(“grey90”, “orangered”)))\n\n\\(H_0\\) : There is no association between Department, Gender and Admits. That is, the distribution of admits are not influenced by either department or gender.”\n\nTo generate null samples for the doubledecker plot, we employ permutation since the data consists of categorical variables (department, gender, and admission status). This involves randomly rearranging the admission status while leaving the department and gender categories unchanged. This breaks any original associations between admission and the other variables, resulting in new datasets with randomized relationships. By repeating this process several times, we can build a collection of null samples, each with a different random assignment of admission status, illustrating how the data behaves under random circumstances.\n\n\n\n\n\nggplot(landmine, aes(x, y)) + geom_point(alpha=0.6)\n\n\\(H_0\\) : There is no spatial association between the x and y coordinates of the landmines. The distribution of landmines across the field is random, with no significant clustering or pattern in their spatial arrangement.\n\nTo generate null samples for this plot, we use simulation due to the continuous nature of the x and y spatial coordinates. The process involves randomly reassigning new x and y coordinates to each landmine within the boundary of the field, which disrupts any existing spatial patterns or clustering and assumes randomness. By repeating this random reassignment of new coordinates multiple times, we create a series of null samples, each with a different random spatial distribution of landmines. This method allows us to assess whether the observed clustering in the original data is statistically significant or could simply be due to random variation.\n\n\n\n\n\n\nCode\nlandmine &lt;- read_csv(\"data/landmine3.csv\")\n\n\n\nAlternative plots for the data that might help to discover the landmine locations are\n\n\nScatter Plot: Displays the spatial distribution of landmines, helping to identify any potential clustering or spatial patterns.\nHeatmap: Highlights regions with high landmine concentrations by using color gradients to represent density.\nContour Plot: Displays lines representing regions of equal density to highlight areas with varying concentrations of landmines.\nOverlayed Density Plot: Integrates density estimation with spatial data to pinpoint regions where landmine density is particularly high.\nHexbin Map: Groups landmine data into hexagonal bins to provide a clearer view of density patterns and clustering.\nKernel Density Estimation (KDE) Plot: Offers a smoothed view of landmine distribution to uncover patterns in the spatial density of landmines.\n\n\n\nCode\n# 1. Scatter Plot\nggplot(landmine, aes(x = x, y = y)) +\n  geom_point(alpha=0.3) +\n  labs(title = \"Scatter Plot of Coordinates\") \n\n\n\n\n\n\n\n\n\nCode\n# 2. Heatmap\nggplot(landmine, aes(x = x, y = y)) +\n  stat_bin2d(bins = 30) +\n  scale_fill_viridis_c() +\n  labs(title = \"Heatmap of Landmine Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 3. Contour Plot\nggplot(landmine, aes(x = x, y = y)) +\n  geom_density_2d() +\n  labs(title = \"Contour Plot of Landmine Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 4. Overlayed Density Plot\nggplot(landmine, aes(x, y)) + \n  geom_point(alpha = 0.4) +  \n  geom_density2d(color = \"blue\") + \n  stat_density2d(aes(fill = ..level..), geom = \"polygon\", alpha = 0.4) +  \n  scale_fill_gradient(low = \"yellow\", high = \"red\") +  \n  theme_minimal() + \n  labs(title = \"Overlayed Density Plot of Landmine Locations\",\n       x = \"X Coordinate\",\n       y = \"Y Coordinate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 5. Hexbin Plot\nggplot(landmine, aes(x = x, y = y)) +\n  stat_bin_hex(bins = 30) +     \n  scale_fill_viridis() +   \n  labs(title = \"Hexbin Plot of Landmine Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 6. KDE Plot \nggplot(landmine, aes(x, y)) + \n  geom_density_2d_filled() +\n  labs(title = \"KDE of Landmine Locations\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nCan we see any potential locations of landmines? \n\nYes, we can observe potential locations of landmines from the plots generated above.\nLets consider the overlayed density plot: In the plot, the red area represents the region with the highest density of points, indicating that the landmines are likely concentrated in that area.\nObservations:\n\nThe highest density of points (in red) is around the center of the plot. This is the most probable region where landmines are clustered.\nThe dark yellow to orange gradient surrounding the red zone indicates areas with moderately high densities, which are also potential regions with landmine locations, but with lower certainty compared to the red area.\nThe light yellow regions and blue contour lines on the outer parts of the plot show regions of lower density , indicating a lower likelihood of landmines in these areas.\nHence, the landmines are most likely concentrated in the red, high-density region at the center of the plot. There could be other smaller clusters in some of the lighter orange areas, but they are less significant compared to the central cluster. To effectively detect landmines, the red and orange zones where the density is highest should be prioritized.\n\n\nLine up experiment\n\n\nConstructing the lineup for Kernel Density Estimation (KDE) Plot\n\n\n\nCode\nset.seed(20190709)\nggplot(lineup(null_permute('y'), landmine), \n  aes(x=x, y=y)) +\n  geom_density_2d_filled() +\n  theme_minimal() +\n  facet_wrap(~ .sample) +\n  theme(axis.text=element_blank(),\n        axis.title=element_blank())\n\n\n\n\n\n\n\n\n\nCode\n# decrypt(\"o0vr 8ZGZ D3 k5fDGD53 YM\")\n\n\n\nI presented the lineup to 8 of my friends, and all 8 independently identified plot 13 as the most distinctive. Each of them noted that the bright yellow spot at the center of plot 13 made it stand out clearly from the other plots, highlighting it as the most unique in the lineup.\n\nComputing the p-value\nAs true data position is indeed plot 13\n\nx = 8 ; As 8 people chose the data plot\nn = 8 ; Total number of people the lineup was shown to is 8\nTotal number of plots in the lineup = 20\n\nWe suppose that each person has the same ability to identify the data plot. If we let X be the number of people who correctly identified the data plot in the lineup, then X ~ B(8,p). The visual inference p-value is calculated from testing the hypothesis \\(h_0\\): p = 0.05 vs \\(H_1\\): p != 0.05 , and so P(X = 8) is an extremely small value. The visual inference p-value is extremely small so there is strong evidence to reject the null hypothesis. Hence, it strongly suggests that the data plot is distinguishable from the random plots, meaning it likely contains meaningful features that set it apart from the null distribution and is not just a random occurrence.\n\n\nCode\nnullabor::pvisual(8, 8, 20)\n\n\n     x simulated   binom\n[1,] 8         0 3.9e-11\n\n\nThe p-value is \\(3.9e^{-11}\\).\n\n\n\n\n\nCode\nfuel_data &lt;- read_csv(\"data/wdi_valid.csv\")\n\n\n\n\nCode\noptions(scipen = 999)\nfuel &lt;- fuel_data |&gt;\n        filter(year == \"2022\")\n\n\nChecking the Composition of Data types\n\n\nCode\nvis_dat(fuel)\n\n\n\n\n\n\n\n\n\n\nAs we can observe from the above plot, the data consists of character and numeric datatypes.\n\n\nSummary of the distribution of each of the variable\n\nPlotting the histograms of each variable to assess the distribution.\n\n\nCode\ng1 &lt;- ggplot(fuel, aes(x = clean_fuels_all)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng2 &lt;- ggplot(fuel, aes(x = clean_fuels_rural)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng3 &lt;- ggplot(fuel, aes(x = clean_fuels_urban)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng4 &lt;- ggplot(fuel, aes(x = fuel_exports)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng5 &lt;- ggplot(fuel, aes(x = fuel_imports)) +\n  geom_histogram(binwidth = 5, color = \"white\") \n\ng1 + g2 + g3 + g4 + g5 + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\nFrom the above plot, we can observe that:\n\nclean_fuels_all\n\n\nThe distribution is not symmetric. It is left skewed as there are more countries with high values close to 100%, fewer countries with low values.\nNo clear outliers.\nWeak multi-modality. There exists a slight peaks around 25% and a proper peak around 100%.\nHeaping around 100%, where many countries report high access to clean fuel.\nWhile uneven, the distribution is continuous with no significant gaps.\nNo implausible values. The values are within possible range.\n\n\nclean_fuels_rural\n\n\nThe distribution is somewhat uniform with a few peaks. It is more balanced with peaks on both ends. There is no notable skewness.\nThere are no outliers.\nIt is bi-modal, with peaks around 15% and at 100% indicating two distinct groups.\nThere are gaps in the 25-50% range, indicating fewer countries with medium levels of rural clean fuel access.\nNo heaping is present. No sharp accumulation of values at any specific point.\nNo implausible values. The values are within possible range.\n\n\nclean_fuel_urban\n\n\nThe distribution is not symmetric. There are more countries with high values close to 100%, fewer countries with low values. The distribution is left skewed with most countries showing high levels of urban access to clean fuels.\nThere are outliers towards 0%. While this may not be considered as an extreme outlier.\nIt is unimodal, with a significant peak at 100%.\nThere are gaps between 0 - 50% range, suggesting fewer countries have mid-range urban access to clean fuels.\nThere is considerate heaping around 100%, indicating that many countries have almost full access to clean fuels in urban areas.\nThere seems to be some discreteness and no implausible values. The values are within possible range.\n\n\nfuel_exports\n\n\nThe distribution is not symmetric. It is right skewed with the tail extending toward the higher values, as the majority of countries report low percentages of fuel exports, with fewer countries showing higher export levels.\nThere are outliers towards 100%.\nIt is unimodal, with a clean peak in the lower range around 0 - 10%.\n\nThere are gaps between the 75 - 90% range, where fewer countries report moderate to high fuel exports.\nThere is some heaping in the lower ranges around 0 - 15%, where many countries have minimal fuel exports.\nNo implausible values. The values are within possible range.\n\n\nfuel_imports\n\n\nThe distribution is mostly symmetric with data centered around the middle. It largely follows a normal distribution with a central peak and balanced tails. There is no skewness.\nThe distribution is unimodel, with a prominent peak around 20%.\nThere are no clear outliers observable in the distribution.\nNo heaping is evident, the values are distributed relatively evenly.\nThe data is spread continuously without any gaps.\nNo implausible values. The values are within possible range.\n\nCreating Boxplots to confirm outliers\n\n\nCode\ndf_long &lt;- fuel |&gt;\n  pivot_longer(\n    cols = -c(country_code, year),     \n    names_to = \"fuel_type\",    \n    values_to = \"percentage\"  \n  )\n\n\n\n\nCode\nggplot(df_long, aes(x = percentage)) +\n  geom_boxplot() +\n  facet_wrap(~ fuel_type) +  \n  labs(title = \"Boxplot of fuel variables\",\n       x = \"Percentage\",\n       y = \"Count\") + \n      theme_minimal()\n\n\n\n\n\n\n\n\n\n\nUsing the histograms, while we were able to observe outliers only for clean_fuels_urban and fuel_exports. From the boxplots we can actually detect outliers in fuel_imports as well.\nTherefore, along with clean_fuels_urban and fuel_exports , fuel_imports variable also has outliers present.\n\n\nSummary of the relationship between each of the pairs of variables\n\n\n\nCode\nfuel |&gt; select(-country_code, -year) |&gt; \n    GGally::ggpairs()\n\n\n\n\n\n\n\n\n\n\nclean_fuels_all and clean_fuels_rural\n\n\nLinear form: There is a clear linear relationship between the two variables.\nPositive trend: As clean_fuels_all increases, so does clean_fuels_rural.\nStrong: Corr:0.950, the relationship is very strong with minimal variation around the trend line.\nOutliers: No noticeable outliers.\nHeteroskedastic: No evidence of heteroskedasticity.\nNo gaps, clusters, or discreteness: The points are tightly packed along the trend line.\n\n\nclean_fuels_all and clean_fuels_urban\n\n\nLinear form: The relationship is strongly linear.\nPositive trend: Both variables increase together.\nStrong: Corr:0.950, the correlation is strong with minimal variation.\nOutliers: No significant outliers.\nHeteroskedastic: No heteroskedasticity.\nNo gaps, clusters, or discreteness: Points closely follow the trend without clustering or gaps.\n\n\nclean_fuels_all and fuel_exports\n\n\nNo trend: There is no visible relationship.\nNo form: The scatterplot is random, showing no linear or nonlinear form.\nCorr:−0.067, the correlation is extremely weak with significant variation.\nOutliers: There are a few outliers at the top, where fuel_exports is much higher than the general spread of the data.\nNo heteroskedasticity: The variation is constant throughout the plot.\nNo gaps, clusters, or discreteness: The points are scattered randomly.\n\n\nclean_fuels_all vs fuel_imports\n\n\nLinear form: A weak linear relationship exists.\nNegative trend: As clean_fuels_all increases, fuel imports tend to decrease slightly.\nWeak: Corr:−0.278, the relationship is weak, with significant variation around the trend.\nHeteroskedastic: No clear signs of heteroskedasticity exists, while there is a slightly more spread at higher levels of clean_fuels_all but the change is minimal.\nOutliers: There is one significant outlier below the trend line.\nNo gaps or clusters: No noticeable clustering.\n\n\nclean_fuels_rural and clean_fuels_urban\n\n\nNonlinear form: The relationship is slightly nonlinear, with a curve.\nPositive trend: Both variables increase together, though not at a constant rate.\nModerate: Corr:0.851, the relationship is moderately strong with noticeable variation.\nOutliers: Some outliers where clean_fuels_rural is lower than expected, despite high urban access.\nHeteroskedastic: Slight heteroskedasticity is visible, as the spread of points increases a bit at higher values.\nNo gaps or clusters: No noticeable clustering.\n\n\nclean_fuels_rural and fuel_exports\n\n\nNo trend: No discernible trend exists between the two variables.\nNo form: The points are scattered randomly, no visible pattern either linear or nonlinear.\nCorr:−0.069, the correlation is very weak with significant variation suggesting no relationship.\nOutliers: There are outliers at the top, where a few countries have very high fuel exports despite varying levels of rural clean fuel access.\nNo gaps, clusters, or discreteness: The points are spread randomly.\nNo heteroskedasticity: The variation is fairly consistent.\n\n\nclean_fuels_rural and fuel_imports\n\n\nLinear form: A weak linear relationship exists.\nNegative trend: As clean_fuels_rural increases, fuel imports tend to decrease slightly.\nWeak: Corr:−0.281, the relationship is weak, with considerable variation around the trend.\nHeteroskedastic: Slight heteroskedasticity is present with more variation in fuel imports at higher values of clean_fuels_rural.\nOutliers: There are ouliers.\nNo gaps or clusters: Points are widely spread.\n\n\nclean_fuels_urban and fuel_exports\n\n\nNo trend: There is no clear trend between clean_fuels_urban and fuel_exports.\nNo form: The scatterplot shows random points.\nCorr:−0.109: the correlation is very weak with significant variation suggesting no relationship.\nOutliers: There are outliers at the top, where a few countries have very high fuel exports despite typical or high urban access to clean fuels.\nNo gaps, or discreteness: The points are randomly scattered.\nClustering: There is significant concentration of points in the lower right corner, where countries have high clean_fuels_urban values but low fuel_exports.\nNo heteroskedasticity: No clear variation in the spread.\n\n\nclean_fuels_urban and fuel_imports\n\n\nLinear form: A very weak linear relationship is present.\nNegative trend: As clean_fuels_urban increases, fuel imports tend to decrease slightly.\nWeak: Corr:−0.161, the relationship is weak with significant variation.\nHeteroskedastic: Slight heteroskedasticity is present, as the spread of fuel_imports increases slightly at higher values of clean_fuels_urban.\nOutliers: There are outliers at the lower end, where some countries have very low fuel imports.\nNo gaps or discreteness.\nClustering: There is considerable concentration of points toward the right side of the plot, where many countries have high urban clean fuel access and moderate to low fuel imports.\n\n\nfuel_exports and fuel_imports\n\n\nNo trend: There is no visible relationship.\nNo form: The scatterplot is random, no visible pattern either linear or nonlinear.\nCorr:−0.039, the correlation is extremely weak with significant variation suggesting no relationship.\nOutliers: There are outliers on the right side where a few countries have high fuel exports.\nNo gaps, or discreteness:\nClustering: There is concentration of points in the lower-left corner, where most countries have low fuel exports and low fuel imports.\nNo heteroskedasticity: There is no significant change in the spread of the points across the plot.\n\n\nDeciding on which variables to transform, and examine the before and after patterns\n\nData Plot before transformation\n\n\nCode\nfuel |&gt; select(-country_code, -year) |&gt; \n    GGally::ggpairs()\n\n\n\n\n\n\n\n\n\n\nPost analysis of the histograms of each variable, given that variables clean_fuels_all, clean_fuels_urban show considerable left skewness, applying square and cube transformations respectively would be appropriate.\nFurther, for fuel_exports which exhibits right skewness, I am applying log transformation.\nclean_fuels_rural seems somewhat uniform and is bi-modal, while fuel_imports appears to be fairly symmetric and close to normal so based on this, I am not applying transformations for both.\n\nApplying the transformations\n\n\nCode\ntransformed &lt;- fuel |&gt; \n       mutate(clean_fuels_all_power_2 = (clean_fuels_all)^2, \n              clean_fuels_rural_nt = clean_fuels_rural, \n              clean_fuels_urban_power_3 = (clean_fuels_urban)^3, \n              fuel_exports_log = log1p(fuel_exports), \n              fuel_imports_nt = fuel_imports)\n\ntransformed |&gt; select(-country_code, -year, -clean_fuels_all, -clean_fuels_rural, -clean_fuels_urban, -fuel_exports, -fuel_imports) |&gt; \n    GGally::ggpairs()\n\n\n\n\n\n\n\n\n\n\nPost transformation we can observe in clean_fuels_all and clean_fuels_urban, the skewness is less visible as compared to before. While the transformation has reduced skewness, the plots are not symmetric and nonlinear.\nThe log transformation on fuel_exports has worked really well. The transformation has significantly reduced the skewness and the plot is now roughly symmetric.\nIn the scatter plots, the transformations have increased variance in few such as clean_fuels_all vs fuel_imports.\nHeteroskedasticity in most scatter plots has slightly reduced and we can observe a reduction in skewness as well. That is, in the plots without transformations, few plots have significant concentration of data points in one side of the plot, post transformation this has reduced.\nThe transformation has helped in increasing linearity and reducing curvature as observed in plots related to clean_fuels_all vs clean_fuels_rural. The same can be observed in few other plots as well.\n\n\nThree expected observations from the data\n\n\nNegative relationship between fuel exports and imports: As countries export more fuel, their need to import fuel typically decreases, leading to a negative relationship between these two variables.\nUrban areas having better access to clean fuels than rural areas: Due to better infrastructure and access to resources, urban areas generally are expected to have significantly higher access to clean fuels compared to rural areas.\nCountries with higher economic development are likely to have greater access to clean fuels: Countries with stronger economies and more resources are expected to show better access to clean fuels, as they can invest in cleaner energy infrastructure and technology.\n\n\nThree things that are most surprising, or unexpected in the data\n\n\nSimultaneous increases in both fuel exports and imports: In some cases, countries like Brazil and Jamaica have similar levels of of both fuel exports and fuel imports. This is unexpected, as one would assume that increased exports would reduce the need for imports, but it may point to the import and export of different types of fuel.\nSimilar levels of clean fuel access in both urban and rural areas: In many countries like Algeria and Belarus the access to clean fuels in rural and urban are at par. Moreover, there are instances where rural areas show slightly higher access to clean fuels than urban areas like in Jamaica, which is unexpected since urban regions generally have better infrastructure.\nMinimal progress in clean fuel access in some countries: Despite global initiatives, some countries like Benin and Ethiopia still show very low access to clean fuels, especially in rural areas. This disparity is surprising, given the overall progress worldwide.\n\n\n\n\n\n\nCode\nwinner &lt;- read_csv(\"data/polls_Sep1_2024.csv\")\n\n\n\nPopulation Categories\n\nChecking the categories in population\n\n\nCode\nunique(winner$population)\n\n\n[1] \"lv\" \"rv\" \"a\" \n\n\n\n“lv” - Likely Voters\n\n\nLikely voters are those considered most probable to cast a ballot in the election. This determination is made based on factors such as their past voting habits, involvement in political matters, and their stated intention to participate. This group represents a narrower, more focused group compared to registered voters, as they have demonstrated a stronger commitment to actually casting a ballot.\n\n\n“rv” - Registered Voters\n\n\nThis group consists of individuals who are registered to vote but might not necessarily participate in the election. Registered voters represent a broader category than likely voters, as they include anyone who has met the legal requirements to vote, regardless of their intention to do so. This group covers a diverse range of people, including regular voters, those who vote occasionally, and others who may not vote at all. It offers a general view of the electorate but may not accurately represent the group that will turn out on Election Day.\n\n\n“a” - Adults\n\n\nThis category includes all adults, regardless of whether they are registered or plan to vote. It is the broadest group, capturing opinions from a wide range of people, many of whom may not participate in the election. While it provides a general view of public sentiment, it includes many non-voters, making it less reflective of the actual electorate.\n\nHow the results differ\n\nLikely Voters: Polls of likely voters are considered the most dependable when predicting election outcomes because they specifically focus on individuals who are highly expected to vote. By narrowing the survey sample to people most likely to show up on Election Day, these polls reduce the uncertainty found in broader population categories, such as registered voters or all adults. As a result, the data from likely voter polls closely aligns with actual voting behavior, making them more accurate in estimating the final results. Additionally, because these voters are more politically engaged, their preferences often reflect more informed and committed choices, further increasing the reliability of the polling outcome.\nRegistered Voters: Polls of registered voters tend to be less accurate in predicting election results because they include people who are eligible to vote but might not turn out on Election Day. This group consists of individuals with varying levels of political involvement, from those highly motivated to vote to those who are less inclined to do so. As a result, these polls offer a broader view of public opinion but often misjudge actual voter turnout. Since registered voter polls capture opinions from people who may not be fully committed to voting, the results can change as the election draws nearer. This makes these polls less reliable when compared to likely voter polls, as the surveyed preferences may not convert into actual votes. While they provide a broader perspective, the chances of their results differing from the final outcome are higher.\nAdults: Polls of all adults provide a broader perspective on public opinion, capturing the views of both voters and non-voters. However, this makes them less accurate for predicting election outcomes, as many respondents may not actually vote. These polls can reflect general societal attitudes or preferences, but they may overrepresent groups that are less likely to participate, such as younger adults or those less politically engaged. As a result, while they offer valuable insights into the overall mood of the public, they are not a reliable measure of how the electorate will behave on Election Day, making them less useful for predicting actual results compared to polls focused on likely or registered voters.\n\n\nPollster Bias\n\nExamining the average results for Trump and Harris by each Pollster\n\n\nCode\n# Average results for Harris and Trump by pollster\npollster_bias &lt;- winner |&gt;\n  group_by(pollster) |&gt;\n  summarise(Harris = mean(Harris, na.rm = TRUE),\n            Trump = mean(Trump, na.rm = TRUE)) |&gt;\n  pivot_longer(cols = c(Harris, Trump), names_to = \"Candidate\", values_to = \"Average_Result\")\n\n\nggplot(pollster_bias, aes(x = pollster, y = Average_Result, fill = Candidate)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +  \n  labs(title = \"Average Results for Harris and Trump by Pollster\",\n       x = \"Pollster\", y = \"Average Result (%)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5, size = 10)) +  \n  scale_fill_manual(values = c(\"blue\", \"red\")) +\n  theme(legend.position = \"right\") \n\n\n\n\n\n\n\n\n\nFrom the bar plot, it seems that most pollsters report relatively balanced results for Harris and Trump. However, there are a few observations:\n\nPollsters showing larger differences: Some pollsters, such as Change Research and Outward Intelligence, appear to report noticeably higher values for Harris compared to Trump. Conversely, pollsters like Bullfinch and Fabrizio/GBAO seem to report higher results for Trump compared to Harris.\nThe majority of pollsters display bars for Harris and Trump that are quite close in height, which suggests no strong favoritism from most pollsters. However, the few mentioned exceptions with more significant gaps could indicate some level of bias toward one candidate.\n\nExamining the transparency score for each Pollster\n\n\nCode\n#Average transparency score for each pollster\npollster_transparency &lt;- winner |&gt;\n  group_by(pollster) |&gt;\n  summarise(avg_transparency = mean(transparency_score, na.rm = TRUE))\n\nggplot(pollster_transparency, aes(x = pollster, y = avg_transparency, fill = avg_transparency)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Average Transparency Score by Pollster\",\n       x = \"Pollster\",\n       y = \"Avg Transparency Score\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))  \n\n\n\n\n\n\n\n\n\nFrom the Transparency Plot, we can make the following observations:\n\nPollsters with high transparency score: Pollsters like Marist, Kaplan Strategies, Ipsos, and Emerson, with average transparency scores above 8, tend to be more open about their methodologies and data collection practices. This openness suggests that their results are more reliable and balanced. Typically, pollsters with higher transparency scores are less prone to significant bias, as transparency often aligns with greater credibility in polling.\nPollsters with low transparency score: Pollsters like Big Village, Civics, Clarity, and Change Research, with transparency scores around 4 or lower, often demonstrate less openness regarding their polling methodologies, which can raise concerns about potential bias. Low transparency typically makes it harder to assess the reliability of their results, increasing the likelihood of bias due to the lack of visibility into their methods and data sources.\nPollsters with mid level transparency score: Pollsters with transparency scores between 5 and 7, show some openness about their methodologies. This partial transparency can raise questions about their credibility and potential biases, highlighting the need for greater clarity to build trust in their results.\n\nBased on the average result by pollster and transparency scores we can make the following observations:\n\nChange Research, Bullfinch, and Fabrizio/GBAO all display low transparency scores and noticeable bias—Change Research toward Harris, and both Bullfinch and Fabrizio/GBAO toward Trump. This combination of low transparency and bias strongly suggests that their polling results may not be fully impartial or reliable.\nOutward Intelligence shows moderate transparency, but given their bias toward Harris, their results should still be scrutinized. While their transparency is not as low as Change Research or Bullfinch, the bias is still noticeable.\nSimilar observations can be made for other pollsters showing difference in average result for Trump and Harris.\nHence, Pollsters with low transparency and visible bias in their results are more likely to be influenced by unreliable methods. Change Research, Bullfinch, and Fabrizio/GBAO fit this pattern, suggesting a closer examination in their polling outcomes. Higher transparency generally correlates with more reliable results, so focusing on pollsters with high transparency scores might provide more balanced and credible insights."
  },
  {
    "objectID": "posts/assignment-3-praj0022/index.html#generative-ai-analysis",
    "href": "posts/assignment-3-praj0022/index.html#generative-ai-analysis",
    "title": "Diving Deeper into Data Exploration: Visualisation",
    "section": "Generative AI analysis",
    "text": "Generative AI analysis\n\nI used ChatGPT to discover new alternative plots like Kernel Density Estimation Plot and Overlayed Density Plot and understand more about these plots. I also used it to understand functions like “doubledecker” and “xtabs”.\nI was able to get information on websites where I could read about the population categories in US election data. Further, I used it to understand what demographics made up these categories.\nI also used it to understand the power of ladder transformations. Lastly, it helped me in formatting ggplot labels in the right way for the graphs made to check pollster bias."
  },
  {
    "objectID": "posts/ml2/index.html#grand-tour",
    "href": "posts/ml2/index.html#grand-tour",
    "title": "Principle Component Analysis and Classifiers",
    "section": "Grand Tour",
    "text": "Grand Tour\n\n\n\nThe grand tour reveals how the five features interact across varying projections. While some projections reveal partial separation between birdsongs and financial time series, there remains a significant overlap in many views. This overlap, along with the irregular and unequal spread of each class, suggests that the assumptions of multivariate normality and equal variance-covariance matrices are not fully satisfied. The finance series tend to cluster more tightly, whereas birdsongs exhibit more variation, particularly in entropy and x_acf1. Although classification is still possible, the way the data breaks some of LDA’s assumptions suggests that more flexible models, like decision trees or ensemble methods might handle it more effectively.\n1. Multivariate Normality\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# List of predictors \npredictors &lt;- setdiff(names(t_data), 'type')\nplots &lt;- list()\n\n# QQ plots by class\nfor (predictor in predictors) {\n  plot &lt;- ggplot(t_data, aes(sample = .data[[predictor]], color = type)) +\n    stat_qq() +\n    stat_qq_line() +\n    labs(title = paste(\"QQ Plot for\", predictor, \"by Class\")) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n  plots[[predictor]] &lt;- plot\n}\n\n#  QQ plots\nwrap_plots(plots, ncol = 3)\n\n\n\n\n\n\n\n\nThe QQ plots revealed that linearity and covariate2 exhibited heavy tails and significant deviations from normality, while entropy and x_acf1 also deviated, particularly at the tails. In contrast, covariate1 was the closest to normality with its points aligning well with the theoretical line. Therefore, the multivariate normality assumption is not fully satisfied.\n2. Equal Variance-Covariance Matrices\nThe scatterplot matrix above reveals noticeable differences in spread and separation between birdsongs and finance, particularly in entropy, x_acf1, and covariate2. These disparities indicate that the assumption of equal variance-covariance matrices is not met, suggesting LDA may produce biased decision boundaries due to differing variance structures across classes.\n3. Independence The dataset is ordered with 500 finance samples followed by 474 birdsongs samples. While this structure might suggest some sequencing, it doesn’t automatically imply dependency. Since there are no clear time-based indicators or hierarchical groupings, we can assume the independence assumption is satisfied.\n\nb.\nSplit the data into training and testing sets\n\n\nCode\nset.seed(42)  \n\n# Data Split\ndata_split &lt;- initial_split(t_data, prop = 2/3, strata = type)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\ntrain_data |&gt; count(type)\n\n\n# A tibble: 2 × 2\n  type          n\n  &lt;fct&gt;     &lt;int&gt;\n1 birdsongs   316\n2 finance     333\n\n\nCode\ntest_data |&gt; count(type)\n\n\n# A tibble: 2 × 2\n  type          n\n  &lt;fct&gt;     &lt;int&gt;\n1 birdsongs   158\n2 finance     167\n\n\nDistribution of Classes in Training and Testing Sets\n\n\nCode\n# Training Set Distribution\ntrain_plot &lt;- train_data |&gt; \n  count(type) |&gt; \n  ggplot(aes(x = type, y = n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Training Set Distribution\", y = \"Count\", x = \"Type\") +\n  theme_minimal()\n\n# Testing Set Distribution\ntest_plot &lt;- test_data |&gt; \n  count(type) |&gt; \n  ggplot(aes(x = type, y = n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Testing Set Distribution\", y = \"Count\", x = \"Type\") +\n  theme_minimal()\n\n\ntrain_plot + test_plot\n\n\n\n\n\n\n\n\n\nThe distribution looks good and well-balanced across the training and testing datasets, which means the stratified sampling worked correctly. The bars are nearly equal for both birdsongs and finance, indicating there is no class imbalance.\n\n\nc.\nFitting a linear classifier to the training data\nLogistic model\n\n\nCode\n# logistic regression model\nlogistic_mod &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  set_mode(\"classification\")\n\n# Fit the model\nlogistic_fit &lt;- logistic_mod |&gt; \n  fit(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, data = train_data)\n\n# Display the model coefficients\ntidy(logistic_fit)\n\n\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  -0.613     0.785    -0.781  4.35e- 1\n2 linearity     0.0521    0.0100    5.18   2.19e- 7\n3 entropy      -2.64      0.477    -5.53   3.27e- 8\n4 x_acf1       -0.460     0.211    -2.18   2.90e- 2\n5 covariate1    0.0195    0.225     0.0867 9.31e- 1\n6 covariate2    1.36      0.125    10.9    1.81e-27\n\n\nCode\nglance(logistic_fit) \n\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1          899.     648  -252.  516.  543.     504.         643   649\n\n\n\nLinearity (Estimate = 0.052, p &lt; 0.001): Slightly boosts the chances of classifying the series as finance. This means more linear time series are more likely to be financial data.\nEntropy (Estimate = -2.638, p &lt; 0.001): Has a strong negative effect, making it much more likely for the series to be classified as birdsongs. It reflects the chaotic and unpredictable nature typical of birdsong patterns compared to finance.\nx_acf1 (Estimate = -0.460, p &lt; 0.05): Shows a moderate negative link with finance. Higher autocorrelation seems to be more common in birdsongs than in financial data.\nCovariate1 (Estimate = 0.019, p = 0.93): Almost no effect on classification, it’s statistically insignificant. It doesn’t really help in telling birdsongs and finance apart.\nCovariate2 (Estimate = 1.355, p &lt; 0.001): Strongly leans towards identifying finance. It suggests that financial time series are more structured and consistent.\n\nIn conclusion, the logistic regression model reveals entropy as the strongest predictor for birdsongs, reflecting their randomness, while covariate2 distinguishes financial data due to its structured patterns. Linearity and x_acf1 provide moderate contribution, and covariate1 is redundant. This indicates birdsongs are more disordered, whereas finance data are more structured.\nLDA \n\n\nCode\n# Load necessary libraries\nlibrary(discrim)\nlibrary(tidymodels)\n\n# Define the LDA model specification\nlda_spec &lt;- discrim_linear() |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"MASS\", prior = c(0.5, 0.5))\n\n# Fit the LDA model to the training data\nlda_fit &lt;- lda_spec |&gt; \n  fit(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, data = train_data)\n\n# Display the fitted model\nlda_fit\n\n\nparsnip model object\n\nCall:\nlda(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, \n    data = data, prior = ~c(0.5, 0.5))\n\nPrior probabilities of groups:\nbirdsongs   finance \n      0.5       0.5 \n\nGroup means:\n            linearity   entropy    x_acf1 covariate1 covariate2\nbirdsongs -0.06582734 0.8403870 0.2152013   2.989098   1.023164\nfinance   17.28510479 0.5066114 0.5103231   3.000907   2.447719\n\nCoefficients of linear discriminants:\n                   LD1\nlinearity   0.02048526\nentropy    -1.38450178\nx_acf1     -0.23675688\ncovariate1  0.03386640\ncovariate2  0.76381428\n\n\n\nGroup Means Analysis: Financial time series have higher average values for linearity (17.285) and covariate2 (2.4), indicating more structured and consistent patterns. Birdsongs exhibit higher entropy (0.84), suggesting more randomness and disorder.\nLinear Discriminant Coefficients: Entropy has the strongest negative coefficient (-1.385), reinforcing its role in distinguishing birdsongs due to their unpredictable nature. Covariate2 (0.764) and linearity (0.020) positively influence classification towards finance, aligning with their structured behavior.\nx_acf1 (-0.237) is moderately negative, slightly leaning towards birdsongs, but less influential than entropy.\nCovariate1 (0.034) shows a minimal positive influence, suggesting it has a negligible impact on classification.\nPrior Probabilities: Both classes are equally weighted (0.5), reflecting balanced consideration during classification.\n\nThe LDA model leverages entropy as the most powerful discriminator for birdsongs, while linearity and covariate2 drive the classification towards finance. This reinforces the understanding that financial data are more structured, while birdsongs are more random and disordered.\n\n\nd. \nEvaluate the model\nTrainig data evaluation\n1. Predictions\n\n\nCode\n# Generate predictions for the Logistic Regression model\nlogistic_train_pred &lt;- predict(logistic_fit, new_data = train_data, type = \"class\")\nlogistic_train_pred &lt;- bind_cols(train_data, logistic_train_pred)\n\n# Generate predictions for the LDA model\nlda_train_pred &lt;- predict(lda_fit, new_data = train_data, type = \"class\")\nlda_train_pred &lt;- bind_cols(train_data, lda_train_pred)\n\n# Display the first few rows\nhead(logistic_train_pred)\n\n\n# A tibble: 6 × 7\n  linearity entropy x_acf1 type      covariate1 covariate2 .pred_class\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;      \n1    -0.126   0.898 -0.609 birdsongs       2.72      2.81  finance    \n2     1.01    0.856  1.23  birdsongs       3.26     -0.216 birdsongs  \n3    -0.424   0.929  1.01  birdsongs       3.44      0.871 birdsongs  \n4    -0.225   0.807  0.453 birdsongs       2.39      0.597 birdsongs  \n5    -3.39    0.678  0.306 birdsongs       2.99      1.82  birdsongs  \n6    -0.373   0.813 -0.259 birdsongs       3.07      0.674 birdsongs  \n\n\nCode\nhead(lda_train_pred)\n\n\n# A tibble: 6 × 7\n  linearity entropy x_acf1 type      covariate1 covariate2 .pred_class\n      &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;      \n1    -0.126   0.898 -0.609 birdsongs       2.72      2.81  finance    \n2     1.01    0.856  1.23  birdsongs       3.26     -0.216 birdsongs  \n3    -0.424   0.929  1.01  birdsongs       3.44      0.871 birdsongs  \n4    -0.225   0.807  0.453 birdsongs       2.39      0.597 birdsongs  \n5    -3.39    0.678  0.306 birdsongs       2.99      1.82  birdsongs  \n6    -0.373   0.813 -0.259 birdsongs       3.07      0.674 birdsongs  \n\n\nBased on the initial observations from the training data, both the logistic regression and LDA models misclassified the first “birdsongs” instance as “finance.” For the remaining rows, both models correctly identified “birdsongs.” This indicates a shared misclassification pattern for the first observation, but overall alignment with the true class for the rest of the initial data. No strong bias towards either class is evident from this small sample.\n2. Confusion Matrix\n\nLogistic Regression Confusion Matrix\n\n\n\nCode\n# confusion matrix for Logistic Regression\nlogistic_conf_matrix &lt;- logistic_train_pred |&gt;\n  count(type, .pred_class) |&gt;\n  group_by(type) |&gt;\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |&gt;\n  pivot_wider(names_from = .pred_class, values_from = n, values_fill = 0) |&gt;\n  select(type, birdsongs, finance, Accuracy)\n\n\nlogistic_conf_matrix\n\n\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  &lt;fct&gt;         &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 birdsongs       262      54    0.829\n2 finance          67     266    0.799\n\n\nThe logistic regression model demonstrates strong performance on the training data, achieving an accuracy of 83% for birdsongs and 80% for finance. Among the birdsongs, 262 instances were correctly classified, while 54 were misclassified as finance. For the finance category, 266 were correctly identified, with 67 incorrectly labeled as birdsongs. The model shows a slight bias towards better identification of birdsongs, although this difference is minimal. Overall, the misclassifications are relatively balanced, with finance being slightly more prone to confusion with birdsongs.\n\nLDA Confusion Matrix\n\n\n\nCode\n# confusion matrix for LDA\nlda_conf_matrix &lt;- lda_train_pred |&gt;\n  count(type, .pred_class) |&gt;\n  group_by(type) |&gt;\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |&gt;\n  pivot_wider(names_from = .pred_class, values_from = n, values_fill = 0) |&gt;\n  select(type, birdsongs, finance, Accuracy)\n\n\nlda_conf_matrix\n\n\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  &lt;fct&gt;         &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 birdsongs       272      44    0.861\n2 finance          72     261    0.784\n\n\nThe LDA model shows a strong performance on the training data, achieving an 86% accuracy for birdsongs and 78% accuracy for finance. In the birdsongs category, 272 instances were correctly classified, while 44 were misclassified as finance. For finance, 261 instances were correctly identified, with 72 incorrectly labeled as birdsongs. Compared to logistic regression, LDA slightly improves accuracy for birdsongs but has a marginally lower accuracy for finance. The model seems to better differentiate birdsongs while facing slightly more difficulty with finance series.\nTest data evaluation\n1. Predictions\n\n\nCode\n# Generate predictions for the Logistic Regression model on test data\nlogistic_test_pred &lt;- predict(logistic_fit, new_data = test_data, type = \"class\")\n\n# Bind the predictions to the test data\nlogistic_test_pred &lt;- bind_cols(test_data, logistic_test_pred)\n\n# Display the first few rows\nhead(logistic_test_pred)\n\n\n# A tibble: 6 × 7\n  linearity entropy  x_acf1 type      covariate1 covariate2 .pred_class\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;      \n1   -0.169    1.04  -0.248  birdsongs       2.58      1.41  birdsongs  \n2   -0.443    0.806  0.571  birdsongs       3.61      1.12  birdsongs  \n3   -0.0896   1.14  -0.0945 birdsongs       3.50      1.67  birdsongs  \n4   -0.0525   0.979 -0.688  birdsongs       3.91      1.39  birdsongs  \n5    0.227    0.931  0.665  birdsongs       2.22      0.454 birdsongs  \n6    0.180    0.817  0.681  birdsongs       2.98      1.09  birdsongs  \n\n\nCode\nlda_test_pred &lt;- predict(lda_fit, new_data = test_data, type = \"class\")\n\n# Bind the predictions to the test data\nlda_test_pred &lt;- bind_cols(test_data, lda_test_pred)\n\n# Display the first few rows\nhead(lda_test_pred)\n\n\n# A tibble: 6 × 7\n  linearity entropy  x_acf1 type      covariate1 covariate2 .pred_class\n      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;      \n1   -0.169    1.04  -0.248  birdsongs       2.58      1.41  birdsongs  \n2   -0.443    0.806  0.571  birdsongs       3.61      1.12  birdsongs  \n3   -0.0896   1.14  -0.0945 birdsongs       3.50      1.67  birdsongs  \n4   -0.0525   0.979 -0.688  birdsongs       3.91      1.39  birdsongs  \n5    0.227    0.931  0.665  birdsongs       2.22      0.454 birdsongs  \n6    0.180    0.817  0.681  birdsongs       2.98      1.09  birdsongs  \n\n\nLooking at the first few predictions, both the Logistic Regression and LDA models seem to confidently classify “birdsongs” correctly. This indicates that, at least for this subset, both models are picking up on the right features to distinguish birdsongs. Confusion matrix should help us understand the braoder performance and classification when it comes to finance.\n2. Confusion Matrix\n\nLogistic Regression Confusion Matrix\n\n\n\nCode\n# Generate the confusion matrix for Logistic Regression\nlogistic_test_conf_matrix &lt;- logistic_test_pred |&gt;\n  count(type, .pred_class) |&gt;\n  group_by(type) |&gt;\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |&gt;\n  pivot_wider(names_from = \".pred_class\", values_from = n, values_fill = 0) |&gt;\n  select(type, birdsongs, finance, Accuracy)\n\n# Display the confusion matrix\nlogistic_test_conf_matrix\n\n\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  &lt;fct&gt;         &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 birdsongs       134      24    0.848\n2 finance          45     122    0.731\n\n\nCode\nlogistic_accuracy &lt;- accuracy(logistic_test_pred, truth = type, estimate = .pred_class)$.estimate\nlogistic_accuracy\n\n\n[1] 0.7876923\n\n\nThe confusion matrix for the Logistic Regression model shows strong performance for Birdsongs, with 134 correctly classified and only 24 misclassified, resulting in an 85% accuracy. For Finance, the model correctly identified 122 instances but misclassified 45 as birdsongs, achieving 73% accuracy. This suggests the model is more effective at identifying birdsongs, while it struggles a bit more with distinguishing finance time series, possibly due to overlapping features.\n\nLDA Confusion Matrix\n\n\n\nCode\n# Generate the confusion matrix for LDA\nlda_test_conf_matrix &lt;- lda_test_pred |&gt;\n  count(type, .pred_class) |&gt;\n  group_by(type) |&gt;\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |&gt;\n  pivot_wider(names_from = \".pred_class\", values_from = n, values_fill = 0) |&gt;\n  select(type, birdsongs, finance, Accuracy)\n\n# Display the confusion matrix\nlda_test_conf_matrix\n\n\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  &lt;fct&gt;         &lt;int&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 birdsongs       139      19    0.880\n2 finance          45     122    0.731\n\n\nCode\nlda_accuracy &lt;- accuracy(lda_test_pred, truth = type, estimate = .pred_class)$.estimate\n\nlda_accuracy\n\n\n[1] 0.8030769\n\n\nThe confusion matrix for the LDA model on the test data shows a solid performance for Birdsongs, with 139 correctly classified and only 19 misclassified, resulting in an 88% accuracy. For Finance, the model matched the Logistic Regression with 122 correct predictions and 45 misclassifications, achieving a 73% accuracy. This indicates LDA is slightly more effective at identifying birdsongs compared to Logistic Regression, but similarly struggles to distinguish finance time series.\nThe accuracy results reveal a slight edge for LDA over Logistic Regression, with LDA achieving 0.80 compared to 0.79 for Logistic Regression. While the difference is minimal, it suggests that LDA might be capturing the underlying structure of the data just a bit more effectively.\nVariable importance\nBased on the observed output of tidy(logistic_fit) and summary of LDA coeffiencts seen above in model fit:\nBoth LDA and Logistic Regression identify entropy as a strong predictor of birdsongs and covariate2 as a major indicator of finance. linearity is more pronounced in LDA (17.285 vs. -0.066), suggesting LDA finds it more influential in distinguishing between the two classes. Both models downplay the importance of covariate1, though LDA still assigns it a positive coefficient. x_acf1 remains consistently aligned with birdsongs in both models. Both models align on the importance of entropy and covariate2, but LDA amplifies the role of linearity more significantly than logistic regression.\nPrediction Probabilities\n\n\nCode\n# Logistic Regression - Training Predictions\nlogistic_train_prob &lt;- logistic_fit |&gt; \n  augment(new_data = train_data, type.predict = \"prob\") |&gt; \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n# Logistic Regression - Test Predictions\nlogistic_test_prob &lt;- logistic_fit |&gt; \n  augment(new_data = test_data, type.predict = \"prob\") |&gt; \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n# LDA - Training Predictions\nlda_train_prob &lt;- lda_fit |&gt; \n  augment(new_data = train_data, type.predict = \"prob\") |&gt; \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n# LDA - Test Predictions\nlda_test_prob &lt;- lda_fit |&gt; \n  augment(new_data = test_data, type.predict = \"prob\") |&gt; \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n\n\n\nCode\n# Logistic Regression - Training\np1 &lt;- ggplot(logistic_train_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"Logistic Regression - Training Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n# Logistic Regression - Test\np2 &lt;- ggplot(logistic_test_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"Logistic Regression - Test Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n# LDA - Training\np3 &lt;- ggplot(lda_train_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"LDA - Training Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n# LDA - Test\np4 &lt;- ggplot(lda_test_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"LDA - Test Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n\n\n\n\nBased on Training Data: Both models are quite confident with birdsongs, showing narrow IQRs and medians around 0.75, and are able to differentiate them well. For finance, the spread is much wider. Logistic Regression has a larger IQR with visible outliers, hinting at more uncertainty compared to birdsongs. Based on Test Data: The same trends can be observed, birdsongs are being predicted well with compact IQRs and solid medians. For finance, both models struggle a bit more. LDA, in particular, shows a wider IQR and more scattered outliers.\nBoth models are consistently strong with birdsongs but show noticeable uncertainty with finance, especially in the test set. LDA’s predictions are more spread out, while Logistic Regression displays more extreme outliers.\nIdentifying Misclassifications in Logistic Regression\n\n\nCode\n# Logistic Regression Misclassifications\nlogistic_misclassified &lt;- logistic_test_pred |&gt;\n  filter(type != .pred_class)\n\n# LDA Misclassifications\nlda_misclassified &lt;- lda_test_pred |&gt;\n  filter(type != .pred_class)\n\n# Common Misclassifications \ncommon_misclassifications &lt;- inner_join(\n  logistic_misclassified |&gt; select(linearity, entropy, x_acf1, covariate1, covariate2, type),\n  lda_misclassified |&gt; select(linearity, entropy, x_acf1, covariate1, covariate2, type),\n  by = c(\"linearity\", \"entropy\", \"x_acf1\", \"covariate1\", \"covariate2\", \"type\")\n)\n\n\nmisclassification_summary &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"LDA\", \"Common Misclassifications\"),\n  Misclassifications = c(nrow(logistic_misclassified), nrow(lda_misclassified), nrow(common_misclassifications))\n)\n\n# Summary \n\nkable(misclassification_summary, caption = \"Summary of Misclassifications\")\n\n\n\nSummary of Misclassifications\n\n\nModel\nMisclassifications\n\n\n\n\nLogistic Regression\n69\n\n\nLDA\n64\n\n\nCommon Misclassifications\n62\n\n\n\n\n\nCode\n# Logistic Regression breakdown\nlogistic_misclassified_summary &lt;- logistic_misclassified |&gt;\n  count(type, .pred_class) |&gt;\n  rename(Misclassified_As = .pred_class, Count = n)\n\n\nkable(logistic_misclassified_summary, caption = \"Logistic Regression Misclassifications\")\n\n\n\nLogistic Regression Misclassifications\n\n\ntype\nMisclassified_As\nCount\n\n\n\n\nbirdsongs\nfinance\n24\n\n\nfinance\nbirdsongs\n45\n\n\n\n\n\nCode\n# LDA breakdown\nlda_misclassified_summary &lt;- lda_misclassified |&gt;\n  count(type, .pred_class) |&gt;\n  rename(Misclassified_As = .pred_class, Count = n)\n\n\nkable(lda_misclassified_summary, caption = \"LDA Misclassifications\")\n\n\n\nLDA Misclassifications\n\n\ntype\nMisclassified_As\nCount\n\n\n\n\nbirdsongs\nfinance\n19\n\n\nfinance\nbirdsongs\n45\n\n\n\n\n\nCode\n# Common misclassification breakdown\ncommon_misclassifications_summary &lt;- common_misclassifications |&gt;\n  count(type) |&gt;\n  rename(Count = n)\n\n\nkable(common_misclassifications_summary, caption = \"Common Misclassifications Breakdown\")\n\n\n\nCommon Misclassifications Breakdown\n\n\ntype\nCount\n\n\n\n\nbirdsongs\n19\n\n\nfinance\n43\n\n\n\n\n\nBased on the misclassification analysis: - Logistic Regression had 69 misclassifications, while LDA had 64. Both models struggled with the same observations, sharing 62 common errors. - Logistic Regression misclassified Birdsongs as finance 24 times, while LDA did so 19 times. - Both models struggled more with Finance, with 45 instances being misclassified as birdsongs in each case. - Out of the 62 shared errors, 19 were birdsongs incorrectly labeled as finance, and 43 were finance mislabeled as birdsongs. This highlights a clear difficulty in distinguishing finance series, which seem to overlap more with birdsongs in terms of feature space. - Finance series are consistently more difficult to classify correctly, while birdsongs show slightly better separation in both models.\nMisclassification Plot\n\n\nCode\nggplot() +\n  geom_point(data = logistic_misclassified, aes(x = linearity, y = entropy, color = \"Logistic Regression\"), alpha = 0.7) +\n  geom_point(data = lda_misclassified, aes(x = linearity, y = entropy, color = \"LDA\"), alpha = 0.7) +\n  labs(title = \"Overlay of Misclassifications: Logistic Regression vs LDA\",\n       x = \"Linearity\",\n       y = \"Entropy\") +\n  scale_color_manual(values = c(\"Logistic Regression\" = \"blue\", \"LDA\" = \"red\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFrom the plot, both models struggle the most around the zero region of Linearity, suggesting overlap in features between classes.\nLDA misclassifies across a wider range, especially with negative linearity and higher entropy.\nLogistic Regression’s errors are more clustered, indicating it is more selective but still misses specific patterns.\nSeveral points overlap, confirming the 62 common misclassifications.\n\n\n\ne.\n\n\nCode\n# ROC Curve for Logistic Regression - Test Data\nlogistic_roc &lt;- roc_curve(logistic_test_prob, truth = type, .pred_birdsongs)\nlogistic_auc &lt;- roc_auc(logistic_test_prob, truth = type, .pred_birdsongs)\n\n# ROC Curve for LDA - Test Data\nlda_roc &lt;- roc_curve(lda_test_prob, truth = type, .pred_birdsongs)\nlda_auc &lt;- roc_auc(lda_test_prob, truth = type, .pred_birdsongs)\n\n# Plotting both ROC Curves\nlogistic_plot &lt;- ggplot(logistic_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"blue\") +\n  geom_abline(linetype = \"dashed\") +\n  labs(title = \"Logistic Regression ROC Curve\",\n       subtitle = paste(\"AUC:\", round(logistic_auc$.estimate, 3))) +\n  theme_minimal()\n\nlda_plot &lt;- ggplot(lda_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"red\") +\n  geom_abline(linetype = \"dashed\") +\n  labs(title = \"LDA ROC Curve\",\n       subtitle = paste(\"AUC:\", round(lda_auc$.estimate, 3))) +\n  theme_minimal()\n\n\nlogistic_plot + lda_plot\n\n\n\n\n\n\n\n\n\nLogistic Regression - The ROC curve shows good separation, with an AUC of 0.876, this indicates strong ability to differentiate between birdsongs and finance time series. - It’s clearly above the diagonal, the model performs better than random guessing consistently. LDA - LDA’s ROC curve is quite similar to Logistic Regression but has a slightly higher AUC of 0.882. - This small improvement suggests LDA handles classification boundaries a bit more accurately, especially when separating birdsongs from finance. Both models perform well, but LDA has a slight advantage with its higher AUC. While the difference is not massive, it indicates LDA is just a bit better at identifying true positives while keeping false positives lower.\n\n\nf. \n\nThe analysis shows that financial time series and birdsongs have pretty distinct patterns. Financial data are generally more structured and linear, with higher linearity and covariate2 values reflecting the smooth, predictable trends you’d expect in markets. On the other hand, birdsongs are much more random and unpredictable, which is captured well by their higher entropy values.\nBoth models picked up on these differences, entropy was a strong indicator for birdsongs, while linearity pointed more towards finance. Interestingly, finance series were sometimes mistaken for birdsongs, suggesting there are some irregularities in financial data that mirror the chaotic patterns of birdsongs.\nLastly, the models did a good job capturing these differences, though there are still some tricky overlaps that lead to misclassifications.\n\n\n\nTuning a non-linear classifier\n\n\na. Fitting a bad decision tree\nBelow, a decision tree is constructed using constarints min_n = 1 and cost_complexity = 0. This is a bad decision tree as it is not regularized and will likely overfit the training data.\n\n\nCode\n# decision tree model \ntree_spec &lt;- decision_tree() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"rpart\", \n             control = rpart.control(minsplit = 1, cp = 0),\n             model = TRUE)\n\n# Fit the model \nbad_tree &lt;- tree_spec %&gt;%\n  fit(type ~ ., data = train_data)\n\n\nPrediction Metrics\nTrain Predictions\n\n\nCode\ntrain_preds &lt;- predict(bad_tree, train_data) %&gt;%\n  bind_cols(train_data %&gt;% select(type))\n\ntest_preds &lt;- predict(bad_tree, test_data) %&gt;%\n  bind_cols(test_data %&gt;% select(type))\n\n\n\n\nCode\ntrain_metrics &lt;- train_preds %&gt;%\n  metrics(truth = type, estimate = .pred_class)\n\nkable(train_metrics, caption = \"Training Metrics for Bad Decision Tree\")\n\n\n\nTraining Metrics for Bad Decision Tree\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n1\n\n\nkap\nbinary\n1\n\n\n\n\n\nThe model achieves a perfect 100% accuracy on the training data. This is a classic sign of overfitting, where the tree memorizes the training samples exactly instead of learning generalizable patterns. The Kappa statistic is also 1.00, indicating perfect agreement with the true labels, which is unrealistic for real-world applications.\nTesting Predictions\n\n\nCode\ntest_metrics &lt;- test_preds %&gt;%\n  metrics(truth = type, estimate = .pred_class)\n\n\nkable(test_metrics, caption = \"Testing Metrics for Bad Decision Tree\")\n\n\n\nTesting Metrics for Bad Decision Tree\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.8369231\n\n\nkap\nbinary\n0.6739850\n\n\n\n\n\nOn the testing set, the model’s accuracy drops to 84%, and the Kappa statistic reduces to 0.67. Although the accuracy remains relatively high, it is clear that the model struggles to generalize beyond the training data. The noticeable drop from 100% to 84% reflects the impact of overfitting, where the tree’s excessive branching failed to capture meaningful, general patterns.\nVisualizing the Bad Decision Tree\n\n\nCode\n# Extract the fitted model \nbad_tree_model &lt;- bad_tree %&gt;%\n  extract_fit_engine()\n\n# Plot \nrpart.plot::rpart.plot(bad_tree_model, main = \"Bad Decision Tree\", roundint = FALSE, cex = 0.5)\n\n\n\n\n\n\n\n\n\n\nThe decision tree visual shows how deeply the model has branched. It’s highly fragmented, with many branches and tiny nodes, expected when min_n = 1, allowing splits on nearly every observation.\nWith cost_complexity = 0, there’s no pruning to reduce unnecessary splits, resulting in an overgrown structure that’s cluttered and difficult to interpret.\nThe model memorises the training data but fails to generalise, this is a clear case of overfitting. It captures noise instead of meaningful patterns, highlighting the importance of proper tuning.\n\n\n\nb. Hyper parameters\nDefineing the decision tree model with the hyperparameters set to tune()\n\n\nCode\n# decision tree model with tunable hyperparameters\ntree_spec &lt;- decision_tree(tree_depth = tune(),\n  min_n = tune(),\n  cost_complexity = tune()) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"rpart\")\n\n\nCreating a grid of hyperparameters with 5 levels for each parameter, and set up a 5-fold cross-validation process to evaluate the model\n\n\nCode\n# grid of hyperparameters\nhyper_grid &lt;- grid_regular(\n  tree_depth(),\n  min_n(),\n  cost_complexity(),\n  levels = 5\n)\n\n# 5-fold cross-validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = type)\n\n\nThe workflow is defined with the decision tree specification and the formula for prediction. A grid search is then performed to evaluate all combinations of parameters.\n\n\nCode\n# Define the workflow \ntree_workflow &lt;- workflow() %&gt;%\n  add_model(tree_spec) %&gt;%\n  add_formula(type ~ .)\n\n# Tune the model\nset.seed(123)\ntuned_results &lt;- tune_grid(\n  tree_workflow,\n  resamples = cv_folds,\n  grid = hyper_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\n\nExtracting all metrics to identify the best performing configurations.\n\n\nCode\n# metrics from the tuning process\ntree_metrics &lt;- tuned_results %&gt;%\n  collect_metrics()\n\n# top configurations by accuracy (sorted)\nsorted_configs &lt;- tree_metrics %&gt;%\n  filter(.metric == \"accuracy\") %&gt;%\n  arrange(desc(mean))\n\n# top 15 rows \nsorted_configs %&gt;%\n  slice_head(n = 15)\n\n\n# A tibble: 15 × 9\n   cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1    0.0000000001          8    11 accuracy binary     0.912     5 0.00758\n 2    0.0000000178          8    11 accuracy binary     0.912     5 0.00758\n 3    0.00000316            8    11 accuracy binary     0.912     5 0.00758\n 4    0.000562              8    11 accuracy binary     0.912     5 0.00758\n 5    0.0000000001         11    11 accuracy binary     0.904     5 0.00516\n 6    0.0000000178         11    11 accuracy binary     0.904     5 0.00516\n 7    0.00000316           11    11 accuracy binary     0.904     5 0.00516\n 8    0.000562             11    11 accuracy binary     0.904     5 0.00516\n 9    0.0000000001         15    11 accuracy binary     0.903     5 0.00509\n10    0.0000000178         15    11 accuracy binary     0.903     5 0.00509\n11    0.00000316           15    11 accuracy binary     0.903     5 0.00509\n12    0.000562             15    11 accuracy binary     0.903     5 0.00509\n13    0.0000000001          8    30 accuracy binary     0.895     5 0.0132 \n14    0.0000000001         11    30 accuracy binary     0.895     5 0.0132 \n15    0.0000000001         15    30 accuracy binary     0.895     5 0.0132 \n# ℹ 1 more variable: .config &lt;chr&gt;\n\n\n\nThe hyperparameter tuning process explored different combinations of tree_depth, min_n, and cost_complexity using 5-fold cross-validation. As the tree depth increased, the model’s accuracy improved up to a certain point. The best configuration, with an average accuracy of 0.91, was, Tree Depth: 8, Min Node Size: 11, Cost Complexity: 1e-10.\nThis setup achieved a strong ROC AUC of 0.91, indicating it was effective at distinguishing between classes. The low standard error values suggest the model performed consistently well across the folds, capturing patterns in the data reliably.\n\nPlotting the performance metrics\n\n\nCode\n# performance metrics for accuracy and ROC AUC\nautoplot(tuned_results, metric = \"accuracy\") +\n  labs(title = \"Model Accuracy Across Hyperparameters\")\n\n\n\n\n\n\n\n\n\nModel Accuracy Across Hyperparameters - The plot shows how accuracy changes across different combinations of tree depth, min_n, and cost complexity. - The most consistent and accurate results (around 0.91) came from the combination of tree depth = 8, min_n = 11, and cost complexity = 1e-10. - Models with smaller min_n values like 2 also reached close to 0.90 accuracy but didn’t improve further, and were more prone to overfitting. - Higher min_n values (like 30 or 40) gave stable results but didn’t outperform the configuration with min_n = 11. - Accuracy dropped noticeably when cost complexity increased to 1e-02, suggesting that too much pruning prevented the model from learning useful patterns.\nIn conclusion, the best-performing model struck a good balance: it was deep enough to capture structure, had enough data in each node to avoid noise, and wasn’t over-pruned. The final choice: Tree Depth: 8 Min Node Size: 11 Cost Complexity: 1e-10\nPlot: Model ROC AUC Across Hyperparameters\n\n\nCode\nautoplot(tuned_results, metric = \"roc_auc\") +\n  labs(title = \"Model ROC AUC Across Hyperparameters\")\n\n\n\n\n\n\n\n\n\nModel ROC AUC Across Hyperparameters: - The highest ROC AUC of 0.95 is observed in, Minimal Node Size = 30, with a Tree Depth of 15, and Cost Complexity set to a very low value of 1e-10. - For Minimal Node Size = 11, the model achieves 0.91 ROC AUC when paired with a Tree Depth = 8. When Cost Complexity is too high (e.g., 1e-02), the model is pruned excessively, losing its capacity to effectively separate classes.\nConfirming the optimal hyperparameters determined from the grid search are: Tree Depth: 8 Min Node Size: 11 Cost Complexity: 1e-10\nSelecting the best hyperparameters based on accuracy\n\n\nCode\n# best hyperparameters based on accuracy\nbest_params &lt;- select_best(tuned_results, metric = \"accuracy\")\n\n\nbest_params\n\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1    0.0000000001          8    11 Preprocessor1_Model008\n\n\n\nAs observed before the configuration that performed the best during cross-validation is: Tree Depth: 8 Min Node Size: 11 Cost Complexity: 1e-10\n\nThis configuration achieved the highest average accuracy of 0.91, with strong ROC AUC and low standard errors, indicating reliable and consistent performance across cross-validation folds.\n\n\nc. \nFit the final model using the best hyperparameters\n\n\nCode\n# final model with optimal parameters\nfinal_tree_spec &lt;- decision_tree(\n  tree_depth = 8,\n  min_n = 11,\n  cost_complexity = 1e-10\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"rpart\")\n\nfinal_tree_wf &lt;- workflow() %&gt;%\n  add_model(final_tree_spec) %&gt;%\n  add_formula(type ~ .) %&gt;%\n  fit(data = train_data)\n\n\nEvaluting the model performance\nTraining\n\n\nCode\n# predictions on the training data\ntrain_predictions &lt;- predict(final_tree_wf, new_data = train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_tree_wf, new_data = train_data)) %&gt;%\n  bind_cols(train_data)\n\n# metrics - training set\ntrain_metrics &lt;- train_predictions %&gt;%\n  metrics(truth = type, estimate = .pred_class) \n\n\ntrain_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.948\n2 kap      binary         0.895\n\n\nThe tuned decision tree achieved 95% accuracy and a Kappa of 0.90 on the training data. This shows the model correctly classified most observations and that its predictions strongly agree with the true labels, beyond chance. Unlike the overfit model from above, this tree avoids memorising the data, suggesting a good balance between flexibility and generalisation.\nTesting\n\n\nCode\n# predictions on the testing data\ntest_predictions &lt;- predict(final_tree_wf, new_data = test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_tree_wf, new_data = test_data, type = \"class\")) %&gt;%\n  bind_cols(test_data)\n\n\n# metrics- testing set\ntest_metrics &lt;- test_predictions %&gt;%\n  metrics(truth = type, estimate = .pred_class) \n\n\ntest_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.818\n2 kap      binary         0.637\n\n\n\nOn the test set, the model reached 82% accuracy with a Kappa of 0.64. This is a noticeable drop from the training accuracy of 95% and Kappa of 0.90. The decrease suggests the model doesn’t overfit, but also shows it picked up on some training specific patterns that didn’t carry over as well.\nDespite that, the test results are good and show the model generalises fairly well.\n\nConfusion Matrixes\nConfusion Matrix for Training Data\n\n\nCode\n# Confusion Matrix for Training Data\ntrain_conf_matrix &lt;- train_predictions %&gt;%\n  conf_mat(truth = type, estimate = .pred_class)\n\n\ntrain_conf_matrix\n\n\n           Truth\nPrediction  birdsongs finance\n  birdsongs       304      22\n  finance          12     311\n\n\n\nThe model classified most training examples correctly, with only 34 misclassifications out of 649.\nIt confused some finance texts as birdsongs (22) and a few birdsongs as finance (12).\nThis shows the model learned the training patterns well without becoming overly specific, which is what we look for after tuning.\n\nConfusion Matrix for Testing Data\n\n\nCode\n# Confusion Matrix for Testing Data\ntest_conf_matrix &lt;- test_predictions %&gt;%\n  conf_mat(truth = type, estimate = .pred_class)\n\n\ntest_conf_matrix\n\n\n           Truth\nPrediction  birdsongs finance\n  birdsongs       132      33\n  finance          26     134\n\n\n\nThe model got most predictions right on the test set, correctly identifying 132 birdsongs and 134 finance texts.\nA total of 59 errors, it misclassifies 33 finance as birdsongs and 26 birdsongs as finance\nThis matches the earlier accuracy of 82%, and it’s a sign that while the model generalises fairly well, it still struggles a bit with overlap between the two classes.\n\nROC Curve for Testing Data\n\n\nCode\n# ROC Curve for Testing Data\nroc_curve_test &lt;- test_predictions %&gt;%\n  roc_curve(truth = type, .pred_birdsongs) %&gt;%\n  autoplot() +\n  ggtitle(\"ROC Curve for Optimized Decision Tree (Test Data)\")\n\n\nroc_curve_test\n\n\n\n\n\n\n\n\n\nCode\nroc_auc(test_predictions, truth = type, .pred_birdsongs)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.904\n\n\n\nThe ROC curve for the test set shows that the model separates the two classes well, with a steep rise toward the top left corner.\nThe AUC value of 0.90 confirms this, it means that, across all possible thresholds, the model ranks birdsongs higher than finance 90% of the time.\nEven though the test accuracy was lower at 82%, the high AUC shows that the model is still effective at distinguishing between the two classes.\n\nCompasion with models on test data Q2\n\nCompared to the linear models in Q2, the tuned decision tree performed slightly better on both accuracy and AUC.\nLogistic regression achieved 79% accuracy and an AUC of 0.876, while LDA reached 80% accuracy with an AUC of 0.882.\nThe decision tree outperformed both, with 82% accuracy and an AUC of 0.90, suggesting it separated the classes more effectively.\nThe tree also had the advantage of capturing non-linear patterns and didn’t rely on assumptions like multivariate normality or equal covariance, which the data didn’t fully satisfy.\nWhile the linear models were easier to interpret — especially LDA, which clearly highlighted entropy and covariate2 — they struggled more with overlapping feature space, particularly for finance series.\nGiven the stronger performance and flexibility, the tuned decision tree would be the more reliable choice in practice.\n\n\n\nWhich is the better classifier?\n\n\na. Random Forest Model\nDefining a random forest model using the with 1000 trees, and tuned two key parameters: - mtry - number of predictors to consider at each split - min_n - minimum observations per node.\nDefine the random forest model specification\n\n\nCode\n# random forest model \nrf_spec &lt;- rand_forest(\n  mode = \"classification\",\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\")\n\n\nSet up the tuning grid and cross-validation\n5-fold cross-validation using a regular grid of 25 combinations\n\n\nCode\n# 5-fold cross-validation\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = type)\n\n# Grid of hyperparameters\nrf_grid &lt;- grid_regular(\n  mtry(range = c(1, 5)),\n  min_n(range = c(2, 40)),\n  levels = 5\n)\n\n\nTune the random forest model\n\n\nCode\n# Workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_formula(type ~ .)\n\n# Tune the model\nset.seed(123)\nrf_tune_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\n\nExtract the best hyperparameters based on accuracy\n\n\nCode\n# Review tuning metrics\ncollect_metrics(rf_tune_results) %&gt;%\n  filter(.metric == \"accuracy\") %&gt;%\n  arrange(desc(mean))\n\n\n# A tibble: 25 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     2    11 accuracy binary     0.909     5 0.00828 Preprocessor1_Model07\n 2     3    11 accuracy binary     0.908     5 0.00641 Preprocessor1_Model08\n 3     3    30 accuracy binary     0.908     5 0.00651 Preprocessor1_Model18\n 4     3    21 accuracy binary     0.906     5 0.00620 Preprocessor1_Model13\n 5     3    40 accuracy binary     0.904     5 0.00583 Preprocessor1_Model23\n 6     4    21 accuracy binary     0.903     5 0.00515 Preprocessor1_Model14\n 7     4    11 accuracy binary     0.903     5 0.00711 Preprocessor1_Model09\n 8     2     2 accuracy binary     0.903     5 0.00671 Preprocessor1_Model02\n 9     5    21 accuracy binary     0.901     5 0.00738 Preprocessor1_Model15\n10     4    40 accuracy binary     0.901     5 0.00622 Preprocessor1_Model24\n# ℹ 15 more rows\n\n\nCode\n# Best hyperparameters\nbest_rf_params &lt;- select_best(rf_tune_results, metric = \"accuracy\")\nbest_rf_params\n\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     2    11 Preprocessor1_Model07\n\n\n\nAfter tuning 25 combinations of mtry and min_n, the best-performing setup was mtry = 2 and min_n = 11, with an average accuracy of 0.909.\nThe low standard error of 0.008, also suggests the model performed consistently across the 5 cross-validation folds.\nThis setup performed well across folds, likely because it limited overfitting while still capturing useful patterns.\n\nFinalize the model with the best hyperparameters and fit it to the training data\n\n\nCode\n# Final model spec with best params\nfinal_rf_spec &lt;- finalize_model(rf_spec, best_rf_params)\n\n# Final workflow\nfinal_rf_workflow &lt;- rf_workflow %&gt;%\n  update_model(final_rf_spec)\n\n# Fit the model\nfinal_rf_fit &lt;- fit(final_rf_workflow, data = train_data)\n\n\nEvaluate the model on training data\nTraining predictions and metrics\n\n\nCode\n# Training predictions\ntrain_rf_preds &lt;- predict(final_rf_fit, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_rf_fit, train_data)) %&gt;%\n  bind_cols(train_data)\n\ntrain_rf_metrics &lt;- train_rf_preds %&gt;%\n  metrics(truth = type, estimate = .pred_class)\n\ntrain_rf_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.974\n2 kap      binary         0.948\n\n\n\nThe model achieved 97.5% accuracy and a Kappa of 0.95, indicating it classified most training observations correctly with very few errors.\nThe high Kappa shows that the predictions closely matched the true labels, not just by chance. While strong performance is expected, it also raises the possibility of slight overfitting.\n\nTesting predictions and metrics\n\n\nCode\n# Testing predictions\ntest_rf_preds &lt;- predict(final_rf_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_rf_fit, test_data)) %&gt;%\n  bind_cols(test_data)\n\ntest_rf_metrics &lt;- test_rf_preds %&gt;%\n  metrics(truth = type, estimate = .pred_class)\n\ntest_rf_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.868\n2 kap      binary         0.736\n\n\nThe model achieved 86.5% accuracy and a Kappa of 0.73 on the test set. The ~11% drop from training accuracy is reasonable and indicates that the model didn’t overfit. It was able to learn meaningful patterns from the training data that carry over to new data. The Kappa score still shows strong agreement with the true labels, suggesting reliable generalisation.\n\n\nb.\nBoosted tree model\nA boosted tree model defined using the xgboost with 1000 boosting rounds. The key hyperparameters tuned were tree_depth, which controls model complexity, and learn_rate, which determines how quickly the model adapts to errors.\n\n\nCode\n# Model spec\nboost_spec &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  learn_rate = tune(),\n  loss_reduction = 0\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n\nSet up the tuning grid and 5 fold cross-validation\nA regular grid of 25 combinations created by varying tree_depth and learn_rate across 5 levels each.\n\n\nCode\n# Define tuning grid\nboost_grid &lt;- grid_regular(\n  tree_depth(range = c(1, 10)),\n  learn_rate(range = c(0.001, 0.3)),\n  levels = 5\n)\n\n\n\n\nTune the boosted tree model\n\n\nCode\n# Workflow\nboost_wf &lt;- workflow() %&gt;%\n  add_model(boost_spec) %&gt;%\n  add_formula(type ~ .)\n\n# Grid search tuning\nset.seed(123)\nboost_tune_results &lt;- tune_grid(\n  boost_wf,\n  resamples = cv_folds,\n  grid = boost_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n\n\n\n\nCode\n# Review results and select best\nboost_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"accuracy\") %&gt;%\n  arrange(desc(mean))\n\n\n# A tibble: 25 × 8\n   tree_depth learn_rate .metric  .estimator  mean     n std_err .config        \n        &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;          \n 1         10       1.41 accuracy binary     0.903     5 0.0125  Preprocessor1_…\n 2          3       1.19 accuracy binary     0.903     5 0.00723 Preprocessor1_…\n 3         10       1.00 accuracy binary     0.901     5 0.00463 Preprocessor1_…\n 4          7       1.41 accuracy binary     0.901     5 0.00762 Preprocessor1_…\n 5          5       1.19 accuracy binary     0.900     5 0.0129  Preprocessor1_…\n 6          3       1.00 accuracy binary     0.898     5 0.00793 Preprocessor1_…\n 7          7       1.68 accuracy binary     0.897     5 0.0105  Preprocessor1_…\n 8          5       1.41 accuracy binary     0.897     5 0.00577 Preprocessor1_…\n 9          7       1.00 accuracy binary     0.897     5 0.00794 Preprocessor1_…\n10          7       1.19 accuracy binary     0.897     5 0.00632 Preprocessor1_…\n# ℹ 15 more rows\n\n\nCode\nbest_boost_params &lt;- select_best(boost_tune_results, metric = \"accuracy\")\nbest_boost_params\n\n\n# A tibble: 1 × 3\n  tree_depth learn_rate .config              \n       &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                \n1         10       1.41 Preprocessor1_Model15\n\n\n\nThe best configuration was tree_depth = 10 and learn_rate = 1.41, with an average accuracy of 0.903.\nThe deeper trees helped capture complex patterns, while the high learning rate allowed faster corrections, improving training speed.\nAlthough this increases the risk of overfitting, the model showed stable performance across folds with std. error = 0.0125 , suggesting good generalisation during cross-validation.\n\nFinalize and fit the boosted tree model\n\n\nCode\n# Final model and workflow\nfinal_boost_spec &lt;- finalize_model(boost_spec, best_boost_params)\n\nfinal_boost_wf &lt;- boost_wf %&gt;%\n  update_model(final_boost_spec)\n\nfinal_boost_fit &lt;- fit(final_boost_wf, data = train_data)\n\n\nEvaluate the boosted tree model on training data\n\n\nCode\n# Training performance\ntrain_boost_preds &lt;- predict(final_boost_fit, train_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_boost_fit, train_data)) %&gt;%\n  bind_cols(train_data)\n\ntrain_boost_metrics &lt;- train_boost_preds %&gt;%\n  metrics(truth = type, estimate = .pred_class)\n\ntrain_boost_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary             1\n2 kap      binary             1\n\n\n\nThe boosted tree model achieved 100% accuracy and a Kappa of 1.00 on the training set. — This confirmins it fully learned the data, it made no classification errors and its predictions perfectly matched the true labels, which is often a sign of overfitting. It likely captured both true patterns and noise.\n\nConfusion Matrix for Training Data\n\n\nCode\n# Confusion Matrix - Training\ntrain_boost_conf &lt;- train_boost_preds %&gt;%\n  conf_mat(truth = type, estimate = .pred_class)\n\ntrain_boost_conf\n\n\n           Truth\nPrediction  birdsongs finance\n  birdsongs       316       0\n  finance           0     333\n\n\nThe confusion matrix shows perfect classification on the training set, all 316 birdsongs and 333 finance examples were predicted correctly. This matches the accuracy and Kappa of 1, and clearly suggests the model has overfitted to the training data.\nEvaluate the boosted tree model on testing data\n\n\nCode\ntest_boost_preds &lt;- predict(final_boost_fit, test_data, type = \"prob\") %&gt;%\n  bind_cols(predict(final_boost_fit, test_data)) %&gt;%\n  bind_cols(test_data)\n\ntest_boost_metrics &lt;- test_boost_preds %&gt;%\n  metrics(truth = type, estimate = .pred_class)\n\ntest_boost_metrics\n\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.843\n2 kap      binary         0.686\n\n\n\nOn the test set, the boosted tree achieved an accuracy of 84.3% and a Kappa of 0.69. - While this is a noticeable drop from the perfect training results, it’s still a good outcome. The gap between training and testing confirms that the model overfit to the training data to some extent.\nHowever, the test accuracy and Kappa still reflect good predictive performance and reasonable generalisation.\n\nConfusion Matrix for Testing Data\n\n\nCode\n# Confusion Matrix - Testing\ntest_boost_conf &lt;- test_boost_preds %&gt;%\n  conf_mat(truth = type, estimate = .pred_class)\n\ntest_boost_conf\n\n\n           Truth\nPrediction  birdsongs finance\n  birdsongs       135      28\n  finance          23     139\n\n\n\nThe confusion matrix shows the model correctly classified 135 birdsongs and 139 finance cases, with 28 finance misclassified as birdsongs and 23 birdsongs misclassified as finance.\nThis aligns with the test accuracy of 84.3% and Kappa of 0.69, indicating the model generalises reasonably well despite some confusion between the two classes.\n\n\n\nc\nROC Comparison\n\n\nCode\n# Logistic Regression\nroc_log &lt;- roc_curve(logistic_test_prob, truth = type, .pred_birdsongs) %&gt;%\n  mutate(model = \"Logistic\")\n\n# LDA\nroc_lda &lt;- roc_curve(lda_test_prob, truth = type, .pred_birdsongs) %&gt;%\n  mutate(model = \"LDA\")\n\n# Decision Tree\nroc_tree &lt;- roc_curve(test_predictions, truth = type, .pred_birdsongs) %&gt;%\n  mutate(model = \"Decision Tree\")\n\n# Random Forest \nroc_rf &lt;- roc_curve(test_rf_preds, truth = type, .pred_birdsongs) %&gt;%\n  mutate(model = \"Random Forest\")\n\n# Boosted Tree \nroc_boost &lt;- roc_curve(test_boost_preds, truth = type, .pred_birdsongs) %&gt;%\n  mutate(model = \"Boosted Tree\")\n\n\nroc_all &lt;- bind_rows(roc_log, roc_lda, roc_tree, roc_rf, roc_boost)\n\n#AUC \nauc_scores &lt;- bind_rows(\n  roc_auc(logistic_test_prob, truth = type, .pred_birdsongs) %&gt;% mutate(model = \"Logistic\"),\n  roc_auc(lda_test_prob, truth = type, .pred_birdsongs) %&gt;% mutate(model = \"LDA\"),\n  roc_auc(test_predictions, truth = type, .pred_birdsongs) %&gt;% mutate(model = \"Decision Tree\"),\n  roc_auc(test_rf_preds, truth = type, .pred_birdsongs) %&gt;% mutate(model = \"Random Forest\"),\n  roc_auc(test_boost_preds, truth = type, .pred_birdsongs) %&gt;% mutate(model = \"Boosted Tree\")\n)\n\n\nauc_labels &lt;- auc_scores %&gt;%\n  mutate(label = paste0(model, \" (AUC = \", round(.estimate, 3), \")\"))\n\nmodel_order &lt;- auc_labels$model\ncolor_map &lt;- c(\n  \"Logistic\" = \"#1f77b4\",\n  \"LDA\" = \"#17becf\",\n  \"Decision Tree\" = \"#bcbd22\",\n  \"Random Forest\" = \"#e377c2\",\n  \"Boosted Tree\" = \"#ff7f0e\"\n)\nnames(color_map) &lt;- model_order\nlabel_map &lt;- auc_labels$label\nnames(label_map) &lt;- model_order\n\n# Plot ROC curves \nggplot(roc_all, aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(linewidth = 0.6) +\n  geom_abline(linetype = \"dashed\", color = \"grey\") +\n  scale_color_manual(\n    values = color_map,\n    breaks = model_order,\n    labels = label_map\n  ) +\n  labs(\n    title = \"ROC Curves for All Models\",\n    x = \"1 - Specificity (False Positive Rate)\",\n    y = \"Sensitivity (True Positive Rate)\",\n    color = \"Model\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ROC curves for all five models highlight how well each one distinguishes between birdsongs and finance time series.\n\nLogistic Regression and LDA have similar curves, with LDA performing slightly better AUC = 0.882 , as comapred to AUC = 0.876 of logistic. This matches earlier oberveations , where LDA handled class separation a bit more effectively.\nThe Tuned Decision Tree has a noticeably steeper curve and a higher AUC of 0.904, showing that tuning helped it better capture decision boundaries.\nBoosted Tree performs strongly as well, with an AUC of 0.918. Despite overfitting the training data, its test ROC performance indicates it generalised well overall.\nRandom Forest has the highest curve overall, with an AUC of 0.942. It ranked the classes most effectively and remained stable on the test set, confirming its strong overall performance.\n\nIn conclusion, the ROC curve shows that Random Forest did the best job at separating the classes, followed closely by Boosted Tree and the Tuned Decision Tree. The linear models performed reasonably well but struggled more with the class overlap in the data. This conclusion is consistent with the earlier observations when fitiing and summarizing the models. Random Forest and Boosted Tree are the most reliable classifiers for this dataset, effectively capturing the complex patterns in the time series data.\n\n\nd. \nBest Model and how the time series for financial data and birdsongs typically differ\nAfter comparing all five models, Random Forest stands out as the most effective classifier, with the highest ROC AUC of 0.942. It consistently generalised well on the test data, outperforming both linear and non-linear alternatives without showing signs of overfitting. This aligns with the model’s strength in handling feature interactions and non-linearities, which are apparent in this dataset. In contrast, Boosted Tree, while also strong (AUC = 0.918), slightly overfit the training data, and Tuned Decision Tree (AUC = 0.904) offered a good balance but didn’t quite match Random Forest’s stability. From the analysis, we can observe that financial time series is more structured, with higher values in linearity and covariate2, while birdsong series is more random and irregular, showing higher entropy and weaker autocorrelation. These differences are also reflected druing the individual model analysis, where linear models like Logistic Regression and LDA struggled more due to class overlap and violated assumptions, whereas tree-based models adapted better to the complexity of the data."
  },
  {
    "objectID": "posts/ml2/index.html#references",
    "href": "posts/ml2/index.html#references",
    "title": "Principle Component Analysis and Classifiers",
    "section": "References",
    "text": "References\nHadley Wickham, Dianne Cook, Heike Hofmann, Andreas Buja (2011). tourr: An R Package for Exploring Multivariate Data with Projections. Journal of Statistical Software, 40(2), 1-18. URL http://www.jstatsoft.org/v40/i02/.\nKuhn et al., (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. https://www.tidymodels.org"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Pooja Rajendran Raju\nI am currently pursuing a Master’s degree in Business Analytics at Monash University, where I am deepening my expertise in data analysis and statistical modeling. With a background in Android development, I previously worked at Zebra Technologies, developing data extraction applications that empowered enterprises to streamline their operations through robust mobile solutions.\nMy passion lies in exploring mental health data due to its complex and unpredictable nature, which presents unique challenges and opportunities for discovery. I am particularly interested in leveraging advanced analytics and machine learning to uncover patterns that can enhance our understanding of mental health trends and interventions.\nIn addition to my technical pursuits, I am committed to using data-driven insights to address real-world problems, especially those with social impact. I am continually honing my skills in areas like predictive modeling, data visualization, and data storytelling, aiming to bridge the gap between data science and meaningful change."
  }
]