{
  "hash": "20ab58091315e5a09511466ac2f30a34",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principle Component Analysis and Classifiers\"\nauthor: \"Pooja Rajendran Raju\"\ndate: \"2025-05-04\"\nquarto-required: \">=1.3.0\"\noutput:\n    html:\n        embed-resources: true\ncategories: [Machine Learning]\n---\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nThis project explores two areas of statistical learning: understanding structure in multivariate data using resampling, and comparing modelling approaches for a supervised classification task. In the first part, the focus is on the women’s cricket dataset, where principal component analysis (PCA) is used to identify the main sources of variation after removing non-numeric variables. Bootstrap resampling and permutation tests are then used to assess which variables contribute most strongly to the leading components and whether observed relationships between variables are stronger than what would be expected by chance.\n\nThe second part shifts to a classification problem involving time-series features from birdsongs and financial data. Here, several classifiers: logistic regression, linear discriminant analysis, decision trees, random forests, and boosted trees are fitted and evaluated. The analysis examines model performance, tuning behaviour, and how well each approach captures the patterns that differentiate the two classes.\n\n#### Bootstrapping and permuting your way to provide evidence \n\nThis section applies PCA to the women’s cricket data (after removing irrelevant variables) to identify the main directions of variation in player performance. Bootstrap resampling is then used to determine which variables consistently contribute to PC1 and PC2, providing stability insight beyond the original PCA loadings.\n\n**Data Wrangling**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load the data\n\nengwt20 <- read.csv(\"engwt20.csv\")\n```\n:::\n\n\n**Checking Missing Values**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Check for missing values\nany(is.na(engwt20))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n**Data Structure**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n#Analysing the structure of the data\n#str(engwt20)\n#summary(engwt20)\n```\n:::\n\n\n**Variable Selection**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n#Variable Selection\ndata_criket <- engwt20 |> \n  select(-Player, -Country, -Start, -End, -HighScoreNotOut, -Matches, -InningsBowled, -Overs, -InningsBatted, -FiveWickets, -Hundreds)\n\nnames(data_criket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Maidens\"           \"RunsConceeded\"     \"Wickets\"          \n [4] \"BowlingAverage\"    \"Economy\"           \"BowlingStrikeRate\"\n [7] \"FourWickets\"       \"NotOuts\"           \"RunsScored\"       \n[10] \"HighScore\"         \"BattingAverage\"    \"Fifties\"          \n[13] \"Ducks\"            \n```\n\n\n:::\n:::\n\n\n**PCA**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Applying PCA \npca_res <- prcomp(data_criket, scale = TRUE, center = TRUE)\nsummary(pca_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4    PC5     PC6    PC7\nStandard deviation     2.2570 1.8905 1.4020 0.78804 0.7398 0.65106 0.5757\nProportion of Variance 0.3919 0.2749 0.1512 0.04777 0.0421 0.03261 0.0255\nCumulative Proportion  0.3919 0.6668 0.8180 0.86576 0.9079 0.94046 0.9660\n                           PC8     PC9    PC10    PC11    PC12    PC13\nStandard deviation     0.46836 0.31245 0.25622 0.19035 0.11782 0.09903\nProportion of Variance 0.01687 0.00751 0.00505 0.00279 0.00107 0.00075\nCumulative Proportion  0.98283 0.99034 0.99539 0.99818 0.99925 1.00000\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npca_res$rotation[, 1:2]  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                           PC1         PC2\nMaidens           -0.220802813  0.31424912\nRunsConceeded     -0.294161264  0.35568617\nWickets           -0.286828045  0.35361350\nBowlingAverage     0.001003561  0.33803083\nEconomy            0.026102389  0.35620360\nBowlingStrikeRate -0.017606471  0.33259501\nFourWickets       -0.226182850  0.27976834\nNotOuts           -0.396975004  0.02335211\nRunsScored        -0.363915135 -0.20632957\nHighScore         -0.351615787 -0.25637332\nBattingAverage    -0.319672992 -0.22681130\nFifties           -0.315194665 -0.21558036\nDucks             -0.339261537 -0.11355162\n```\n\n\n:::\n:::\n\n\nPC1 explains 39.2% of the total variance and reflects overall player performance. It has strong negative loadings from batting stats like NotOuts, RunsScored, HighScore, and BattingAverage, alongside bowling variables such as Wickets, RunsConceeded, and Maidens. Since both batting and bowling contribute in the same direction, lower PC1 scores indicate players who perform well in both areas.\n\nPC2 accounts for 27.5% of the variance and highlights a contrast between bowling and batting performance. Bowling variables like Economy, BowlingAverage, BowlingStrikeRate, and Wickets have strong positive loadings, while batting stats such as RunsScored, HighScore, and BattingAverage load negatively. Higher PC2 scores therefore indicate players who are stronger bowlers relative to their batting ability\n\n**Scree Plot**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Extract eigenvalues\neigenvalues <- pca_res$sdev^2\n# Create data frame for ggplot\n\nscree_df <- tibble(\n  PC = 1:length(eigenvalues),\n  Eigenvalue = eigenvalues\n)\n\n# Plot using ggplot2\nggplot(scree_df, aes(x = PC, y = Eigenvalue)) +\n  geom_line(color = \"black\", linewidth = 1) +\n  geom_point(color = \"black\", size = 2) +\n  scale_x_continuous(breaks = seq(1, max(scree_df$PC), by = 2)) +\n  labs(title = \"Scree Plot\", x = \"Number of PCs\", y = \"Eigenvalue\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe elbow in the scree plot appears at the 4th component, with the first three PCs explaining over 81% of the total variance (PC1: 39.2%, PC2: 27.5%, PC3: 15.1%). Since the eigenvalues flatten out after that, adding more PCs wouldn’t capture much additional information and variance. Moving forward we will focus only on the first two components.\n\n\n\n\n**Biplot**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create data frame with PCA scores (PC1 and PC2) and player names\neng_pca_scores <- as_tibble(pca_res$x[, 1:2]) |>\n  mutate(Player = engwt20$Player)\n\n# Create data frame with PCA loadings (eigenvectors)\neng_pca_loadings <- as_tibble(pca_res$rotation[, 1:2]) |>\n  mutate(\n    origin = 0,\n    variable = colnames(data_criket),\n    PC1s = PC1 * (pca_res$sdev[1]^2 * 2.5),\n    PC2s = PC2 * (pca_res$sdev[2]^2 * 2.5)\n  )\n\n# Create biplot\nggplot() + \n  geom_segment(data = eng_pca_loadings, \n               aes(x = origin, xend = PC1s, \n                   y = origin, yend = PC2s), \n               colour = \"orange\") +\n  geom_text(data = eng_pca_loadings, \n            aes(x = PC1s, y = PC2s, label = variable),\n            colour = \"orange\", nudge_x = 0.7) +\n  geom_point(data = eng_pca_scores, \n             aes(x = PC1, y = PC2)) +\n  geom_text(data = filter(eng_pca_scores, abs(PC2) > 1.3),\n            aes(x = PC1, y = PC2, label = Player), \n            nudge_y = 0.15, nudge_x = -0.5, size = 3) +\n  xlab(\"PC1\") + ylab(\"PC2\") +\n  ggtitle(\"Biplot of First Two Principal Components (engwt20)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- Variables like Economy, BowlingAverage, and BowlingStrikeRate point strongly upward, indicating they contribute most to PC2 and represent bowling efficiency. Meanwhile, variables such as RunsScored, HighScore, BattingAverage, and Fifties point sharply to the left, confirming they primarily influence PC1 and reflect batting performance. \n- The long arrow lengths suggest that these variables are well captured in the two-dimensional PC space and play a key role in defining the player profiles seen in the plot.\n- Players positioned far to the left, like DN Wyatt, tend to have strong batting performances across multiple metrics. Those higher on the plot, such as S Ecclestone and K Sciver-Brunt, are likely more efficient bowlers, as they align closely with bowling related variables on PC2.\n\n**Bootstrapping PCA Loadings**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Bootstrap function for PC1\ncompute_PC1 <- function(data, index) {\n  pc1 <- prcomp(data[index, ], center = TRUE, scale. = TRUE)$rotation[, 1]\n  if (sign(pc1[1]) < 0) pc1 <- -pc1\n  return(pc1)\n}\n\nset.seed(2025)\nPC1_boot <- boot(data = scale(data_criket), statistic = compute_PC1, R = 1000)\ncolnames(PC1_boot$t) <- colnames(data_criket)\n\n# Summarise PC1 bootstrap \nPC1_boot_ci <- as_tibble(PC1_boot$t) |>\n  pivot_longer(everything(), names_to = \"var\", values_to = \"coef\") |>\n  group_by(var) |>\n  summarise(\n    q2.5 = quantile(coef, 0.025),\n    median = median(coef),\n    q97.5 = quantile(coef, 0.975)\n  ) |>\n  mutate(t0 = PC1_boot$t0)\n\nPC1_boot_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 5\n   var                  q2.5   median q97.5       t0\n   <chr>               <dbl>    <dbl> <dbl>    <dbl>\n 1 BattingAverage    -0.362   0.315   0.386  0.221  \n 2 BowlingAverage    -0.207   0.00641 0.230  0.294  \n 3 BowlingStrikeRate -0.183   0.0251  0.222  0.287  \n 4 Ducks             -0.316   0.333   0.384 -0.00100\n 5 Economy           -0.244  -0.0209  0.268 -0.0261 \n 6 Fifties           -0.351   0.314   0.372  0.0176 \n 7 FourWickets        0.0100  0.224   0.314  0.226  \n 8 HighScore         -0.404   0.343   0.420  0.397  \n 9 Maidens            0.0327  0.213   0.294  0.364  \n10 NotOuts           -0.311   0.389   0.421  0.352  \n11 RunsConceeded      0.0241  0.294   0.357  0.320  \n12 RunsScored        -0.396   0.358   0.413  0.315  \n13 Wickets            0.0227  0.288   0.344  0.339  \n```\n\n\n:::\n:::\n\n\n**95% Bootstrap Confidence Intervals for PC1 Loadings**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npb_pc1 <- ggplot(PC1_boot_ci, aes(x = var, y = t0)) + \n  geom_hline(yintercept = 0, linetype = 2, colour = \"red\") +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = q2.5, ymax = q97.5), width = 0.1) +\n  xlab(\"\") + ylab(\"Loading\") +\n  ggtitle(\"PC1 Loadings with 95% Bootstrap CI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \npb_pc1\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n**Bootstrap Analysis of PC2 Loadings**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Bootstrap function for PC2\ncompute_PC2 <- function(data, index) {\n  pc2 <- prcomp(data[index, ], center = TRUE, scale. = TRUE)$rotation[, 2]\n  if (sign(pc2[1]) < 0) pc2 <- -pc2\n  return(pc2)\n}\n\nset.seed(2025)\nPC2_boot <- boot(data = scale(data_criket), statistic = compute_PC2, R = 1000)\ncolnames(PC2_boot$t) <- colnames(data_criket)\n\n# Summarise PC2 bootstrap results\nPC2_boot_ci <- as_tibble(PC2_boot$t) |>\n  pivot_longer(everything(), names_to = \"var\", values_to = \"coef\") |>\n  group_by(var) |>\n  summarise(\n    q2.5 = quantile(coef, 0.025),\n    median = median(coef),\n    q97.5 = quantile(coef, 0.975)\n  ) |>\n  mutate(t0 = PC2_boot$t0)\n\nPC2_boot_ci\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 5\n   var                 q2.5  median  q97.5      t0\n   <chr>              <dbl>   <dbl>  <dbl>   <dbl>\n 1 BattingAverage    -0.332 -0.221  0.0629  0.314 \n 2 BowlingAverage     0.120  0.320  0.445   0.356 \n 3 BowlingStrikeRate  0.126  0.317  0.431   0.354 \n 4 Ducks             -0.228 -0.104  0.157   0.338 \n 5 Economy            0.163  0.344  0.427   0.356 \n 6 Fifties           -0.341 -0.205  0.0176  0.333 \n 7 FourWickets        0.155  0.283  0.406   0.280 \n 8 HighScore         -0.351 -0.252  0.0382  0.0234\n 9 Maidens            0.217  0.320  0.427  -0.206 \n10 NotOuts           -0.107  0.0246 0.291  -0.256 \n11 RunsConceeded      0.221  0.353  0.486  -0.227 \n12 RunsScored        -0.314 -0.199  0.0634 -0.216 \n13 Wickets            0.225  0.353  0.483  -0.114 \n```\n\n\n:::\n:::\n\n\n\n**95% Bootstrap Confidence Intervals for PC2 Loadings**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npb_pc2 <- ggplot(PC2_boot_ci, aes(x = var, y = t0)) + \n  geom_hline(yintercept = 0, linetype = 2, colour = \"red\") +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = q2.5, ymax = q97.5), width = 0.1) +\n  xlab(\"\") + ylab(\"Loading\") +\n  ggtitle(\"PC2 Loadings with 95% Bootstrap CI\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\npb_pc2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nBased on the analysis, PC1 reflects overall activity, with large contributions from both batting (NotOuts, RunsScored, HighScore) and bowling (Wickets, RunsConceded), as all loadings share the same direction. However, when analysing the bootstrap 95% confidence intervals, only FourWickets, Maidens, RunsConceded, and Wickets show intervals above zero, which indicates these four bowling metrics are the most stable and significantly associated with PC1. In contrast, key batting variables like RunsScored, BattingAverage, and HighScore cross zero, suggesting their influence on PC1 is weaker and not consistently reflected across resamples.\n\nWhereas in PC2, The original PCA loadings show a clear contrast between bowling and batting as in bowling metrics have positive loadings, while batting metrics are negative. This separation suggests that PC2 distinguishes bowlers from batters. The bootstrap confidence intervals support this finding, intervals for bowling-related variables such as Economy, Wickets, BowlingAverage, BowlingStrikeRate, RunsConceeded and Maidens remain fully above zero, indicating a consistent and significant contribution to PC2. In contrast, intervals for batting metrics like RunsScored, HighScore, BattingAverage, and Fifties are negative or cross zero, suggesting their influence on PC2 is weaker and  contribution is less stable.\n\n#### Observing Correlations Through Permutation Tests and Lineups\n\nHere we investigate whether observed correlations in the cricket data are stronger than what would occur under randomness. Using permutation tests, null distributions, and lineup plots, we assess the strength, visual detectability, and statistical significance of relationships such as RunsScored–HighScore.\n\n**RunsScored vs HighScore** \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(201)\n# Original correlation\noriginal_corr <- data_criket |>\n  summarise(correlation = cor(RunsScored, HighScore)) |>\n  pull(correlation)\n\n# Single permutation\ndf_perm <- data_criket |>\n  specify(RunsScored ~ HighScore) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1, type = \"permute\")\n\ndata_with_perm <- data_criket |>\n  mutate(Permuted_RunsScored = df_perm$RunsScored)\n\n# Plot the Original Data\np1 <- ggplot(data_criket, aes(x = RunsScored, y = HighScore)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  labs(\n    title = \"Original Data\",\n    subtitle = paste0(\"Original Correlation: \", round(original_corr, 3)),\n    x = \"RunsScored\",\n    y = \"HighScore\"\n  ) +\n  theme_minimal()\n\n# Permuted plot\np2 <- ggplot(data_with_perm, aes(x = Permuted_RunsScored, y = HighScore)) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  labs(\n    title = \"Permuted Data\",\n    subtitle = paste0(\"Permuted Correlation: \", round(cor(data_with_perm$Permuted_RunsScored, data_criket$HighScore), 3)),\n    x = \"Permuted RunsScored\",\n    y = \"HighScore\"\n  ) +\n  theme_minimal()\n\n# Combine with patchwork\np1 | p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- The Original Data plot shows a strong positive linear relationship between RunsScored and HighScore with a correlation of 0.851, indicating that as runs scored increase, high scores also tend to rise proportionally. The points are tightly clustered along an upward trend, reflecting this strong association.\n- In contrast, when the data is permuted, this relationship no longer holds, as shown by the randomly scattered points. The correlation value drops significantly to 0.027, showing random scattering of points with no clear trend.\n- The comparison between the original and permuted plots illustrates that the correlation observed in the original data is unlikely to be from random chance, otherwise the permuted data would show a similar pattern, but it clearly does not. Hence, we can infer that the relationship between RunsScored and HighScore is statistically significant and meaningful.\n\n**Permutation Distribution**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Set seed\nset.seed(123)\n\n# Permutation distribution\nperm_cor <- data_criket |>\n  specify(RunsScored ~ HighScore) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 5000, type = \"permute\") |>\n  calculate(stat = \"correlation\")\n\n# Plot permutation \nggplot(perm_cor, aes(x = stat)) +\n  geom_histogram(bins = 40, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = original_corr, color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = original_corr, y = 400, \n           label = sprintf(\"Observed correlation: %.3f\", original_corr),\n           color = \"red\", hjust = 1.1, vjust = 1) +\n  scale_x_continuous(breaks = seq(-0.5, 1, by = 0.1)) +\n  labs(\n    title = \"Permutation Test: RunsScored vs HighScore\",\n    x = \"Permuted Correlations\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# One-sided p-value\np_value_1 <- perm_cor |>\n  summarise(p = mean(stat >= original_corr)) |>\n  pull(p)\n\noriginal_corr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.850749\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\np_value_1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nObserved correlation: 0.851, indicating a very strong positive relationship. This means that players who score more total runs are likely to have higher individual top scores as well.\nNull hypothesis: There is no association between RunsScored and HighScore.\nPermutation result: After 5000 permutations under null hypothesis, none of the permuted correlations reach/exceed the observed value. The permutation distribution of correlations is approximately centered around 0, as expected if there were no true relationship.\nP-value: 0.00, indicating strong evidence against the null hypothesis.\n\n-  The observed correlation (red line) is far to the right of the permutation distribution, highlighting that it is statistically significant and not due to random variation. This strongly supports the existence of a genuine association between RunsScored and HighScore.\n\n**Line up**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(912)\ndf_l <- lineup(null_permute('HighScore'), data_criket)\n\n\n\nggplot(df_l, aes(x=HighScore, y=RunsScored)) + \n  geom_point() + \n  facet_wrap(~.sample)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\nPlot 15 clearly stands out from the lineup, shows a structured upward trend that is distinct compared to the other plots. This indicates that the observed relationship between RunsScored and HighScore is strong enough to be visually identifiable and the correlation is statistically significant. The true data plot is 15. \n\n\n**RunsScored vs RunsConceeded**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(123)\n\n# Original correlation\noriginal_corr_con <- data_criket |>\n  summarise(correlation = cor(RunsScored, RunsConceeded)) |>\n  pull(correlation)\n\noriginal_corr_con\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.20957\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Hypothesize null of independence and permute\ndf_perm_con <- data_criket |>\n  specify(RunsScored ~ RunsConceeded) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1, type = \"permute\") \n\nperm_data <- data_criket |>\n  mutate(Permuted_RunsScored = df_perm_con$RunsScored)\n\n# Original plot\np1_conceded <- ggplot(data_criket, aes(x = RunsScored, y = RunsConceeded)) +\n  geom_point(alpha = 0.5, color = \"blue\") +\n  labs(\n    title = \"Original Data\",\n    subtitle = paste0(\"Original Correlation: \", round(original_corr_con, 3)),\n    x = \"RunsScored\",\n    y = \"RunsConceded\"\n  ) +\n  theme_minimal()\n\n# Permuted plot\np2_conceeded <- ggplot(perm_data, aes(x = Permuted_RunsScored, y = RunsConceeded)) +\n  geom_point(alpha = 0.5, color = \"red\") +\n  labs(\n    title = \"Permuted Data\",\n    subtitle = paste0(\"Permuted Correlation: \", round(cor(perm_data$Permuted_RunsScored, perm_data$RunsConceeded), 3)),\n    x = \"Permuted RunsScored\",\n    y = \"RunsConceded\"\n  ) +\n  theme_minimal()\n\n\np1_conceded | p2_conceeded\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=100%}\n:::\n:::\n\n- The Original Data plot shows a weak positive relationship between RunsScored and RunsConceeded, with a correlation of 0.21. The points are broadly scattered, with a few large outliers. This indicates that as the number of runs scored increases, there is a slight tendency for more runs to be conceded, but the relationship is not strong.\n- In the Permuted Data plot, the correlation drops to 0.01, suggesting almost no association. However, the distribution of points still resembles the original pattern, with most points clustered near zero and a few large outliers. The fact that the structure remains similar despite permutation indicates that the original association is quite weak and may be influenced more by the distribution of the data.\nTo confirm if the observed correlation of 0.21 is statistically significant and not just a product of the underlying distribution, a 5000 permutation test and evaluation of p-value is carried out.\n\n**Permutation Distribution**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(201)\n\n# Step 2: Perform 5000 permutations\nperm_cor_con <- data_criket |>\n  specify(RunsScored ~ RunsConceeded) |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 5000, type = \"permute\") |>\n  calculate(stat = \"correlation\")\n\n# Step 3: Plot the permutation distribution\nggplot(perm_cor_con, aes(x = stat)) +\n  geom_histogram(bins = 40, fill = \"lightblue\", color = \"white\") +\n  geom_vline(xintercept = original_corr_con, color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = original_corr_con, y = 400, \n           label = sprintf(\"Observed correlation: %.3f\", original_corr_con),\n           color = \"red\", hjust = -0.1, vjust = -0.1) +\n  scale_x_continuous(breaks = seq(-0.5, 0.5, by = 0.1)) +\n  labs(\n    title = \"Permutation Test: RunsScored vs RunsConceeded\",\n    x = \"Permuted Correlations\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Step 4: Calculate the one-sided p-value\np_value_con <- perm_cor_con |>\n  summarise(p = mean(stat >= original_corr_con)) |>\n  pull(p)\n\n# Display results\noriginal_corr_con\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.20957\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\np_value_con\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0674\n```\n\n\n:::\n:::\n\n\nObserved correlation: 0.210 -  a weak positive relationship.\nNull hypothesis: There is no association between RunsScored and RunsConceeded.\nPermutation result: 6.7% of permuted correlations were as extreme or more than the observed value.\nP-value: 0.0674\nInterpretation:\nThe observed correlation is weak and not statistically significant, as the p-value exceeds the 5% threshold. The similar scatter patterns in the permuted plot suggest the association may be influenced more by the distribution of RunsConceeded rather than a true underlying relationship.\nConclusion:\nThere is weak evidence of a positive association, but it is not strong enough to confidently rule out random chance as a possible explanation.\n\n**Line up**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(912)\ndf_l <- lineup(null_permute('RunsConceeded'), data_criket)\n\n\n\nggplot(df_l, aes(x=RunsConceeded, y=RunsScored)) + \n  geom_point() + \n  facet_wrap(~.sample)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nIn the lineup plot for RunsScored versus RunsConceded, the true data plot is positioned at 15. Unlike the clear distinction observed with RunsScored and HighScore, the data plot is not visually distinctive from the other plots. This lack of clear separation suggests that the association between RunsScored and RunsConceded is not strong enough to be easily detected visually, as observed above the relationship is weak and may be influenced by randomness.\n\n\n\n#### How well can we build a simple classifier? \n\nHere we examine how birdsong and financial time-series features differ through visualisations, grand tours, and plots. The goal is to understand class separation and assess assumptions such as normality, equal variances and independence required for linear classifiers.\n\n**Data Analysis**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n#load data \nt_data <- read_csv(\"finance_and_birds.csv\")\nglimpse(t_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 974\nColumns: 6\n$ linearity  <dbl> -0.126415857, 1.005038671, -0.424191444, -0.225349569, -3.3…\n$ entropy    <dbl> 0.8983480, 0.8557268, 0.9292932, 0.8072891, 0.6784242, 0.81…\n$ x_acf1     <dbl> -0.60861249, 1.23397887, 1.01013491, 0.45327656, 0.30615984…\n$ type       <chr> \"birdsongs\", \"birdsongs\", \"birdsongs\", \"birdsongs\", \"birdso…\n$ covariate1 <dbl> 2.715089, 3.263722, 3.435156, 2.387939, 2.988109, 3.074436,…\n$ covariate2 <dbl> 2.81219129, -0.21610577, 0.87087536, 0.59675859, 1.81582598…\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nsummary(t_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   linearity            entropy            x_acf1            type          \n Min.   :-46.54211   Min.   :-0.4317   Min.   :-1.2944   Length:974        \n 1st Qu.: -0.17822   1st Qu.: 0.4122   1st Qu.:-0.2514   Class :character  \n Median :  0.09786   Median : 0.7521   Median : 0.5120   Mode  :character  \n Mean   :  7.87493   Mean   : 0.6778   Mean   : 0.3519                     \n 3rd Qu.:  4.23087   3rd Qu.: 0.9514   3rd Qu.: 0.9297                     \n Max.   : 62.78745   Max.   : 1.4751   Max.   : 1.6774                     \n   covariate1      covariate2     \n Min.   :1.565   Min.   :-2.1790  \n 1st Qu.:2.651   1st Qu.: 0.9126  \n Median :2.980   Median : 1.7112  \n Mean   :3.002   Mean   : 1.7641  \n 3rd Qu.:3.337   3rd Qu.: 2.6843  \n Max.   :4.375   Max.   : 5.8076  \n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Check for missing values\nsum(is.na(t_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Summary statistics\nt_data %>%\n  summarise(across(where(is.numeric), list(mean = mean, sd = sd, min = min, max = max), na.rm = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 20\n  linearity_mean linearity_sd linearity_min linearity_max entropy_mean\n           <dbl>        <dbl>         <dbl>         <dbl>        <dbl>\n1           7.87         19.4         -46.5          62.8        0.678\n# ℹ 15 more variables: entropy_sd <dbl>, entropy_min <dbl>, entropy_max <dbl>,\n#   x_acf1_mean <dbl>, x_acf1_sd <dbl>, x_acf1_min <dbl>, x_acf1_max <dbl>,\n#   covariate1_mean <dbl>, covariate1_sd <dbl>, covariate1_min <dbl>,\n#   covariate1_max <dbl>, covariate2_mean <dbl>, covariate2_sd <dbl>,\n#   covariate2_min <dbl>, covariate2_max <dbl>\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Check the class distribution\nt_data %>% count(type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  type          n\n  <chr>     <int>\n1 birdsongs   474\n2 finance     500\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Set the levels explicitly\nt_data <- t_data |> mutate(type = factor(type, levels = c(\"birdsongs\", \"finance\")))\n\n\nlevels(t_data$type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"birdsongs\" \"finance\"  \n```\n\n\n:::\n:::\n\n\n**Boxplots: Distribution of Variables by Type**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Box plot of numerical variables grouped by class\nt_data %>%\n  pivot_longer(cols = -type, names_to = \"variable\", values_to = \"value\") %>%\n  ggplot(aes(x = type, y = value, fill = type)) +\n  geom_boxplot() +\n  facet_wrap(~variable, scales = \"free\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Variables by Type\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe boxplots reveal clear differences between birdsongs and finance time series across several features. Linearity and covariate2 display the most noticeable separations—finance time series tend to have higher values and a wider spread in both features, making them strong candidates for classification. Entropy also shows some separation, with birdsongs generally having slightly higher median values and finance exhibiting more variability. In contrast, covariate1 and x_acf1 are quite similar across both types, suggesting they may not be as useful for classification. These findings indicate that linearity, covariate2, and entropy could be the strongest predictors moving forward.\n\n\n\n**Scatterplot Matrix**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Convert the 'type' to a factor\nt_data <- t_data %>%\n  mutate(type = as.factor(type))\n\n# Improved ggpairs plot\nggpairs(t_data,\n        columns = c(1,2,3,5,6),\n        aes(color = type, alpha = 0.6),\n        lower = list(continuous = wrap(\"points\", size = 0.6)),\n        diag = list(continuous = wrap(\"densityDiag\", alpha = 0.4)),\n        upper = list(continuous = wrap(\"cor\", size = 3))) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- Linearity: Both bird songs and finance time series are centered around zero with minimal spread, showing no visible separation.\n- Entropy: Finance series have a strong negative correlation (-0.539), while birdsongs are mostly uncorrelated (0.055). Finance tends to have lower entropy, while birdsongs are skewed towards higher values.\n- x_acf1: Finance displays a positive correlation (0.493) and a wider spread, suggesting stronger autocorrelation patterns compared to birdsongs, which show no correlation (-0.002).\n- Covariate1: Minimal differences are observed; distributions are almost identical between the two types, with near-zero correlations.\n- Covariate2: Finance has a slight positive correlation (0.023) and is skewed towards higher values, while birdsongs show a slight negative correlation (-0.044).\n\nOverall, entropy and x_acf1 show the strongest separation, with finance exhibiting clear negative correlations with entropy and positive correlations with x_acf1. Covariate2 shows moderate distinction, while linearity and covariate1 display minimal separation.\n\n\n \n**Assumptions** \n\n**Tourr**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Standardize the numerical columns\ndata_standardized <- t_data %>%\n  mutate(across(where(is.numeric), scale))\n\n# Data for the tour\ntour_data <- data_standardized %>%\n  select(linearity, entropy, x_acf1, covariate1, covariate2)\n\n#x11()\n#animate_xy(tour_data, col = as.factor(data_standardized$type), axes = \"bottomleft\",\n#           rescale=TRUE)\n\n#set.seed(645)\nrender_gif(tour_data,\n           grand_tour(),\n           display_xy(half_range = 3.8,\n           axes=\"bottomleft\", cex=1.1,\n           col = as.factor(data_standardized$type)),\n           gif_file = \"tour_fb.gif\",\n           apf = 1/60,\n           frames = 1800,\n           width = 600, \n           height = 600)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"/Users/poojarajendranraju/Desktop/personal_website/git_verson/Personal-Portfolio/posts/ml2/tour_fb.gif\"\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nknitr::include_graphics(\"tour_fb.gif\")\n```\n\n::: {.cell-output-display}\n![](tour_fb.gif){fig-align='center' width=100%}\n:::\n:::\n\n\n## Grand Tour\n\nThe grand tour reveals how the five features interact across varying projections. While some projections reveal partial separation between birdsongs and financial time series, there remains a significant overlap in many views. This overlap, along with the irregular and unequal spread of each class, suggests that the assumptions of multivariate normality and equal variance-covariance matrices are not fully satisfied. The finance series tend to cluster more tightly, whereas birdsongs exhibit more variation, particularly in entropy and x_acf1. Although classification is still possible, the way the data breaks some of LDA’s assumptions suggests that more flexible models, like decision trees or ensemble methods might handle it more effectively.\n\n**1. Multivariate Normality**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# List of predictors \npredictors <- setdiff(names(t_data), 'type')\nplots <- list()\n\n# QQ plots by class\nfor (predictor in predictors) {\n  plot <- ggplot(t_data, aes(sample = .data[[predictor]], color = type)) +\n    stat_qq() +\n    stat_qq_line() +\n    labs(title = paste(\"QQ Plot for\", predictor, \"by Class\")) +\n    theme_minimal() +\n    theme(legend.position = \"bottom\")\n\n  plots[[predictor]] <- plot\n}\n\n#  QQ plots\nwrap_plots(plots, ncol = 3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe QQ plots revealed that linearity and covariate2 exhibited heavy tails and significant deviations from normality, while entropy and x_acf1 also deviated, particularly at the tails. In contrast, covariate1 was the closest to normality with its points aligning well with the theoretical line. Therefore, the multivariate normality assumption is not fully satisfied. \n\n**2. Equal Variance-Covariance Matrices**\n\nThe scatterplot matrix above reveals noticeable differences in spread and separation between birdsongs and finance, particularly in entropy, x_acf1, and covariate2. These disparities indicate that the assumption of equal variance-covariance matrices is not met, suggesting LDA may produce biased decision boundaries due to differing variance structures across classes.\n\n\n**3. Independence**\nThe dataset is ordered with 500 finance samples followed by 474 birdsongs samples. While this structure might suggest some sequencing, it doesn't automatically imply dependency. Since there are no clear time-based indicators or hierarchical groupings, we can assume the independence assumption is satisfied.\n\n\n\n#### Train-Test Split\n\nA stratified train–test split is used to preserve the class balance between birdsongs and finance data. This prepares a fair setting for training models and evaluating their out of sample performance.\n\n**Split the data into training and testing sets**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(42)  \n\n# Data Split\ndata_split <- initial_split(t_data, prop = 2/3, strata = type)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\ntrain_data |> count(type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  type          n\n  <fct>     <int>\n1 birdsongs   316\n2 finance     333\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\ntest_data |> count(type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  type          n\n  <fct>     <int>\n1 birdsongs   158\n2 finance     167\n```\n\n\n:::\n:::\n\n\n\n**Distribution of Classes in Training and Testing Sets**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Training Set Distribution\ntrain_plot <- train_data |> \n  count(type) |> \n  ggplot(aes(x = type, y = n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Training Set Distribution\", y = \"Count\", x = \"Type\") +\n  theme_minimal()\n\n# Testing Set Distribution\ntest_plot <- test_data |> \n  count(type) |> \n  ggplot(aes(x = type, y = n, fill = type)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Testing Set Distribution\", y = \"Count\", x = \"Type\") +\n  theme_minimal()\n\n\ntrain_plot + test_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe distribution looks good and well-balanced across the training and testing datasets, which means the stratified sampling worked correctly. The bars are nearly equal for both birdsongs and finance, indicating there is no class imbalance.\n\n#### Exploring Linear Classifiers\n\n**Fitting a linear classifier to the training data**\n\nLinear models are fitted to study how well simple decision boundaries separate the two time-series classes. Coefficients and discriminant directions are interpreted to understand which features drive model decisions.\n\n**Logistic model**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# logistic regression model\nlogistic_mod <- logistic_reg() |> \n  set_engine(\"glm\") |> \n  set_mode(\"classification\")\n\n# Fit the model\nlogistic_fit <- logistic_mod |> \n  fit(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, data = train_data)\n\n# Display the model coefficients\ntidy(logistic_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -0.613     0.785    -0.781  4.35e- 1\n2 linearity     0.0521    0.0100    5.18   2.19e- 7\n3 entropy      -2.64      0.477    -5.53   3.27e- 8\n4 x_acf1       -0.460     0.211    -2.18   2.90e- 2\n5 covariate1    0.0195    0.225     0.0867 9.31e- 1\n6 covariate2    1.36      0.125    10.9    1.81e-27\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nglance(logistic_fit) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          899.     648  -252.  516.  543.     504.         643   649\n```\n\n\n:::\n:::\n\n\n- Linearity (Estimate = 0.052, p < 0.001):\nSlightly boosts the chances of classifying the series as finance. This means more linear time series are more likely to be financial data.\n- Entropy (Estimate = -2.638, p < 0.001):\nHas a strong negative effect, making it much more likely for the series to be classified as birdsongs. It reflects the chaotic and unpredictable nature typical of birdsong patterns compared to finance.\n- x_acf1 (Estimate = -0.460, p < 0.05):\nShows a moderate negative link with finance. Higher autocorrelation seems to be more common in birdsongs than in financial data.\n- Covariate1 (Estimate = 0.019, p = 0.93):\nAlmost no effect on classification, it’s statistically insignificant. It doesn't really help in telling birdsongs and finance apart.\n- Covariate2 (Estimate = 1.355, p < 0.001):\nStrongly leans towards identifying finance. It suggests that financial time series are more structured and consistent.\n\nIn conclusion, the logistic regression model reveals entropy as the strongest predictor for birdsongs, reflecting their randomness, while covariate2 distinguishes financial data due to its structured patterns. Linearity and x_acf1 provide moderate contribution, and covariate1 is redundant. This indicates birdsongs are more disordered, whereas finance data are more structured.\n\n\n\n**LDA **\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load necessary libraries\nlibrary(discrim)\nlibrary(tidymodels)\n\n# Define the LDA model specification\nlda_spec <- discrim_linear() |>\n  set_mode(\"classification\") |>\n  set_engine(\"MASS\", prior = c(0.5, 0.5))\n\n# Fit the LDA model to the training data\nlda_fit <- lda_spec |> \n  fit(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, data = train_data)\n\n# Display the fitted model\nlda_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\nCall:\nlda(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, \n    data = data, prior = ~c(0.5, 0.5))\n\nPrior probabilities of groups:\nbirdsongs   finance \n      0.5       0.5 \n\nGroup means:\n            linearity   entropy    x_acf1 covariate1 covariate2\nbirdsongs -0.06582734 0.8403870 0.2152013   2.989098   1.023164\nfinance   17.28510479 0.5066114 0.5103231   3.000907   2.447719\n\nCoefficients of linear discriminants:\n                   LD1\nlinearity   0.02048526\nentropy    -1.38450178\nx_acf1     -0.23675688\ncovariate1  0.03386640\ncovariate2  0.76381428\n```\n\n\n:::\n:::\n\n\n- Group Means Analysis:\nFinancial time series have higher average values for linearity (17.285) and covariate2 (2.4), indicating more structured and consistent patterns. Birdsongs exhibit higher entropy (0.84), suggesting more randomness and disorder.\n- Linear Discriminant Coefficients:\nEntropy has the strongest negative coefficient (-1.385), reinforcing its role in distinguishing birdsongs due to their unpredictable nature.\nCovariate2 (0.764) and linearity (0.020) positively influence classification towards finance, aligning with their structured behavior.\n- x_acf1 (-0.237) is moderately negative, slightly leaning towards birdsongs, but less influential than entropy.\n- Covariate1 (0.034) shows a minimal positive influence, suggesting it has a negligible impact on classification.\n- Prior Probabilities:\nBoth classes are equally weighted (0.5), reflecting balanced consideration during classification.\n\nThe LDA model leverages entropy as the most powerful discriminator for birdsongs, while linearity and covariate2 drive the classification towards finance. This reinforces the understanding that financial data are more structured, while birdsongs are more random and disordered.\n\n#### Evaluating Classifier Performance\n\n**Evaluate the model** \n\nUsing confusion matrices, prediction probabilities, and misclassification patterns, we assess how logistic regression and LDA perform on both training and testing sets. This shows where linear models succeed and where class overlap leads to errors.\n\n**Trainig data evaluation**\n\n_1. Predictions_ \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate predictions for the Logistic Regression model\nlogistic_train_pred <- predict(logistic_fit, new_data = train_data, type = \"class\")\nlogistic_train_pred <- bind_cols(train_data, logistic_train_pred)\n\n# Generate predictions for the LDA model\nlda_train_pred <- predict(lda_fit, new_data = train_data, type = \"class\")\nlda_train_pred <- bind_cols(train_data, lda_train_pred)\n\n# Display the first few rows\nhead(logistic_train_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  linearity entropy x_acf1 type      covariate1 covariate2 .pred_class\n      <dbl>   <dbl>  <dbl> <fct>          <dbl>      <dbl> <fct>      \n1    -0.126   0.898 -0.609 birdsongs       2.72      2.81  finance    \n2     1.01    0.856  1.23  birdsongs       3.26     -0.216 birdsongs  \n3    -0.424   0.929  1.01  birdsongs       3.44      0.871 birdsongs  \n4    -0.225   0.807  0.453 birdsongs       2.39      0.597 birdsongs  \n5    -3.39    0.678  0.306 birdsongs       2.99      1.82  birdsongs  \n6    -0.373   0.813 -0.259 birdsongs       3.07      0.674 birdsongs  \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nhead(lda_train_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  linearity entropy x_acf1 type      covariate1 covariate2 .pred_class\n      <dbl>   <dbl>  <dbl> <fct>          <dbl>      <dbl> <fct>      \n1    -0.126   0.898 -0.609 birdsongs       2.72      2.81  finance    \n2     1.01    0.856  1.23  birdsongs       3.26     -0.216 birdsongs  \n3    -0.424   0.929  1.01  birdsongs       3.44      0.871 birdsongs  \n4    -0.225   0.807  0.453 birdsongs       2.39      0.597 birdsongs  \n5    -3.39    0.678  0.306 birdsongs       2.99      1.82  birdsongs  \n6    -0.373   0.813 -0.259 birdsongs       3.07      0.674 birdsongs  \n```\n\n\n:::\n:::\n\nBased on the initial observations from the training data, both the logistic regression and LDA models misclassified the first \"birdsongs\" instance as \"finance.\" For the remaining rows, both models correctly identified \"birdsongs.\" This indicates a shared misclassification pattern for the first observation, but overall alignment with the true class for the rest of the initial data. No strong bias towards either class is evident from this small sample.\n\n_2. Confusion Matrix_\n\na. Logistic Regression Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# confusion matrix for Logistic Regression\nlogistic_conf_matrix <- logistic_train_pred |>\n  count(type, .pred_class) |>\n  group_by(type) |>\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>\n  pivot_wider(names_from = .pred_class, values_from = n, values_fill = 0) |>\n  select(type, birdsongs, finance, Accuracy)\n\n\nlogistic_conf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  <fct>         <int>   <int>    <dbl>\n1 birdsongs       262      54    0.829\n2 finance          67     266    0.799\n```\n\n\n:::\n:::\n\n\nThe logistic regression model demonstrates strong performance on the training data, achieving an accuracy of 83% for birdsongs and 80% for finance. Among the birdsongs, 262 instances were correctly classified, while 54 were misclassified as finance. For the finance category, 266 were correctly identified, with 67 incorrectly labeled as birdsongs. The model shows a slight bias towards better identification of birdsongs, although this difference is minimal. Overall, the misclassifications are relatively balanced, with finance being slightly more prone to confusion with birdsongs.\n\n\nb. LDA Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# confusion matrix for LDA\nlda_conf_matrix <- lda_train_pred |>\n  count(type, .pred_class) |>\n  group_by(type) |>\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>\n  pivot_wider(names_from = .pred_class, values_from = n, values_fill = 0) |>\n  select(type, birdsongs, finance, Accuracy)\n\n\nlda_conf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  <fct>         <int>   <int>    <dbl>\n1 birdsongs       272      44    0.861\n2 finance          72     261    0.784\n```\n\n\n:::\n:::\n\nThe LDA model shows a strong performance on the training data, achieving an 86% accuracy for birdsongs and 78% accuracy for finance. In the birdsongs category, 272 instances were correctly classified, while 44 were misclassified as finance. For finance, 261 instances were correctly identified, with 72 incorrectly labeled as birdsongs. Compared to logistic regression, LDA slightly improves accuracy for birdsongs but has a marginally lower accuracy for finance. The model seems to better differentiate birdsongs while facing slightly more difficulty with finance series.\n\n**Test data evaluation**\n\n_1. Predictions_ \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate predictions for the Logistic Regression model on test data\nlogistic_test_pred <- predict(logistic_fit, new_data = test_data, type = \"class\")\n\n# Bind the predictions to the test data\nlogistic_test_pred <- bind_cols(test_data, logistic_test_pred)\n\n# Display the first few rows\nhead(logistic_test_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  linearity entropy  x_acf1 type      covariate1 covariate2 .pred_class\n      <dbl>   <dbl>   <dbl> <fct>          <dbl>      <dbl> <fct>      \n1   -0.169    1.04  -0.248  birdsongs       2.58      1.41  birdsongs  \n2   -0.443    0.806  0.571  birdsongs       3.61      1.12  birdsongs  \n3   -0.0896   1.14  -0.0945 birdsongs       3.50      1.67  birdsongs  \n4   -0.0525   0.979 -0.688  birdsongs       3.91      1.39  birdsongs  \n5    0.227    0.931  0.665  birdsongs       2.22      0.454 birdsongs  \n6    0.180    0.817  0.681  birdsongs       2.98      1.09  birdsongs  \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nlda_test_pred <- predict(lda_fit, new_data = test_data, type = \"class\")\n\n# Bind the predictions to the test data\nlda_test_pred <- bind_cols(test_data, lda_test_pred)\n\n# Display the first few rows\nhead(lda_test_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  linearity entropy  x_acf1 type      covariate1 covariate2 .pred_class\n      <dbl>   <dbl>   <dbl> <fct>          <dbl>      <dbl> <fct>      \n1   -0.169    1.04  -0.248  birdsongs       2.58      1.41  birdsongs  \n2   -0.443    0.806  0.571  birdsongs       3.61      1.12  birdsongs  \n3   -0.0896   1.14  -0.0945 birdsongs       3.50      1.67  birdsongs  \n4   -0.0525   0.979 -0.688  birdsongs       3.91      1.39  birdsongs  \n5    0.227    0.931  0.665  birdsongs       2.22      0.454 birdsongs  \n6    0.180    0.817  0.681  birdsongs       2.98      1.09  birdsongs  \n```\n\n\n:::\n:::\n\nLooking at the first few predictions, both the Logistic Regression and LDA models seem to confidently classify \"birdsongs\" correctly. This indicates that, at least for this subset, both models are picking up on the right features to distinguish birdsongs. Confusion matrix should help us understand the braoder performance and classification when it comes to finance.\n\n_2. Confusion Matrix_\n\na. Logistic Regression Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate the confusion matrix for Logistic Regression\nlogistic_test_conf_matrix <- logistic_test_pred |>\n  count(type, .pred_class) |>\n  group_by(type) |>\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>\n  pivot_wider(names_from = \".pred_class\", values_from = n, values_fill = 0) |>\n  select(type, birdsongs, finance, Accuracy)\n\n# Display the confusion matrix\nlogistic_test_conf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  <fct>         <int>   <int>    <dbl>\n1 birdsongs       134      24    0.848\n2 finance          45     122    0.731\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nlogistic_accuracy <- accuracy(logistic_test_pred, truth = type, estimate = .pred_class)$.estimate\nlogistic_accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7876923\n```\n\n\n:::\n:::\n\n\nThe confusion matrix for the Logistic Regression model shows strong performance for Birdsongs, with 134 correctly classified and only 24 misclassified, resulting in an 85% accuracy. For Finance, the model correctly identified 122 instances but misclassified 45 as birdsongs, achieving 73% accuracy. This suggests the model is more effective at identifying birdsongs, while it struggles a bit more with distinguishing finance time series, possibly due to overlapping features.\n\n\nb. LDA Confusion Matrix\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Generate the confusion matrix for LDA\nlda_test_conf_matrix <- lda_test_pred |>\n  count(type, .pred_class) |>\n  group_by(type) |>\n  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>\n  pivot_wider(names_from = \".pred_class\", values_from = n, values_fill = 0) |>\n  select(type, birdsongs, finance, Accuracy)\n\n# Display the confusion matrix\nlda_test_conf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n# Groups:   type [2]\n  type      birdsongs finance Accuracy\n  <fct>         <int>   <int>    <dbl>\n1 birdsongs       139      19    0.880\n2 finance          45     122    0.731\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nlda_accuracy <- accuracy(lda_test_pred, truth = type, estimate = .pred_class)$.estimate\n\nlda_accuracy\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8030769\n```\n\n\n:::\n:::\n\n\nThe confusion matrix for the LDA model on the test data shows a solid performance for Birdsongs, with 139 correctly classified and only 19 misclassified, resulting in an 88% accuracy. For Finance, the model matched the Logistic Regression with 122 correct predictions and 45 misclassifications, achieving a 73% accuracy. This indicates LDA is slightly more effective at identifying birdsongs compared to Logistic Regression, but similarly struggles to distinguish finance time series.\n\nThe accuracy results reveal a slight edge for LDA over Logistic Regression, with LDA achieving 0.80 compared to 0.79 for Logistic Regression. While the difference is minimal, it suggests that LDA might be capturing the underlying structure of the data just a bit more effectively.\n\n**Variable importance**\n\nBased on the observed output of tidy(logistic_fit) and summary of LDA coeffiencts seen above in model fit:\n\nBoth LDA and Logistic Regression identify entropy as a strong predictor of birdsongs and covariate2 as a major indicator of finance.\nlinearity is more pronounced in LDA (17.285 vs. -0.066), suggesting LDA finds it more influential in distinguishing between the two classes.\nBoth models downplay the importance of covariate1, though LDA still assigns it a positive coefficient.\nx_acf1 remains consistently aligned with birdsongs in both models.\nBoth models align on the importance of entropy and covariate2, but LDA amplifies the role of linearity more significantly than logistic regression.\n\n**Prediction Probabilities**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Logistic Regression - Training Predictions\nlogistic_train_prob <- logistic_fit |> \n  augment(new_data = train_data, type.predict = \"prob\") |> \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n# Logistic Regression - Test Predictions\nlogistic_test_prob <- logistic_fit |> \n  augment(new_data = test_data, type.predict = \"prob\") |> \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n# LDA - Training Predictions\nlda_train_prob <- lda_fit |> \n  augment(new_data = train_data, type.predict = \"prob\") |> \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n\n# LDA - Test Predictions\nlda_test_prob <- lda_fit |> \n  augment(new_data = test_data, type.predict = \"prob\") |> \n  mutate(.pred_correct = ifelse(type == \"birdsongs\", .pred_birdsongs, .pred_finance))\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Logistic Regression - Training\np1 <- ggplot(logistic_train_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"Logistic Regression - Training Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n# Logistic Regression - Test\np2 <- ggplot(logistic_test_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"Logistic Regression - Test Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n# LDA - Training\np3 <- ggplot(lda_train_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"LDA - Training Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n# LDA - Test\np4 <- ggplot(lda_test_prob, aes(x = type, y = .pred_correct, fill = type)) +\n  geom_boxplot() +\n  labs(title = \"LDA - Test Data\", x = \"Type\", y = \"Prediction Confidence\") +\n  theme_minimal()\n\n\n(p1 | p2) / (p3 | p4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nBased on Training Data:\nBoth models are quite confident with birdsongs, showing narrow IQRs and medians around 0.75, and are able to differentiate them well.\nFor finance, the spread is much wider. Logistic Regression has a larger IQR with visible outliers, hinting at more uncertainty compared to birdsongs.\nBased on Test Data:\nThe same trends can be observed, birdsongs are being predicted well with compact IQRs and solid medians.\nFor finance, both models struggle a bit more. LDA, in particular, shows a wider IQR and more scattered outliers.\n\nBoth models are consistently strong with birdsongs but show noticeable uncertainty with finance, especially in the test set. LDA's predictions are more spread out, while Logistic Regression displays more extreme outliers.\n\n\n**Identifying Misclassifications in Logistic Regression**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Logistic Regression Misclassifications\nlogistic_misclassified <- logistic_test_pred |>\n  filter(type != .pred_class)\n\n# LDA Misclassifications\nlda_misclassified <- lda_test_pred |>\n  filter(type != .pred_class)\n\n# Common Misclassifications \ncommon_misclassifications <- inner_join(\n  logistic_misclassified |> select(linearity, entropy, x_acf1, covariate1, covariate2, type),\n  lda_misclassified |> select(linearity, entropy, x_acf1, covariate1, covariate2, type),\n  by = c(\"linearity\", \"entropy\", \"x_acf1\", \"covariate1\", \"covariate2\", \"type\")\n)\n\n\nmisclassification_summary <- tibble(\n  Model = c(\"Logistic Regression\", \"LDA\", \"Common Misclassifications\"),\n  Misclassifications = c(nrow(logistic_misclassified), nrow(lda_misclassified), nrow(common_misclassifications))\n)\n\n# Summary \n\nkable(misclassification_summary, caption = \"Summary of Misclassifications\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Summary of Misclassifications\n\n|Model                     | Misclassifications|\n|:-------------------------|------------------:|\n|Logistic Regression       |                 69|\n|LDA                       |                 64|\n|Common Misclassifications |                 62|\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Logistic Regression breakdown\nlogistic_misclassified_summary <- logistic_misclassified |>\n  count(type, .pred_class) |>\n  rename(Misclassified_As = .pred_class, Count = n)\n\n\nkable(logistic_misclassified_summary, caption = \"Logistic Regression Misclassifications\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Logistic Regression Misclassifications\n\n|type      |Misclassified_As | Count|\n|:---------|:----------------|-----:|\n|birdsongs |finance          |    24|\n|finance   |birdsongs        |    45|\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# LDA breakdown\nlda_misclassified_summary <- lda_misclassified |>\n  count(type, .pred_class) |>\n  rename(Misclassified_As = .pred_class, Count = n)\n\n\nkable(lda_misclassified_summary, caption = \"LDA Misclassifications\")\n```\n\n::: {.cell-output-display}\n\n\nTable: LDA Misclassifications\n\n|type      |Misclassified_As | Count|\n|:---------|:----------------|-----:|\n|birdsongs |finance          |    19|\n|finance   |birdsongs        |    45|\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Common misclassification breakdown\ncommon_misclassifications_summary <- common_misclassifications |>\n  count(type) |>\n  rename(Count = n)\n\n\nkable(common_misclassifications_summary, caption = \"Common Misclassifications Breakdown\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Common Misclassifications Breakdown\n\n|type      | Count|\n|:---------|-----:|\n|birdsongs |    19|\n|finance   |    43|\n\n\n:::\n:::\n\n\n\nBased on the misclassification analysis:\n- Logistic Regression had 69 misclassifications, while LDA had 64.\nBoth models struggled with the same observations, sharing 62 common errors.\n- Logistic Regression misclassified Birdsongs as finance 24 times, while LDA did so 19 times.\n- Both models struggled more with Finance, with 45 instances being misclassified as birdsongs in each case.\n- Out of the 62 shared errors, 19 were birdsongs incorrectly labeled as finance, and 43 were finance mislabeled as birdsongs.\nThis highlights a clear difficulty in distinguishing finance series, which seem to overlap more with birdsongs in terms of feature space.\n- Finance series are consistently more difficult to classify correctly, while birdsongs show slightly better separation in both models.\n\n**Misclassification Plot**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() +\n  geom_point(data = logistic_misclassified, aes(x = linearity, y = entropy, color = \"Logistic Regression\"), alpha = 0.7) +\n  geom_point(data = lda_misclassified, aes(x = linearity, y = entropy, color = \"LDA\"), alpha = 0.7) +\n  labs(title = \"Overlay of Misclassifications: Logistic Regression vs LDA\",\n       x = \"Linearity\",\n       y = \"Entropy\") +\n  scale_color_manual(values = c(\"Logistic Regression\" = \"blue\", \"LDA\" = \"red\")) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-39-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n- From the plot, both models struggle the most around the zero region of Linearity, suggesting overlap in features between classes.\n- LDA misclassifies across a wider range, especially with negative linearity and higher entropy.\n- Logistic Regression's errors are more clustered, indicating it is more selective but still misses specific patterns.\n- Several points overlap, confirming the 62 common misclassifications.\n\n\n#### ROC and AUC Analysis\n\nROC curves and AUC values are used to compare the ranking ability of the linear models across all possible thresholds. This provides a threshold-free measure of classification quality.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# ROC Curve for Logistic Regression - Test Data\nlogistic_roc <- roc_curve(logistic_test_prob, truth = type, .pred_birdsongs)\nlogistic_auc <- roc_auc(logistic_test_prob, truth = type, .pred_birdsongs)\n\n# ROC Curve for LDA - Test Data\nlda_roc <- roc_curve(lda_test_prob, truth = type, .pred_birdsongs)\nlda_auc <- roc_auc(lda_test_prob, truth = type, .pred_birdsongs)\n\n# Plotting both ROC Curves\nlogistic_plot <- ggplot(logistic_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"blue\") +\n  geom_abline(linetype = \"dashed\") +\n  labs(title = \"Logistic Regression ROC Curve\",\n       subtitle = paste(\"AUC:\", round(logistic_auc$.estimate, 3))) +\n  theme_minimal()\n\nlda_plot <- ggplot(lda_roc, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"red\") +\n  geom_abline(linetype = \"dashed\") +\n  labs(title = \"LDA ROC Curve\",\n       subtitle = paste(\"AUC:\", round(lda_auc$.estimate, 3))) +\n  theme_minimal()\n\n\nlogistic_plot + lda_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nLogistic Regression\n- The ROC curve shows good separation, with an AUC of 0.876, this indicates strong ability to differentiate between birdsongs and finance time series.\n- It's clearly above the diagonal, the model performs better than random guessing consistently.\nLDA \n- LDA's ROC curve is quite similar to Logistic Regression but has a slightly higher AUC of 0.882.\n- This small improvement suggests LDA handles classification boundaries a bit more accurately, especially when separating birdsongs from finance.\nBoth models perform well, but LDA has a slight advantage with its higher AUC.\nWhile the difference is not massive, it indicates LDA is just a bit better at identifying true positives while keeping false positives lower.\n\n\n#### Interpreting Model Findings and Class Differences\n\n- The analysis shows that financial time series and birdsongs have pretty distinct patterns. Financial data are generally more structured and linear, with higher linearity and covariate2 values reflecting the smooth, predictable trends you’d expect in markets. On the other hand, birdsongs are much more random and unpredictable, which is captured well by their higher entropy values. \n- Both models picked up on these differences, entropy was a strong indicator for birdsongs, while linearity pointed more towards finance. Interestingly, finance series were sometimes mistaken for birdsongs, suggesting there are some irregularities in financial data that mirror the chaotic patterns of birdsongs. \n- Lastly, the models did a good job capturing these differences, though there are still some tricky overlaps that lead to misclassifications.\n\n\n\n#### Tuning a non-linear classifier \n\n#### Fitting a bad decision tree \n\nUnderstanding the need for regularization in decision trees:\n\n- Below, a decision tree is constructed using constarints min_n = 1 and cost_complexity = 0. This is a bad decision tree as it is not regularized and will likely overfit the training data. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# decision tree model \ntree_spec <- decision_tree() %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\", \n             control = rpart.control(minsplit = 1, cp = 0),\n             model = TRUE)\n\n# Fit the model \nbad_tree <- tree_spec %>%\n  fit(type ~ ., data = train_data)\n```\n:::\n\n\n\n**Prediction Metrics**\n\n\n**Train Predictions**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrain_preds <- predict(bad_tree, train_data) %>%\n  bind_cols(train_data %>% select(type))\n\ntest_preds <- predict(bad_tree, test_data) %>%\n  bind_cols(test_data %>% select(type))\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntrain_metrics <- train_preds %>%\n  metrics(truth = type, estimate = .pred_class)\n\nkable(train_metrics, caption = \"Training Metrics for Bad Decision Tree\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Training Metrics for Bad Decision Tree\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     |         1|\n|kap      |binary     |         1|\n\n\n:::\n:::\n\nThe model achieves a perfect 100% accuracy on the training data. This is a classic sign of overfitting, where the tree memorizes the training samples exactly instead of learning generalizable patterns. The Kappa statistic is also 1.00, indicating perfect agreement with the true labels, which is unrealistic for real-world applications.\n\n\n**Testing Predictions**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntest_metrics <- test_preds %>%\n  metrics(truth = type, estimate = .pred_class)\n\n\nkable(test_metrics, caption = \"Testing Metrics for Bad Decision Tree\")\n```\n\n::: {.cell-output-display}\n\n\nTable: Testing Metrics for Bad Decision Tree\n\n|.metric  |.estimator | .estimate|\n|:--------|:----------|---------:|\n|accuracy |binary     | 0.8369231|\n|kap      |binary     | 0.6739850|\n\n\n:::\n:::\n\n\nOn the testing set, the model's accuracy drops to 84%, and the Kappa statistic reduces to 0.67. Although the accuracy remains relatively high, it is clear that the model struggles to generalize beyond the training data. The noticeable drop from 100% to 84% reflects the impact of overfitting, where the tree’s excessive branching failed to capture meaningful, general patterns.\n\n**Visualizing the Bad Decision Tree**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Extract the fitted model \nbad_tree_model <- bad_tree %>%\n  extract_fit_engine()\n\n# Plot \nrpart.plot::rpart.plot(bad_tree_model, main = \"Bad Decision Tree\", roundint = FALSE, cex = 0.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-45-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n- The decision tree visual shows how deeply the model has branched. It's highly fragmented, with many branches and tiny nodes, expected when min_n = 1, allowing splits on nearly every observation.\n- With cost_complexity = 0, there’s no pruning to reduce unnecessary splits, resulting in an overgrown structure that’s cluttered and difficult to interpret.\n- The model memorises the training data but fails to generalise, this is a clear case of overfitting. It captures noise instead of meaningful patterns, highlighting the importance of proper tuning.\n\n#### Hyper parameters \n\nTree depth, minimum node size, and cost-complexity are tuned using cross-validation to identify a more stable model. This section examines how tuning affects tree behaviour and generalisation.\n\n**Defineing  the decision tree model with the hyperparameters set to tune()**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# decision tree model with tunable hyperparameters\ntree_spec <- decision_tree(tree_depth = tune(),\n  min_n = tune(),\n  cost_complexity = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\")\n```\n:::\n\n\nCreating a grid of hyperparameters with 5 levels for each parameter, and set up a 5-fold cross-validation process to evaluate the model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# grid of hyperparameters\nhyper_grid <- grid_regular(\n  tree_depth(),\n  min_n(),\n  cost_complexity(),\n  levels = 5\n)\n\n# 5-fold cross-validation\nset.seed(123)\ncv_folds <- vfold_cv(train_data, v = 5, strata = type)\n```\n:::\n\n\nThe workflow is defined with the decision tree specification and the formula for prediction. A grid search is then performed to evaluate all combinations of parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Define the workflow \ntree_workflow <- workflow() %>%\n  add_model(tree_spec) %>%\n  add_formula(type ~ .)\n\n# Tune the model\nset.seed(123)\ntuned_results <- tune_grid(\n  tree_workflow,\n  resamples = cv_folds,\n  grid = hyper_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n```\n:::\n\n\nExtracting all metrics to identify the best performing configurations. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# metrics from the tuning process\ntree_metrics <- tuned_results %>%\n  collect_metrics()\n\n# top configurations by accuracy (sorted)\nsorted_configs <- tree_metrics %>%\n  filter(.metric == \"accuracy\") %>%\n  arrange(desc(mean))\n\n# top 15 rows \nsorted_configs %>%\n  slice_head(n = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 9\n   cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err\n             <dbl>      <int> <int> <chr>    <chr>      <dbl> <int>   <dbl>\n 1    0.0000000001          8    11 accuracy binary     0.912     5 0.00758\n 2    0.0000000178          8    11 accuracy binary     0.912     5 0.00758\n 3    0.00000316            8    11 accuracy binary     0.912     5 0.00758\n 4    0.000562              8    11 accuracy binary     0.912     5 0.00758\n 5    0.0000000001         11    11 accuracy binary     0.904     5 0.00516\n 6    0.0000000178         11    11 accuracy binary     0.904     5 0.00516\n 7    0.00000316           11    11 accuracy binary     0.904     5 0.00516\n 8    0.000562             11    11 accuracy binary     0.904     5 0.00516\n 9    0.0000000001         15    11 accuracy binary     0.903     5 0.00509\n10    0.0000000178         15    11 accuracy binary     0.903     5 0.00509\n11    0.00000316           15    11 accuracy binary     0.903     5 0.00509\n12    0.000562             15    11 accuracy binary     0.903     5 0.00509\n13    0.0000000001          8    30 accuracy binary     0.895     5 0.0132 \n14    0.0000000001         11    30 accuracy binary     0.895     5 0.0132 \n15    0.0000000001         15    30 accuracy binary     0.895     5 0.0132 \n# ℹ 1 more variable: .config <chr>\n```\n\n\n:::\n:::\n\n\n- The hyperparameter tuning process explored different combinations of tree_depth, min_n, and cost_complexity using 5-fold cross-validation. As the tree depth increased, the model's accuracy improved up to a certain point. The best configuration, with an average accuracy of 0.91, was, Tree Depth: 8, \nMin Node Size: 11, Cost Complexity: 1e-10. \n- This setup achieved a strong ROC AUC of 0.91, indicating it was effective at distinguishing between classes. The low standard error values suggest the model performed consistently well across the folds, capturing patterns in the data reliably.\n\n\n**Plotting the performance metrics**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# performance metrics for accuracy and ROC AUC\nautoplot(tuned_results, metric = \"accuracy\") +\n  labs(title = \"Model Accuracy Across Hyperparameters\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-50-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\nModel Accuracy Across Hyperparameters\n- The plot shows how accuracy changes across different combinations of tree depth, min_n, and cost complexity.\n- The most consistent and accurate results (around 0.91) came from the combination of tree depth = 8, min_n = 11, and cost complexity = 1e-10.\n- Models with smaller min_n values like 2 also reached close to 0.90 accuracy but didn’t improve further, and were more prone to overfitting.\n- Higher min_n values (like 30 or 40) gave stable results but didn’t outperform the configuration with min_n = 11.\n- Accuracy dropped noticeably when cost complexity increased to 1e-02, suggesting that too much pruning prevented the model from learning useful patterns.\n\nIn conclusion, the best-performing model struck a good balance: it was deep enough to capture structure, had enough data in each node to avoid noise, and wasn’t over-pruned.\nThe final choice:\nTree Depth: 8\nMin Node Size: 11\nCost Complexity: 1e-10\n\n\n**Plot: Model ROC AUC Across Hyperparameters**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nautoplot(tuned_results, metric = \"roc_auc\") +\n  labs(title = \"Model ROC AUC Across Hyperparameters\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-51-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nModel ROC AUC Across Hyperparameters:\n- The highest ROC AUC of 0.95 is observed in, Minimal Node Size = 30, with a Tree Depth of 15, and Cost Complexity set to a very low value of 1e-10.\n- For Minimal Node Size = 11, the model achieves 0.91 ROC AUC when paired with a Tree Depth = 8.\nWhen Cost Complexity is too high (e.g., 1e-02), the model is pruned excessively, losing its capacity to effectively separate classes.\n\n\nConfirming the optimal hyperparameters determined from the grid search are:\nTree Depth: 8\nMin Node Size: 11\nCost Complexity: 1e-10\n\n**Selecting the best hyperparameters based on accuracy**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# best hyperparameters based on accuracy\nbest_params <- select_best(tuned_results, metric = \"accuracy\")\n\n\nbest_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config               \n            <dbl>      <int> <int> <chr>                 \n1    0.0000000001          8    11 Preprocessor1_Model008\n```\n\n\n:::\n:::\n\n\n- As observed before the configuration that performed the best during cross-validation is:\nTree Depth: 8\nMin Node Size: 11\nCost Complexity: 1e-10\n\nThis configuration achieved the highest average accuracy of 0.91, with strong ROC AUC and low standard errors, indicating reliable and consistent performance across cross-validation folds.\n\n\n#### Fitting the model using the best hyperparameters\n\nThe best hyperparameters are used to fit a refined decision tree, which is then evaluated on the test set. Improvements are compared against the unregularised tree to show the impact of tuning.\n\n**Fit the final model using the best hyperparameters**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# final model with optimal parameters\nfinal_tree_spec <- decision_tree(\n  tree_depth = 8,\n  min_n = 11,\n  cost_complexity = 1e-10\n) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"rpart\")\n\nfinal_tree_wf <- workflow() %>%\n  add_model(final_tree_spec) %>%\n  add_formula(type ~ .) %>%\n  fit(data = train_data)\n```\n:::\n\n\n**Evaluting the model performance** \n\n_Training_ \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# predictions on the training data\ntrain_predictions <- predict(final_tree_wf, new_data = train_data, type = \"prob\") %>%\n  bind_cols(predict(final_tree_wf, new_data = train_data)) %>%\n  bind_cols(train_data)\n\n# metrics - training set\ntrain_metrics <- train_predictions %>%\n  metrics(truth = type, estimate = .pred_class) \n\n\ntrain_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.948\n2 kap      binary         0.895\n```\n\n\n:::\n:::\n\n\nThe tuned decision tree achieved 95% accuracy and a Kappa of 0.90 on the training data.\nThis shows the model correctly classified most observations and that its predictions strongly agree with the true labels, beyond chance.\nUnlike the overfit model from above, this tree avoids memorising the data, suggesting a good balance between flexibility and generalisation.\n\n_Testing_\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# predictions on the testing data\ntest_predictions <- predict(final_tree_wf, new_data = test_data, type = \"prob\") %>%\n  bind_cols(predict(final_tree_wf, new_data = test_data, type = \"class\")) %>%\n  bind_cols(test_data)\n\n\n# metrics- testing set\ntest_metrics <- test_predictions %>%\n  metrics(truth = type, estimate = .pred_class) \n\n\ntest_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.818\n2 kap      binary         0.637\n```\n\n\n:::\n:::\n\n\n- On the test set, the model reached 82% accuracy with a Kappa of 0.64.\nThis is a noticeable drop from the training accuracy of 95% and Kappa of 0.90.\nThe decrease suggests the model doesn’t overfit, but also shows it picked up on some training specific patterns that didn’t carry over as well.\n- Despite that, the test results are good and show the model generalises fairly well.\n\n**Confusion Matrixes**\n\n**Confusion Matrix for Training Data**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Confusion Matrix for Training Data\ntrain_conf_matrix <- train_predictions %>%\n  conf_mat(truth = type, estimate = .pred_class)\n\n\ntrain_conf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  birdsongs finance\n  birdsongs       304      22\n  finance          12     311\n```\n\n\n:::\n:::\n\n- The model classified most training examples correctly, with only 34 misclassifications out of 649.\n- It confused some finance texts as birdsongs (22) and a few birdsongs as finance (12).\n- This shows the model learned the training patterns well without becoming overly specific, which is what we look for after tuning.\n\n**Confusion Matrix for Testing Data**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Confusion Matrix for Testing Data\ntest_conf_matrix <- test_predictions %>%\n  conf_mat(truth = type, estimate = .pred_class)\n\n\ntest_conf_matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  birdsongs finance\n  birdsongs       132      33\n  finance          26     134\n```\n\n\n:::\n:::\n\n- The model got most predictions right on the test set, correctly identifying 132 birdsongs and 134 finance texts.\n- A total of 59 errors, it misclassifies 33 finance as birdsongs and 26 birdsongs as finance \n- This matches the earlier accuracy of 82%, and it’s a sign that while the model generalises fairly well, it still struggles a bit with overlap between the two classes.\n\n**ROC Curve for Testing Data**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# ROC Curve for Testing Data\nroc_curve_test <- test_predictions %>%\n  roc_curve(truth = type, .pred_birdsongs) %>%\n  autoplot() +\n  ggtitle(\"ROC Curve for Optimized Decision Tree (Test Data)\")\n\n\nroc_curve_test\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-58-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nroc_auc(test_predictions, truth = type, .pred_birdsongs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.904\n```\n\n\n:::\n:::\n\n- The ROC curve for the test set shows that the model separates the two classes well, with a steep rise toward the top left corner.\n- The AUC value of 0.90 confirms this, it means that, across all possible thresholds, the model ranks birdsongs higher than finance 90% of the time.\n- Even though the test accuracy was lower at 82%, the high AUC shows that the model is still effective at distinguishing between the two classes.\n\n**Comparison with linear models on test data**\n\n- Compared to the linear models, the tuned decision tree performed slightly better on both accuracy and AUC.\n- Logistic regression achieved 79% accuracy and an AUC of 0.876, while LDA reached 80% accuracy with an AUC of 0.882.\n- The decision tree outperformed both, with 82% accuracy and an AUC of 0.90, suggesting it separated the classes more effectively.\n- The tree also had the advantage of capturing non-linear patterns and didn’t rely on assumptions like multivariate normality or equal covariance, which the data didn’t fully satisfy.\n- While the linear models were easier to interpret — especially LDA, which clearly highlighted entropy and covariate2 — they struggled more with overlapping feature space, particularly for finance series.\n- Given the stronger performance and flexibility, the tuned decision tree would be the more reliable choice in practice. \n\n\n#### Which is the better classifier? \n\nA random forest is tuned and fitted to capture nonlinear patterns and feature interactions. We compare its performance to linear and single-tree models to see how ensemble methods improve stability and accuracy.\n\n#### a. Random Forest Model \n\nDefining a random forest model using the with 1000 trees, and tuned two key parameters: \n- mtry - number of predictors to consider at each split \n- min_n - minimum observations per node. \n\n**Define the random forest model specification**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# random forest model \nrf_spec <- rand_forest(\n  mode = \"classification\",\n  trees = 1000,\n  mtry = tune(),\n  min_n = tune()\n) %>%\n  set_engine(\"ranger\", importance = \"impurity\")\n```\n:::\n\n\n \n**Set up the tuning grid and cross-validation**\n\n5-fold cross-validation using a regular grid of 25 combinations\n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# 5-fold cross-validation\nset.seed(123)\ncv_folds <- vfold_cv(train_data, v = 5, strata = type)\n\n# Grid of hyperparameters\nrf_grid <- grid_regular(\n  mtry(range = c(1, 5)),\n  min_n(range = c(2, 40)),\n  levels = 5\n)\n```\n:::\n\n \n \n**Tune the random forest model**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Workflow\nrf_workflow <- workflow() %>%\n  add_model(rf_spec) %>%\n  add_formula(type ~ .)\n\n# Tune the model\nset.seed(123)\nrf_tune_results <- tune_grid(\n  rf_workflow,\n  resamples = cv_folds,\n  grid = rf_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n```\n:::\n\n \n**Extract the best hyperparameters based on accuracy**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Review tuning metrics\ncollect_metrics(rf_tune_results) %>%\n  filter(.metric == \"accuracy\") %>%\n  arrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     2    11 accuracy binary     0.909     5 0.00828 Preprocessor1_Model07\n 2     3    11 accuracy binary     0.908     5 0.00641 Preprocessor1_Model08\n 3     3    30 accuracy binary     0.908     5 0.00651 Preprocessor1_Model18\n 4     3    21 accuracy binary     0.906     5 0.00620 Preprocessor1_Model13\n 5     3    40 accuracy binary     0.904     5 0.00583 Preprocessor1_Model23\n 6     4    21 accuracy binary     0.903     5 0.00515 Preprocessor1_Model14\n 7     4    11 accuracy binary     0.903     5 0.00711 Preprocessor1_Model09\n 8     2     2 accuracy binary     0.903     5 0.00671 Preprocessor1_Model02\n 9     5    21 accuracy binary     0.901     5 0.00738 Preprocessor1_Model15\n10     4    40 accuracy binary     0.901     5 0.00622 Preprocessor1_Model24\n# ℹ 15 more rows\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Best hyperparameters\nbest_rf_params <- select_best(rf_tune_results, metric = \"accuracy\")\nbest_rf_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1     2    11 Preprocessor1_Model07\n```\n\n\n:::\n:::\n\n\n- After tuning 25 combinations of mtry and min_n, the best-performing setup was mtry = 2 and min_n = 11, with an average accuracy of 0.909.\n- The low standard error of  0.008, also suggests the model performed consistently across the 5 cross-validation folds.\n- This setup performed well across folds, likely because it limited overfitting while still capturing useful patterns.\n\n**Finalize the model with the best hyperparameters and fit it to the training data**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Final model spec with best params\nfinal_rf_spec <- finalize_model(rf_spec, best_rf_params)\n\n# Final workflow\nfinal_rf_workflow <- rf_workflow %>%\n  update_model(final_rf_spec)\n\n# Fit the model\nfinal_rf_fit <- fit(final_rf_workflow, data = train_data)\n```\n:::\n\n \n**Evaluate the model on training data**\n\n**Training predictions and metrics**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Training predictions\ntrain_rf_preds <- predict(final_rf_fit, train_data, type = \"prob\") %>%\n  bind_cols(predict(final_rf_fit, train_data)) %>%\n  bind_cols(train_data)\n\ntrain_rf_metrics <- train_rf_preds %>%\n  metrics(truth = type, estimate = .pred_class)\n\ntrain_rf_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.974\n2 kap      binary         0.948\n```\n\n\n:::\n:::\n\n\n- The model achieved 97.5% accuracy and a Kappa of 0.95, indicating it classified most training observations correctly with very few errors.\n- The high Kappa shows that the predictions closely matched the true labels, not just by chance. While strong performance is expected, it also raises the possibility of slight overfitting.\n\n**Testing predictions and metrics**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Testing predictions\ntest_rf_preds <- predict(final_rf_fit, test_data, type = \"prob\") %>%\n  bind_cols(predict(final_rf_fit, test_data)) %>%\n  bind_cols(test_data)\n\ntest_rf_metrics <- test_rf_preds %>%\n  metrics(truth = type, estimate = .pred_class)\n\ntest_rf_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.868\n2 kap      binary         0.736\n```\n\n\n:::\n:::\n\n \nThe model achieved 86.5% accuracy and a Kappa of 0.73 on the test set. The ~11% drop from training accuracy is reasonable and indicates that the model didn’t overfit. It was able to learn meaningful patterns from the training data that carry over to new data. The Kappa score still shows strong agreement with the true labels, suggesting reliable generalisation.\n\n\n#### Exploring Boosted Trees \n\nA boosted tree model is fitted to explore how sequential learning captures difficult cases. Tuning results and performance metrics help assess bias-variance trade-offs relative to the random forest.\n\n**Boosted tree model**\n\nA boosted tree model defined using the xgboost with 1000 boosting rounds. The key hyperparameters tuned were tree_depth, which controls model complexity, and learn_rate, which determines how quickly the model adapts to errors.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Model spec\nboost_spec <- boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  learn_rate = tune(),\n  loss_reduction = 0\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n```\n:::\n\n\n**Set up the tuning grid and 5 fold cross-validation**\n\nA regular grid of 25 combinations created by varying tree_depth and learn_rate across 5 levels each. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Define tuning grid\nboost_grid <- grid_regular(\n  tree_depth(range = c(1, 10)),\n  learn_rate(range = c(0.001, 0.3)),\n  levels = 5\n)\n```\n:::\n\n\n#### Tune the boosted tree model\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Workflow\nboost_wf <- workflow() %>%\n  add_model(boost_spec) %>%\n  add_formula(type ~ .)\n\n# Grid search tuning\nset.seed(123)\nboost_tune_results <- tune_grid(\n  boost_wf,\n  resamples = cv_folds,\n  grid = boost_grid,\n  metrics = metric_set(accuracy, roc_auc)\n)\n```\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Review results and select best\nboost_tune_results %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  arrange(desc(mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 × 8\n   tree_depth learn_rate .metric  .estimator  mean     n std_err .config        \n        <int>      <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>          \n 1         10       1.41 accuracy binary     0.903     5 0.0125  Preprocessor1_…\n 2          3       1.19 accuracy binary     0.903     5 0.00723 Preprocessor1_…\n 3         10       1.00 accuracy binary     0.901     5 0.00463 Preprocessor1_…\n 4          7       1.41 accuracy binary     0.901     5 0.00762 Preprocessor1_…\n 5          5       1.19 accuracy binary     0.900     5 0.0129  Preprocessor1_…\n 6          3       1.00 accuracy binary     0.898     5 0.00793 Preprocessor1_…\n 7          7       1.68 accuracy binary     0.897     5 0.0105  Preprocessor1_…\n 8          5       1.41 accuracy binary     0.897     5 0.00577 Preprocessor1_…\n 9          7       1.00 accuracy binary     0.897     5 0.00794 Preprocessor1_…\n10          7       1.19 accuracy binary     0.897     5 0.00632 Preprocessor1_…\n# ℹ 15 more rows\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nbest_boost_params <- select_best(boost_tune_results, metric = \"accuracy\")\nbest_boost_params\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  tree_depth learn_rate .config              \n       <int>      <dbl> <chr>                \n1         10       1.41 Preprocessor1_Model15\n```\n\n\n:::\n:::\n\n\n- The best configuration was tree_depth = 10 and learn_rate = 1.41, with an average accuracy of 0.903.\n- The deeper trees helped capture complex patterns, while the high learning rate allowed faster corrections, improving training speed.\n- Although this increases the risk of overfitting, the model showed stable performance across folds with std. error = 0.0125 , suggesting good generalisation during cross-validation.\n\n**Finalize and fit the boosted tree model**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Final model and workflow\nfinal_boost_spec <- finalize_model(boost_spec, best_boost_params)\n\nfinal_boost_wf <- boost_wf %>%\n  update_model(final_boost_spec)\n\nfinal_boost_fit <- fit(final_boost_wf, data = train_data)\n```\n:::\n\n\n\n**Evaluate the boosted tree model on training data**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Training performance\ntrain_boost_preds <- predict(final_boost_fit, train_data, type = \"prob\") %>%\n  bind_cols(predict(final_boost_fit, train_data)) %>%\n  bind_cols(train_data)\n\ntrain_boost_metrics <- train_boost_preds %>%\n  metrics(truth = type, estimate = .pred_class)\n\ntrain_boost_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary             1\n2 kap      binary             1\n```\n\n\n:::\n:::\n\n\n- The boosted tree model achieved 100% accuracy and a Kappa of 1.00 on the training set.\n— This confirmins it fully learned the data, it made no classification errors and its predictions perfectly matched the true labels, which is often a sign of overfitting.  It likely captured both true patterns and noise.\n\n\n**Confusion Matrix for Training Data**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Confusion Matrix - Training\ntrain_boost_conf <- train_boost_preds %>%\n  conf_mat(truth = type, estimate = .pred_class)\n\ntrain_boost_conf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  birdsongs finance\n  birdsongs       316       0\n  finance           0     333\n```\n\n\n:::\n:::\n\nThe confusion matrix shows perfect classification on the training set, all 316 birdsongs and 333 finance examples were predicted correctly. This matches the accuracy and Kappa of 1, and clearly suggests the model has overfitted to the training data.\n\n**Evaluate the boosted tree model on testing data**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntest_boost_preds <- predict(final_boost_fit, test_data, type = \"prob\") %>%\n  bind_cols(predict(final_boost_fit, test_data)) %>%\n  bind_cols(test_data)\n\ntest_boost_metrics <- test_boost_preds %>%\n  metrics(truth = type, estimate = .pred_class)\n\ntest_boost_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.843\n2 kap      binary         0.686\n```\n\n\n:::\n:::\n\n\n- On the test set, the boosted tree achieved an accuracy of 84.3% and a Kappa of 0.69. - While this is a noticeable drop from the perfect training results, it’s still a good outcome. The gap between training and testing confirms that the model overfit to the training data to some extent. \n- However, the test accuracy and Kappa still reflect good predictive performance and reasonable generalisation.\n\n**Confusion Matrix for Testing Data**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Confusion Matrix - Testing\ntest_boost_conf <- test_boost_preds %>%\n  conf_mat(truth = type, estimate = .pred_class)\n\ntest_boost_conf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Truth\nPrediction  birdsongs finance\n  birdsongs       135      28\n  finance          23     139\n```\n\n\n:::\n:::\n\n- The confusion matrix shows the model correctly classified 135 birdsongs and 139 finance cases, with 28 finance misclassified as birdsongs and 23 birdsongs misclassified as finance.\n- This aligns with the test accuracy of 84.3% and Kappa of 0.69, indicating the model generalises reasonably well despite some confusion between the two classes.\n\n\n\n#### Classifier Comparison and Final Insights\n\nAll classifiers (logistic, LDA, tuned tree, random forest, boosted tree) are compared using ROC curves. AUC scores give a clear performance ranking across modelling strategies.\n\n**ROC Comparison**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Logistic Regression\nroc_log <- roc_curve(logistic_test_prob, truth = type, .pred_birdsongs) %>%\n  mutate(model = \"Logistic\")\n\n# LDA\nroc_lda <- roc_curve(lda_test_prob, truth = type, .pred_birdsongs) %>%\n  mutate(model = \"LDA\")\n\n# Decision Tree\nroc_tree <- roc_curve(test_predictions, truth = type, .pred_birdsongs) %>%\n  mutate(model = \"Decision Tree\")\n\n# Random Forest \nroc_rf <- roc_curve(test_rf_preds, truth = type, .pred_birdsongs) %>%\n  mutate(model = \"Random Forest\")\n\n# Boosted Tree \nroc_boost <- roc_curve(test_boost_preds, truth = type, .pred_birdsongs) %>%\n  mutate(model = \"Boosted Tree\")\n\n\nroc_all <- bind_rows(roc_log, roc_lda, roc_tree, roc_rf, roc_boost)\n\n#AUC \nauc_scores <- bind_rows(\n  roc_auc(logistic_test_prob, truth = type, .pred_birdsongs) %>% mutate(model = \"Logistic\"),\n  roc_auc(lda_test_prob, truth = type, .pred_birdsongs) %>% mutate(model = \"LDA\"),\n  roc_auc(test_predictions, truth = type, .pred_birdsongs) %>% mutate(model = \"Decision Tree\"),\n  roc_auc(test_rf_preds, truth = type, .pred_birdsongs) %>% mutate(model = \"Random Forest\"),\n  roc_auc(test_boost_preds, truth = type, .pred_birdsongs) %>% mutate(model = \"Boosted Tree\")\n)\n\n\nauc_labels <- auc_scores %>%\n  mutate(label = paste0(model, \" (AUC = \", round(.estimate, 3), \")\"))\n\nmodel_order <- auc_labels$model\ncolor_map <- c(\n  \"Logistic\" = \"#1f77b4\",\n  \"LDA\" = \"#17becf\",\n  \"Decision Tree\" = \"#bcbd22\",\n  \"Random Forest\" = \"#e377c2\",\n  \"Boosted Tree\" = \"#ff7f0e\"\n)\nnames(color_map) <- model_order\nlabel_map <- auc_labels$label\nnames(label_map) <- model_order\n\n# Plot ROC curves \nggplot(roc_all, aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(linewidth = 0.6) +\n  geom_abline(linetype = \"dashed\", color = \"grey\") +\n  scale_color_manual(\n    values = color_map,\n    breaks = model_order,\n    labels = label_map\n  ) +\n  labs(\n    title = \"ROC Curves for All Models\",\n    x = \"1 - Specificity (False Positive Rate)\",\n    y = \"Sensitivity (True Positive Rate)\",\n    color = \"Model\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-75-1.png){fig-align='center' width=100%}\n:::\n:::\n\nThe ROC curves for all five models highlight how well each one distinguishes between birdsongs and finance time series.\n\n- Logistic Regression and LDA have similar curves, with LDA performing slightly better AUC = 0.882 , as comapred to AUC = 0.876 of logistic. This matches earlier oberveations , where LDA handled class separation a bit more effectively.\n- The Tuned Decision Tree has a noticeably steeper curve and a higher AUC of 0.904, showing that tuning helped it better capture decision boundaries.\n- Boosted Tree performs strongly as well, with an AUC of 0.918. Despite overfitting the training data, its test ROC performance indicates it generalised well overall.\n- Random Forest has the highest curve overall, with an AUC of 0.942. It ranked the classes most effectively and remained stable on the test set, confirming its strong overall performance.\n\nIn conclusion, the ROC curve shows that Random Forest did the best job at separating the classes, followed closely by Boosted Tree and the Tuned Decision Tree. The linear models performed reasonably well but struggled more with the class overlap in the data. This conclusion is consistent with the earlier observations when fitiing and summarizing the models. Random Forest and Boosted Tree are the most reliable classifiers for this dataset, effectively capturing the complex patterns in the time series data.\n\n#### Which model is best?\n\nThe best-performing classifier is identified, and its results are used to characterise how birdsongs differ from financial time-series. This ties the modelling results back to the data structure and feature behaviour observed earlier.\n\n**Best Model and how the time series for financial data and birdsongs typically differ**\n\nAfter comparing all five models, Random Forest stands out as the most effective classifier, with the highest ROC AUC of 0.942. It consistently generalised well on the test data, outperforming both linear and non-linear alternatives without showing signs of overfitting. This aligns with the model’s strength in handling feature interactions and non-linearities, which are apparent in this dataset. In contrast, Boosted Tree, while also strong (AUC = 0.918), slightly overfit the training data, and Tuned Decision Tree (AUC = 0.904) offered a good balance but didn’t quite match Random Forest’s stability. From the analysis, we can observe that financial time series is more structured, with higher values in linearity and covariate2, while birdsong series is more random and irregular, showing higher entropy and weaker autocorrelation. These differences are also reflected druing the individual model analysis, where linear models like Logistic Regression and LDA struggled more due to class overlap and violated assumptions, whereas tree-based models adapted better to the complexity of the data.\n\n\n## References\n\nHadley Wickham, Dianne Cook, Heike Hofmann, Andreas Buja\n  (2011). tourr: An R Package for Exploring Multivariate\n  Data with Projections. Journal of Statistical Software,\n  40(2), 1-18. URL http://www.jstatsoft.org/v40/i02/.\n  \nKuhn et al., (2020). Tidymodels: a collection of\n  packages for modeling and machine learning using\n  tidyverse principles. https://www.tidymodels.org  \n  \n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}