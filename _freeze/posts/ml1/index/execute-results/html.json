{
  "hash": "ddc3d8fcf780a13689cd6dfe56a39b73",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction - Exploring Machine Learning\"\nauthor: \"Pooja Rajendran Raju\"\ndate: \"2025-03-17\"\nquarto-required: \">=1.3.0\"\noutput:\n    html:\n        embed-resources: true\ncategories: [Machine Learning]\n---\n\n<!-- Guide to using quarto at https://quarto.org/docs/get-started/hello/rstudio.html -->\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n#### ML concepts\n\n**Confusion Matrix**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Reading Data\nd_pred <- read_csv(\"pred_data.csv\")\nhead(d_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  y     pred1 pred2 bilby1 bilby2 quokka1 quokka2\n  <chr> <chr> <chr>  <dbl>  <dbl>   <dbl>   <dbl>\n1 bilby bilby bilby   0.8    0.8     0.2     0.2 \n2 bilby bilby bilby   0.9    0.51    0.1     0.49\n3 bilby bilby bilby   0.9    0.6     0.1     0.4 \n4 bilby bilby bilby   0.7    0.6     0.3     0.4 \n5 bilby bilby bilby   0.8    0.8     0.2     0.2 \n6 bilby bilby bilby   0.51   0.8     0.49    0.2 \n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Setting positive class as Bilby\nd_predb <- d_pred |> \n  select(y, pred1, pred2) |> \n  mutate(\n  y = factor(y, levels = c(\"bilby\", \"quokka\")),\n  pred1 = factor(pred1, levels = c(\"bilby\", \"quokka\")),\n  pred2 = factor(pred2, levels = c(\"bilby\", \"quokka\"))\n)\n\n\n\n# Confusion matrix in standard form - model 1\ncm <- d_predb |> count(y, pred1) |>\n  group_by(y) |>\n  mutate(cl_acc = n[pred1==y]/sum(n)) \ncm |>\n  pivot_wider(names_from = pred1, \n              values_from = n) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n# Groups:   y [2]\n  y      cl_acc bilby quokka\n  <fct>   <dbl> <int>  <int>\n1 bilby    0.75    18      6\n2 quokka   0.8      6     24\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n# Confusion matrix in standard form - model 2\ncm2 <- d_predb |> count(y, pred2) |>\n  group_by(y) |>\n  mutate(cl_acc = n[pred2==y]/sum(n)) \ncm2 |>\n  pivot_wider(names_from = pred2, \n              values_from = n) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n# Groups:   y [2]\n  y      cl_acc bilby quokka\n  <fct>   <dbl> <int>  <int>\n1 bilby   0.917    22      2\n2 quokka  0.633    11     19\n```\n\n\n:::\n:::\n\n\n**Model Metrics**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Metrics - Model 1\n\n# Accuracy\nacc_pred1 <- accuracy(d_predb, y, pred1) |> pull(.estimate)\n\n# Balanced Accuracy\nbacc_pred1 <- bal_accuracy(d_predb, y, pred1) |> pull(.estimate)\n\n# True Positive Rate (TPR)\ntpr_pred1 <- sens(d_predb, y, pred1) |> pull(.estimate)\n\n# True Negative Rate (TNR)\ntnr_pred1 <- specificity(d_predb, y, pred1) |> pull(.estimate)\n\n# Metrics - Model 2\n\n# Accuracy\nacc_pred2 <- accuracy(d_predb, y, pred2) |> pull(.estimate)\n\n# Balanced Accuracy\nbacc_pred2 <- bal_accuracy(d_predb, y, pred2) |> pull(.estimate)\n\n# True Positive Rate (TPR)\ntpr_pred2 <- sens(d_predb, y, pred2) |> pull(.estimate)\n\n# True Negative Rate (TNR)\ntnr_pred2 <- specificity(d_predb, y, pred2) |> pull(.estimate)\n\n# Tibble with metrics for both models\nmetrics_table <- tibble(\n  Metric = c(\"Accuracy\", \"Balanced Accuracy\", \"True Positive Rate (TPR)\", \"True Negative Rate (TNR)\"),\n  Model_1 = round(c(acc_pred1, bacc_pred1, tpr_pred1, tnr_pred1), 4),\n  Model_2 = round(c(acc_pred2, bacc_pred2, tpr_pred2, tnr_pred2), 4)\n)\n\n\nmetrics_table |> \n  kable(caption = \"Model Metrics\", align = \"lcc\") |> \n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n    full_width = F, \n    position = \"center\", \n    font_size = 14, \n    stripe_color = \"gray!20\" \n  )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">Model Metrics</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Metric </th>\n   <th style=\"text-align:center;\"> Model_1 </th>\n   <th style=\"text-align:center;\"> Model_2 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Accuracy </td>\n   <td style=\"text-align:center;\"> 0.7778 </td>\n   <td style=\"text-align:center;\"> 0.7593 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Balanced Accuracy </td>\n   <td style=\"text-align:center;\"> 0.7750 </td>\n   <td style=\"text-align:center;\"> 0.7750 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> True Positive Rate (TPR) </td>\n   <td style=\"text-align:center;\"> 0.7500 </td>\n   <td style=\"text-align:center;\"> 0.9167 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> True Negative Rate (TNR) </td>\n   <td style=\"text-align:center;\"> 0.8000 </td>\n   <td style=\"text-align:center;\"> 0.6333 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n**a. TRP and TNR**\n\nModel 2 has a higher True Positive Rate (0.9167), making it better at detecting bilbies. Model 1 has a higher True Negative Rate (0.80), meaning it’s better at correctly identifying quokkas. This reflects a trade-off: Model 2 is more sensitive, while Model 1 is more specific.\n\n**b. Accuracy and Balanced Accuracy**\n\nFor both models, accuracy measures overall correctness, considering all predictions, whereas balanced accuracy focuses on individual class performance. It calculates the average of the True Positive Rate (TPR) and True Negative Rate (TNR), which gives equal importance to each class. This is especially useful in cases like ours, where the dataset is imbalanced—there are more quokkas (30) than bilbies (24).\n\nIn Model 1, the accuracy (77.78%) and balanced accuracy (77.5%) are nearly identical, with only a small difference of 0.28%. This suggests that the model performs evenly across both classes without a strong bias toward the majority class (quokkas). The small gap between the two metrics indicates that Model 1 handles the class imbalance well, making fair predictions for both classes.\n\nOn the other hand, Model 2 shows a lower accuracy (75.93%) but a higher balanced accuracy (77.5%), with a difference of 1.57%. This suggests that while Model 2's overall performance is slightly weaker, it does a better job of predicting the minority class (bilbies). The larger difference between accuracy and balanced accuracy points to the model's ability to focus more on fairness, ensuring better representation for both classes, even though overall accuracy is reduced.\n\n**c. Threshold for Bilby and Quokka**\n\nThe predicted classes in this dataset were determined using a threshold of 0.5. This means that if the probability for bilby was 0.5 or higher, the model predicted the class as bilby, and if the probability for quokka was greater than 0.5, it predicted quokka. While this threshold is a standard choice in binary classification problems, its effectiveness might depend on the balance of the classes and the specific objectives of the model.\n\nGiven that the dataset is imbalanced, with more quokkas than bilbies, this threshold may not be the best fit. To fine-tune the model, we could experiment with different thresholds, which might improve predictions, especially for the minority class (bilby). ROC curves can give us a clearer picture of how the model performs across different threshold values, helping to identify a better balance between sensitivity and specificity for both classes.\n\n**d. ROC Curves** \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nd_pred$y <- factor(d_pred$y, levels = c(\"bilby\", \"quokka\"))\n\nroc_curve(d_pred, y, bilby1) |> \n  autoplot() +\n  ggtitle(\"ROC Curve for Model 1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nroc_curve(d_pred, y, bilby2) |> \n  autoplot() +\n  ggtitle(\"ROC Curve for Model 2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-2.png){fig-align='center' width=100%}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\nmd1 <- roc_auc(d_pred, y, bilby1)\nmd2 <- roc_auc(d_pred, y, bilby2)\n\n# Create a table with ROC AUC values for both models\n\nauc <- tibble(\n  Model = c(\"Model 1\", \"Model 2\"),\n  ROC_AUC = round(c(md1$.estimate, md2$.estimate), 4)\n)\n\nauc |> \n  kable(caption = \"ROC AUC\", align = \"lcc\") |> \n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), \n    full_width = F, \n    position = \"center\", \n    font_size = 14, \n    stripe_color = \"gray!20\" \n  )\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover table-condensed\" style=\"font-size: 14px; width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">ROC AUC</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:center;\"> ROC_AUC </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Model 1 </td>\n   <td style=\"text-align:center;\"> 0.8354 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 2 </td>\n   <td style=\"text-align:center;\"> 0.7597 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nROC curves were generated for both models using predicted probabilities for the bilby class, treated as the positive class. The ROC curve plots the true positive rate (TPR) against the false positive rate (FPR) across different thresholds, offering an overall view of model performance.\n\nModel 1 demonstrates stronger distinction between the classes, with a ROC curve that lies above that of Model 2 across most thresholds. This is confirmed by its higher AUC of 0.8354, indicating that Model 1 has an 83.5% chance of ranking a randomly chosen bilby higher than a randomly chosen quokka. In contrast, Model 2’s AUC is 0.7597, suggesting noticeably weaker separation.\n\nWhile neither curve shows the ideal shape of a near-vertical rise followed by a flat line along the top (indicating perfect classification), Model 1 is clearly more effective across a range of thresholds. Its curve achieves a better balance between sensitivity and specificity and maintains higher TPR at comparable FPRs. Model 2’s curve, closer to the diagonal line, shows more overlap between the two classes less certainty in distinguishing between them.\n\nModel 1 consistently outperforms Model 2, both visually and statistically, and is the stronger choice when evaluating overall classification performance based on ROC analysis.\n\n\n#### Visualisation\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nexplore <- data(c7)\nstr(c7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [1,300 × 6] (S3: tbl_df/tbl/data.frame)\n $ x1: num [1:1300] 10.5 11 10.9 10.4 10.9 ...\n $ x2: num [1:1300] 10.9 10.3 10.5 10.9 10.4 ...\n $ x3: num [1:1300] 9.95 9.98 9.96 10.02 9.97 ...\n $ x4: num [1:1300] -10 -10 -9.99 -9.98 -9.99 ...\n $ x5: num [1:1300] -0.01153 -0.00374 0.01316 0.00691 0.00225 ...\n $ x6: num [1:1300] 0.0824 -0.05948 0.09794 0.00177 0.03837 ...\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\n#summary(c7)\n```\n:::\n\n\n###### Scatterplot matrix\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Scatterplot matrix\n\n# Customising the size of the points\ncustom_lower <- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) +\n    geom_point(alpha = 0.2, size = 0.4, color = \"black\")\n}\n\n\nggpairs(c7,\n        lower = list(continuous = custom_lower),\n        title = \"Scatterplot Matrix of c7\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe scatterplot matrix of the _c7_ dataset reveals several key patterns. The _x1 vs x2_ plot shows a distinct curved, multimodal structure, suggesting three to four clusters that aren’t linearly separable. In contrast, _x3 vs x4_ forms a tight, unimodal cluster centered in the plot, indicating a single group with no visible substructure or correlation.\n\nThe _x1 vs x4_ and _x2 vs x4_ plots display a gradual spread of points leading into a denser region, reflecting a skewed or narrowing distribution rather than distinct groupings. _x1 vs x3_ and _x2 vs x3_ show some spread with slight tapering, but no clear separation. Most variable pairs exhibit weak or no linear relationships, with only _x1_ and _x2_ showing moderate nonlinear association.\n\nVariables like _x5_ and _x6_ show limited variation, suggesting they contribute little to the dataset’s effective dimensionality. Mild outliers appear in dimensions involving _x1_, _x2_, and _x3_, though none are extreme. Overall, the matrix suggests overlapping contributions across variables and a non-linear underlying structure.\n\n###### Tourr\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Tourr the data\n#x11()\nanimate_xy(c7, axes = \"bottomleft\",\n           rescale=TRUE)\n\n#set.seed(645)\nrender_gif(c7,\n           grand_tour(),\n           display_xy(half_range = 3.8,\n             axes=\"bottomleft\", cex=1.1),\n           gif_file = \"c7.gif\",\n           apf = 1/60,\n           frames = 1800,\n           width = 600, \n           height = 600)\n```\n:::\n\n\n![Grand Tour of c7](c7.gif)\n\nThe grand tour provided a more complete view of the structure within the c7 dataset. Across different projections, approximately three to four clusters were consistently visible, although their separation varied depending on the viewing angle. \n\nIn certain projections, the clusters appeared well-separated, while in others they overlapped, suggesting that their distinction is not easily captured in low-dimensional views. Some clusters appeared compact, while others were elongated or curved, indicating variation in spread and shape across dimensions. \n\nNo extreme outliers were observed—only a few points near the cluster edges occasionally drifted outward but returned as the view rotated. The tour also highlighted that some variables contributed more to revealing the structure, with projections involving x1 and x2 showing clearer separation, while x5 and x6 appeared less informative. The remaining variables (x3 and x4) showed moderate influence, occasionally contributing to the separation of certain clusters depending on the projection. \n\nOverall, the tour confirmed that the dataset has a nonlinear and multidimensional structure, which becomes more noticeable when viewed dynamically through different projection views.\n\n###### UMAP\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(253)\np_tidy_umap <- umap(c7, init = \"spca\")\n\nplot(p_tidy_umap,\n     pch = 19,\n     col = \"black\",\n     main = \"UMAP Projection\",\n     xlab = \"UMAP 1\",\n     ylab = \"UMAP 2\")\ngrid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe UMAP projection showed three to four clusters with different shapes. One cluster was clearly separated on the far left, while the right side formed a curved, spread-out structure instead of a tight group. Some clusters were more compact than others, suggesting differences in how tightly the points are grouped. This kind of layout points to a nonlinear structure, where the clusters aren’t aligned along simple axes. There were no strong outliers, as all the points stayed fairly close to the main formations. UMAP made it easier to see both the smaller groupings and the overall arrangement, which lines up with what was seen in the tour and scatterplot matrix. \n\n#### Dimension reduction \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Load the data\n\nengwt20 <- read.csv(\"engwt20.csv\")\nengwt20 |> slice_head(n=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Player Country Start  End Matches InningsBowled Overs Maidens\n1   S Ecclestone England  2016 2025      96            95   355      10\n2 K Sciver-Brunt England  2005 2023     112           111   392      17\n3    A Shrubsole England  2008 2020      79            79   266      10\n  RunsConceeded Wickets BowlingAverage  Economy BowlingStrikeRate FourWickets\n1          2089     137       15.24818 5.884507          15.54745           2\n2          2188     114       19.19298 5.581633          20.63158           1\n3          1587     102       15.55882 5.966165          15.64706           2\n  FiveWickets InningsBatted NotOuts RunsScored HighScore HighScoreNotOut\n1           0            40      23        285        33            TRUE\n2           0            67      30        590        42            TRUE\n3           1            19      10        104        29           FALSE\n  BattingAverage Hundreds Fifties Ducks\n1       16.76471        0       0     1\n2       15.94595        0       0     7\n3       11.55556        0       0     2\n```\n\n\n:::\n:::\n\n\n**Checking Missing Values**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Check for missing values\nany(is.na(engwt20))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n**Data Structure**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n#Analysing the structure of the data\n#str(engwt20)\n#summary(engwt20)\n```\n:::\n\n\n**Variable Selection**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n#Variable Selection\ndata_criket <- engwt20 |> \n  select(-Player, -Country, -Start, -End, -HighScoreNotOut)\n\nnames(data_criket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"Matches\"           \"InningsBowled\"     \"Overs\"            \n [4] \"Maidens\"           \"RunsConceeded\"     \"Wickets\"          \n [7] \"BowlingAverage\"    \"Economy\"           \"BowlingStrikeRate\"\n[10] \"FourWickets\"       \"FiveWickets\"       \"InningsBatted\"    \n[13] \"NotOuts\"           \"RunsScored\"        \"HighScore\"        \n[16] \"BattingAverage\"    \"Hundreds\"          \"Fifties\"          \n[19] \"Ducks\"            \n```\n\n\n:::\n:::\n\n\n**PCA**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Applying PCA \npca_res <- prcomp(data_criket, scale = TRUE)\nsummary(pca_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.8712 2.1696 1.4747 0.94427 0.92171 0.77460 0.65405\nProportion of Variance 0.4339 0.2477 0.1145 0.04693 0.04471 0.03158 0.02252\nCumulative Proportion  0.4339 0.6816 0.7961 0.84302 0.88773 0.91931 0.94183\n                          PC8     PC9    PC10    PC11    PC12   PC13    PC14\nStandard deviation     0.5848 0.54428 0.40521 0.37598 0.24851 0.1947 0.16553\nProportion of Variance 0.0180 0.01559 0.00864 0.00744 0.00325 0.0020 0.00144\nCumulative Proportion  0.9598 0.97542 0.98406 0.99150 0.99475 0.9968 0.99819\n                          PC15    PC16    PC17   PC18    PC19\nStandard deviation     0.13488 0.09559 0.05916 0.0441 0.04030\nProportion of Variance 0.00096 0.00048 0.00018 0.0001 0.00009\nCumulative Proportion  0.99915 0.99963 0.99981 0.9999 1.00000\n```\n\n\n:::\n:::\n\nBased on the PCA results, the first two principal components (PC1 and PC2) explain 43.4% and 24.8% of the variance, respectively. Together, they account for 68.2% of the total variance in the dataset. This indicates that while these two components capture a significant portion of the variability, there is still a considerable amount of information contained in the remaining components. The third component (PC3) explains an additional 11.4% of the variance, bringing the cumulative explained variance to 79.6%. This suggests that while the first two components provide a good overview, including the third component would enhance the representation of the data.\n\n**Biplot**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create data frame with PCA scores (PC1 and PC2) and player names\neng_pca_scores <- as_tibble(pca_res$x[, 1:2]) %>%\n  mutate(Player = engwt20$Player)\n\n# Create data frame with PCA loadings (eigenvectors)\neng_pca_loadings <- as_tibble(pca_res$rotation[, 1:2]) %>%\n  mutate(\n    origin = 0,\n    variable = colnames(data_criket),\n    PC1s = PC1 * (pca_res$sdev[1]^2 * 2.5),\n    PC2s = PC2 * (pca_res$sdev[2]^2 * 2.5)\n  )\n\n# Create biplot\nggplot() + \n  geom_segment(data = eng_pca_loadings, \n               aes(x = origin, xend = PC1s, \n                   y = origin, yend = PC2s), \n               colour = \"orange\") +\n  geom_text(data = eng_pca_loadings, \n            aes(x = PC1s, y = PC2s, label = variable),\n            colour = \"orange\", nudge_x = 0.7) +\n  geom_point(data = eng_pca_scores, \n             aes(x = PC1, y = PC2)) +\n  geom_text(data = filter(eng_pca_scores, abs(PC2) > 1.3),\n            aes(x = PC1, y = PC2, label = Player), \n            nudge_y = 0.15, nudge_x = -0.5, size = 3) +\n  xlab(\"PC1\") + ylab(\"PC2\") +\n  ggtitle(\"Biplot of First Two Principal Components (engwt20)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nThe biplot displays the projection of players and performance variables onto the first two principal components, which together explain 68.2% of the total variance in the dataset.\n\n**PC1 – Batting Performance (43.4%)**\nPC1 reflects batting strength, as variables such as RunsScored, BattingAverage, Fifties, and HighScore point strongly to the left. Players like DN Wyatt, TT Beaumont, and HC Knight, known for their batting performance, appear on the negative end of PC1.\n\n**PC2 – Bowling Performance (24.8%)**\nPC2 captures bowling performance, with variables like Wickets, Overs, and Maidens pointing upward. Bowlers such as Sophie Ecclestone, Anya Shrubsole, and K Sciver-Brunt are positioned higher along this axis.\n\n**Structure and Interpretation**\nLonger arrows indicate stronger contributions to the PCs. Batting-related variables cluster together, as do bowling stats, clearly separating players by role. Players near a particular arrow tend to excel in that area, while those near the origin are more balanced.\n\nThe biplot effectively distinguishes between batting and bowling specialists, providing insight into individual player strengths based on their positioning relative to performance metrics.\n\n**Scree Plot**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Extract eigenvalues\neigenvalues <- pca_res$sdev^2\n# Create data frame for ggplot\n\nscree_df <- tibble(\n  PC = 1:length(eigenvalues),\n  Eigenvalue = eigenvalues\n)\n\n# Plot using ggplot2\nggplot(scree_df, aes(x = PC, y = Eigenvalue)) +\n  geom_line(color = \"black\", linewidth = 1) +\n  geom_point(color = \"black\", size = 2) +\n  scale_x_continuous(breaks = seq(1, max(scree_df$PC), by = 2)) +\n  labs(title = \"Scree Plot\", x = \"Number of PCs\", y = \"Eigenvalue\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nAn appropriate number of principal components to retain is four. Together, the first four PCs explain approximately 84.3% of the total variance in the dataset (PC1: 43.4%, PC2: 24.8%, PC3: 11.4%, PC4: 4.7%), which is a strong representation of the original data.\n\nThe scree plot supports this choice, showing a clear drop in eigenvalues from PC1 to PC4, followed by a noticeable flattening of the curve. This pattern indicates that the first four components capture most of the meaningful structure, while additional components contribute very little. Therefore, retaining four components balances simplification with preservation of important information.\n\n\n**Interpret the PCs**\nAs obsersed fromt the biplot earlier, PC1 captures batting ability, with variables like runs scored, batting average, fifties, and high scores contributing most in the negative direction. Players with lower PC1 scores — such as DN Wyatt, TT Beaumont, and HC Knight are positioned on the left side of the biplot, consistent with their strength as top-order batters.\n\nPC2 reflects bowling performance, driven by variables like wickets, overs bowled, maidens, and four-wicket hauls. Players like Sophie Ecclestone, Anya Shrubsole, and K Sciver-Brunt rank higher on this axis, highlighting their roles as frontline bowlers. Overall, the PCA offers a clear separation between batting and bowling specialists, aligning well with known player profiles.\n\n**Suggestions to improve analysis** \n\n1. Analyse outliers explicitly:\nPlayers like Sophie Ecclestone or K Sciver-Brunt dominate multiple performance categories. Applying PCA or visualising outliers separately could help uncover patterns among the rest of the players who may otherwise be overshadowed.\n\n2. Separate batting and bowling PCA: \nAnalyzing batting and bowling stats separately would provide clearer insights, especially since not all players contribute equally in both areas.\n\n3. New variable- CareerLength:\nThe dataset contains Start and End years for each player. Creating a CareerLength variable (End - Start) and including it in PCA or as a colour group would help identify whether performance trends are related to experience or era.\n\n4. Zero contribution:\nSeveral players have zero values in either batting or bowling columns. Instead of including all players in a single PCA, filtering out players who never bowled or batted, and carrying out the analysis might provide alternative perspectives.\n\n## References\n\n1.Hadley Wickham, Dianne Cook, Heike Hofmann, Andreas Buja\n  (2011). tourr: An R Package for Exploring Multivariate\n  Data with Projections. Journal of Statistical Software,\n  40(2), 1-18. URL http://www.jstatsoft.org/v40/i02/.\n  \n2.Lecture and tuorial notes and codes snippets from ETC3250/5250, 2025, Monash University\n  \n3.Womens Cricket: https://en.wikipedia.org/wiki/Australia_women%27s_national_cricket_team\n  \n4.OpenAI (2023). ChatGPT (version 3.5) [Large language model]. \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}