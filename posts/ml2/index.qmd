---
title: "Principle Component Analysis and Classifiers"
author: "Pooja Rajendran Raju"
date: "2025-05-04"
quarto-required: ">=1.3.0"
output:
    html:
        embed-resources: true
categories: [Machine Learning]
---


```{r}
#| echo: false
# Set up chunk for all slides
knitr::opts_chunk$set(
  fig.width = 6,
  fig.height = 4,
  fig.align = "center",
  out.width = "100%",
  code.line.numbers = FALSE,
  fig.retina = 4,
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  dev.args = list(pointsize = 11)
)
```

```{r}
#| echo: false
# Load libraries used everywhere
library(tidyverse)
library(tidymodels)
library(conflicted)
library(colorspace)
library(patchwork)
library(MASS)
library(randomForest)
library(gridExtra)
library(GGally)
library(geozoo)
library(mulgar)
library(kableExtra)
library(knitr)
library(tourr)    
library(uwot)    
library(dplyr)
library(infer)
library(patchwork)
library(nullabor)
library(gifski)
library(yardstick)
library(ggplot2)
library(dials)
library(ranger)
library(xgboost)
library(rpart)
library(discrim)
library(classifly)
library(detourr)
library(crosstalk)
library(plotly)
library(viridis)
library(colorspace)
library(conflicted)
library(boot)
theme_set(theme_bw(base_size = 14) +
   theme(
     aspect.ratio = 1,
     plot.background = element_rect(fill = 'transparent', colour = NA),
     plot.title.position = "plot",
     plot.title = element_text(size = 24),
     panel.background = element_rect(fill = 'transparent', colour = NA),
     legend.background = element_rect(fill = 'transparent', colour = NA),
     legend.key = element_rect(fill = 'transparent', colour = NA)
   )
)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
conflicts_prefer(dplyr::slice)
conflicts_prefer(palmerpenguins::penguins)
conflicts_prefer(tourr::flea)
```

This project explores two areas of statistical learning: understanding structure in multivariate data using resampling, and comparing modelling approaches for a supervised classification task. In the first part, the focus is on the women’s cricket dataset, where principal component analysis (PCA) is used to identify the main sources of variation after removing non-numeric variables. Bootstrap resampling and permutation tests are then used to assess which variables contribute most strongly to the leading components and whether observed relationships between variables are stronger than what would be expected by chance.

The second part shifts to a classification problem involving time-series features from birdsongs and financial data. Here, several classifiers: logistic regression, linear discriminant analysis, decision trees, random forests, and boosted trees are fitted and evaluated. The analysis examines model performance, tuning behaviour, and how well each approach captures the patterns that differentiate the two classes.

#### Bootstrapping and permuting your way to provide evidence 

This section applies PCA to the women’s cricket data (after removing irrelevant variables) to identify the main directions of variation in player performance. Bootstrap resampling is then used to determine which variables consistently contribute to PC1 and PC2, providing stability insight beyond the original PCA loadings.

**Data Wrangling**

```{r}
#| echo: true 
#| code-fold: true
# Load the data

engwt20 <- read.csv("engwt20.csv")
```

**Checking Missing Values**
```{r}
#| echo: true 
#| code-fold: true

# Check for missing values
any(is.na(engwt20))
```

**Data Structure**
```{r}
#| echo: true 
#| code-fold: true

#Analysing the structure of the data
#str(engwt20)
#summary(engwt20)
```

**Variable Selection**
```{r}
#| echo: true 
#| code-fold: true

#Variable Selection
data_criket <- engwt20 |> 
  select(-Player, -Country, -Start, -End, -HighScoreNotOut, -Matches, -InningsBowled, -Overs, -InningsBatted, -FiveWickets, -Hundreds)

names(data_criket)
```

**PCA**
```{r}
#| echo: true 
#| code-fold: true

# Applying PCA 
pca_res <- prcomp(data_criket, scale = TRUE, center = TRUE)
summary(pca_res)
pca_res$rotation[, 1:2]  

```

PC1 explains 39.2% of the total variance and reflects overall player performance. It has strong negative loadings from batting stats like NotOuts, RunsScored, HighScore, and BattingAverage, alongside bowling variables such as Wickets, RunsConceeded, and Maidens. Since both batting and bowling contribute in the same direction, lower PC1 scores indicate players who perform well in both areas.

PC2 accounts for 27.5% of the variance and highlights a contrast between bowling and batting performance. Bowling variables like Economy, BowlingAverage, BowlingStrikeRate, and Wickets have strong positive loadings, while batting stats such as RunsScored, HighScore, and BattingAverage load negatively. Higher PC2 scores therefore indicate players who are stronger bowlers relative to their batting ability

**Scree Plot**
```{r}
#| echo: true 
#| code-fold: true

# Extract eigenvalues
eigenvalues <- pca_res$sdev^2
# Create data frame for ggplot

scree_df <- tibble(
  PC = 1:length(eigenvalues),
  Eigenvalue = eigenvalues
)

# Plot using ggplot2
ggplot(scree_df, aes(x = PC, y = Eigenvalue)) +
  geom_line(color = "black", linewidth = 1) +
  geom_point(color = "black", size = 2) +
  scale_x_continuous(breaks = seq(1, max(scree_df$PC), by = 2)) +
  labs(title = "Scree Plot", x = "Number of PCs", y = "Eigenvalue") +
  theme_minimal()
```

The elbow in the scree plot appears at the 4th component, with the first three PCs explaining over 81% of the total variance (PC1: 39.2%, PC2: 27.5%, PC3: 15.1%). Since the eigenvalues flatten out after that, adding more PCs wouldn’t capture much additional information and variance. Moving forward we will focus only on the first two components.




**Biplot**
```{r}
#| echo: true 
#| code-fold: true

# Create data frame with PCA scores (PC1 and PC2) and player names
eng_pca_scores <- as_tibble(pca_res$x[, 1:2]) |>
  mutate(Player = engwt20$Player)

# Create data frame with PCA loadings (eigenvectors)
eng_pca_loadings <- as_tibble(pca_res$rotation[, 1:2]) |>
  mutate(
    origin = 0,
    variable = colnames(data_criket),
    PC1s = PC1 * (pca_res$sdev[1]^2 * 2.5),
    PC2s = PC2 * (pca_res$sdev[2]^2 * 2.5)
  )

# Create biplot
ggplot() + 
  geom_segment(data = eng_pca_loadings, 
               aes(x = origin, xend = PC1s, 
                   y = origin, yend = PC2s), 
               colour = "orange") +
  geom_text(data = eng_pca_loadings, 
            aes(x = PC1s, y = PC2s, label = variable),
            colour = "orange", nudge_x = 0.7) +
  geom_point(data = eng_pca_scores, 
             aes(x = PC1, y = PC2)) +
  geom_text(data = filter(eng_pca_scores, abs(PC2) > 1.3),
            aes(x = PC1, y = PC2, label = Player), 
            nudge_y = 0.15, nudge_x = -0.5, size = 3) +
  xlab("PC1") + ylab("PC2") +
  ggtitle("Biplot of First Two Principal Components (engwt20)") +
  theme_minimal()

```

- Variables like Economy, BowlingAverage, and BowlingStrikeRate point strongly upward, indicating they contribute most to PC2 and represent bowling efficiency. Meanwhile, variables such as RunsScored, HighScore, BattingAverage, and Fifties point sharply to the left, confirming they primarily influence PC1 and reflect batting performance. 
- The long arrow lengths suggest that these variables are well captured in the two-dimensional PC space and play a key role in defining the player profiles seen in the plot.
- Players positioned far to the left, like DN Wyatt, tend to have strong batting performances across multiple metrics. Those higher on the plot, such as S Ecclestone and K Sciver-Brunt, are likely more efficient bowlers, as they align closely with bowling related variables on PC2.

**Bootstrapping PCA Loadings**

```{r}
#| echo: true
#| code-fold: true


# Bootstrap function for PC1
compute_PC1 <- function(data, index) {
  pc1 <- prcomp(data[index, ], center = TRUE, scale. = TRUE)$rotation[, 1]
  if (sign(pc1[1]) < 0) pc1 <- -pc1
  return(pc1)
}

set.seed(2025)
PC1_boot <- boot(data = scale(data_criket), statistic = compute_PC1, R = 1000)
colnames(PC1_boot$t) <- colnames(data_criket)

# Summarise PC1 bootstrap 
PC1_boot_ci <- as_tibble(PC1_boot$t) |>
  pivot_longer(everything(), names_to = "var", values_to = "coef") |>
  group_by(var) |>
  summarise(
    q2.5 = quantile(coef, 0.025),
    median = median(coef),
    q97.5 = quantile(coef, 0.975)
  ) |>
  mutate(t0 = PC1_boot$t0)

PC1_boot_ci

```

**95% Bootstrap Confidence Intervals for PC1 Loadings**

```{r}
#| echo: true
#| code-fold: true

pb_pc1 <- ggplot(PC1_boot_ci, aes(x = var, y = t0)) + 
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = q2.5, ymax = q97.5), width = 0.1) +
  xlab("") + ylab("Loading") +
  ggtitle("PC1 Loadings with 95% Bootstrap CI") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
pb_pc1

```

**Bootstrap Analysis of PC2 Loadings**
```{r}
#| echo: true
#| code-fold: true

# Bootstrap function for PC2
compute_PC2 <- function(data, index) {
  pc2 <- prcomp(data[index, ], center = TRUE, scale. = TRUE)$rotation[, 2]
  if (sign(pc2[1]) < 0) pc2 <- -pc2
  return(pc2)
}

set.seed(2025)
PC2_boot <- boot(data = scale(data_criket), statistic = compute_PC2, R = 1000)
colnames(PC2_boot$t) <- colnames(data_criket)

# Summarise PC2 bootstrap results
PC2_boot_ci <- as_tibble(PC2_boot$t) |>
  pivot_longer(everything(), names_to = "var", values_to = "coef") |>
  group_by(var) |>
  summarise(
    q2.5 = quantile(coef, 0.025),
    median = median(coef),
    q97.5 = quantile(coef, 0.975)
  ) |>
  mutate(t0 = PC2_boot$t0)

PC2_boot_ci
```


**95% Bootstrap Confidence Intervals for PC2 Loadings**
```{r}
#| echo: true
#| code-fold: true

pb_pc2 <- ggplot(PC2_boot_ci, aes(x = var, y = t0)) + 
  geom_hline(yintercept = 0, linetype = 2, colour = "red") +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = q2.5, ymax = q97.5), width = 0.1) +
  xlab("") + ylab("Loading") +
  ggtitle("PC2 Loadings with 95% Bootstrap CI") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
pb_pc2

```

Based on the analysis, PC1 reflects overall activity, with large contributions from both batting (NotOuts, RunsScored, HighScore) and bowling (Wickets, RunsConceded), as all loadings share the same direction. However, when analysing the bootstrap 95% confidence intervals, only FourWickets, Maidens, RunsConceded, and Wickets show intervals above zero, which indicates these four bowling metrics are the most stable and significantly associated with PC1. In contrast, key batting variables like RunsScored, BattingAverage, and HighScore cross zero, suggesting their influence on PC1 is weaker and not consistently reflected across resamples.

Whereas in PC2, The original PCA loadings show a clear contrast between bowling and batting as in bowling metrics have positive loadings, while batting metrics are negative. This separation suggests that PC2 distinguishes bowlers from batters. The bootstrap confidence intervals support this finding, intervals for bowling-related variables such as Economy, Wickets, BowlingAverage, BowlingStrikeRate, RunsConceeded and Maidens remain fully above zero, indicating a consistent and significant contribution to PC2. In contrast, intervals for batting metrics like RunsScored, HighScore, BattingAverage, and Fifties are negative or cross zero, suggesting their influence on PC2 is weaker and  contribution is less stable.

#### Observing Correlations Through Permutation Tests and Lineups

Here we investigate whether observed correlations in the cricket data are stronger than what would occur under randomness. Using permutation tests, null distributions, and lineup plots, we assess the strength, visual detectability, and statistical significance of relationships such as RunsScored–HighScore.

**RunsScored vs HighScore** 

```{r}
#| echo: true
#| code-fold: true

set.seed(201)
# Original correlation
original_corr <- data_criket |>
  summarise(correlation = cor(RunsScored, HighScore)) |>
  pull(correlation)

# Single permutation
df_perm <- data_criket |>
  specify(RunsScored ~ HighScore) |>
  hypothesize(null = "independence") |>
  generate(reps = 1, type = "permute")

data_with_perm <- data_criket |>
  mutate(Permuted_RunsScored = df_perm$RunsScored)

# Plot the Original Data
p1 <- ggplot(data_criket, aes(x = RunsScored, y = HighScore)) +
  geom_point(alpha = 0.5, color = "blue") +
  labs(
    title = "Original Data",
    subtitle = paste0("Original Correlation: ", round(original_corr, 3)),
    x = "RunsScored",
    y = "HighScore"
  ) +
  theme_minimal()

# Permuted plot
p2 <- ggplot(data_with_perm, aes(x = Permuted_RunsScored, y = HighScore)) +
  geom_point(alpha = 0.5, color = "red") +
  labs(
    title = "Permuted Data",
    subtitle = paste0("Permuted Correlation: ", round(cor(data_with_perm$Permuted_RunsScored, data_criket$HighScore), 3)),
    x = "Permuted RunsScored",
    y = "HighScore"
  ) +
  theme_minimal()

# Combine with patchwork
p1 | p2




```

- The Original Data plot shows a strong positive linear relationship between RunsScored and HighScore with a correlation of 0.851, indicating that as runs scored increase, high scores also tend to rise proportionally. The points are tightly clustered along an upward trend, reflecting this strong association.
- In contrast, when the data is permuted, this relationship no longer holds, as shown by the randomly scattered points. The correlation value drops significantly to 0.027, showing random scattering of points with no clear trend.
- The comparison between the original and permuted plots illustrates that the correlation observed in the original data is unlikely to be from random chance, otherwise the permuted data would show a similar pattern, but it clearly does not. Hence, we can infer that the relationship between RunsScored and HighScore is statistically significant and meaningful.

**Permutation Distribution**

```{r}
#| echo: true
#| code-fold: true

# Set seed
set.seed(123)

# Permutation distribution
perm_cor <- data_criket |>
  specify(RunsScored ~ HighScore) |>
  hypothesize(null = "independence") |>
  generate(reps = 5000, type = "permute") |>
  calculate(stat = "correlation")

# Plot permutation 
ggplot(perm_cor, aes(x = stat)) +
  geom_histogram(bins = 40, fill = "skyblue", color = "white") +
  geom_vline(xintercept = original_corr, color = "red", linewidth = 1) +
  annotate("text", x = original_corr, y = 400, 
           label = sprintf("Observed correlation: %.3f", original_corr),
           color = "red", hjust = 1.1, vjust = 1) +
  scale_x_continuous(breaks = seq(-0.5, 1, by = 0.1)) +
  labs(
    title = "Permutation Test: RunsScored vs HighScore",
    x = "Permuted Correlations",
    y = "Frequency"
  ) +
  theme_minimal()

# One-sided p-value
p_value_1 <- perm_cor |>
  summarise(p = mean(stat >= original_corr)) |>
  pull(p)

original_corr
p_value_1

```

Observed correlation: 0.851, indicating a very strong positive relationship. This means that players who score more total runs are likely to have higher individual top scores as well.
Null hypothesis: There is no association between RunsScored and HighScore.
Permutation result: After 5000 permutations under null hypothesis, none of the permuted correlations reach/exceed the observed value. The permutation distribution of correlations is approximately centered around 0, as expected if there were no true relationship.
P-value: 0.00, indicating strong evidence against the null hypothesis.

-  The observed correlation (red line) is far to the right of the permutation distribution, highlighting that it is statistically significant and not due to random variation. This strongly supports the existence of a genuine association between RunsScored and HighScore.

**Line up**

```{r}
#| echo: true
#| code-fold: true

set.seed(912)
df_l <- lineup(null_permute('HighScore'), data_criket)



ggplot(df_l, aes(x=HighScore, y=RunsScored)) + 
  geom_point() + 
  facet_wrap(~.sample)
```
Plot 15 clearly stands out from the lineup, shows a structured upward trend that is distinct compared to the other plots. This indicates that the observed relationship between RunsScored and HighScore is strong enough to be visually identifiable and the correlation is statistically significant. The true data plot is 15. 


**RunsScored vs RunsConceeded**

```{r}
#| echo: true
#| code-fold: true


set.seed(123)

# Original correlation
original_corr_con <- data_criket |>
  summarise(correlation = cor(RunsScored, RunsConceeded)) |>
  pull(correlation)

original_corr_con

# Hypothesize null of independence and permute
df_perm_con <- data_criket |>
  specify(RunsScored ~ RunsConceeded) |>
  hypothesize(null = "independence") |>
  generate(reps = 1, type = "permute") 

perm_data <- data_criket |>
  mutate(Permuted_RunsScored = df_perm_con$RunsScored)

# Original plot
p1_conceded <- ggplot(data_criket, aes(x = RunsScored, y = RunsConceeded)) +
  geom_point(alpha = 0.5, color = "blue") +
  labs(
    title = "Original Data",
    subtitle = paste0("Original Correlation: ", round(original_corr_con, 3)),
    x = "RunsScored",
    y = "RunsConceded"
  ) +
  theme_minimal()

# Permuted plot
p2_conceeded <- ggplot(perm_data, aes(x = Permuted_RunsScored, y = RunsConceeded)) +
  geom_point(alpha = 0.5, color = "red") +
  labs(
    title = "Permuted Data",
    subtitle = paste0("Permuted Correlation: ", round(cor(perm_data$Permuted_RunsScored, perm_data$RunsConceeded), 3)),
    x = "Permuted RunsScored",
    y = "RunsConceded"
  ) +
  theme_minimal()


p1_conceded | p2_conceeded


```
- The Original Data plot shows a weak positive relationship between RunsScored and RunsConceeded, with a correlation of 0.21. The points are broadly scattered, with a few large outliers. This indicates that as the number of runs scored increases, there is a slight tendency for more runs to be conceded, but the relationship is not strong.
- In the Permuted Data plot, the correlation drops to 0.01, suggesting almost no association. However, the distribution of points still resembles the original pattern, with most points clustered near zero and a few large outliers. The fact that the structure remains similar despite permutation indicates that the original association is quite weak and may be influenced more by the distribution of the data.
To confirm if the observed correlation of 0.21 is statistically significant and not just a product of the underlying distribution, a 5000 permutation test and evaluation of p-value is carried out.

**Permutation Distribution**

```{r}
#| echo: true
#| code-fold: true

set.seed(201)

# Step 2: Perform 5000 permutations
perm_cor_con <- data_criket |>
  specify(RunsScored ~ RunsConceeded) |>
  hypothesize(null = "independence") |>
  generate(reps = 5000, type = "permute") |>
  calculate(stat = "correlation")

# Step 3: Plot the permutation distribution
ggplot(perm_cor_con, aes(x = stat)) +
  geom_histogram(bins = 40, fill = "lightblue", color = "white") +
  geom_vline(xintercept = original_corr_con, color = "red", linewidth = 1) +
  annotate("text", x = original_corr_con, y = 400, 
           label = sprintf("Observed correlation: %.3f", original_corr_con),
           color = "red", hjust = -0.1, vjust = -0.1) +
  scale_x_continuous(breaks = seq(-0.5, 0.5, by = 0.1)) +
  labs(
    title = "Permutation Test: RunsScored vs RunsConceeded",
    x = "Permuted Correlations",
    y = "Frequency"
  ) +
  theme_minimal()

# Step 4: Calculate the one-sided p-value
p_value_con <- perm_cor_con |>
  summarise(p = mean(stat >= original_corr_con)) |>
  pull(p)

# Display results
original_corr_con
p_value_con

```

Observed correlation: 0.210 -  a weak positive relationship.
Null hypothesis: There is no association between RunsScored and RunsConceeded.
Permutation result: 6.7% of permuted correlations were as extreme or more than the observed value.
P-value: 0.0674
Interpretation:
The observed correlation is weak and not statistically significant, as the p-value exceeds the 5% threshold. The similar scatter patterns in the permuted plot suggest the association may be influenced more by the distribution of RunsConceeded rather than a true underlying relationship.
Conclusion:
There is weak evidence of a positive association, but it is not strong enough to confidently rule out random chance as a possible explanation.

**Line up**

```{r}
#| echo: true
#| code-fold: true

set.seed(912)
df_l <- lineup(null_permute('RunsConceeded'), data_criket)



ggplot(df_l, aes(x=RunsConceeded, y=RunsScored)) + 
  geom_point() + 
  facet_wrap(~.sample)
```

In the lineup plot for RunsScored versus RunsConceded, the true data plot is positioned at 15. Unlike the clear distinction observed with RunsScored and HighScore, the data plot is not visually distinctive from the other plots. This lack of clear separation suggests that the association between RunsScored and RunsConceded is not strong enough to be easily detected visually, as observed above the relationship is weak and may be influenced by randomness.



#### How well can we build a simple classifier? 

Here we examine how birdsong and financial time-series features differ through visualisations, grand tours, and plots. The goal is to understand class separation and assess assumptions such as normality, equal variances and independence required for linear classifiers.

**Data Analysis**

```{r}
#| echo: true
#| code-fold: true

#load data 
t_data <- read_csv("finance_and_birds.csv")
glimpse(t_data)
summary(t_data)
```

```{r}
#| echo: true
#| code-fold: true

# Check for missing values
sum(is.na(t_data))

# Summary statistics
t_data %>%
  summarise(across(where(is.numeric), list(mean = mean, sd = sd, min = min, max = max), na.rm = TRUE))

# Check the class distribution
t_data %>% count(type)


# Set the levels explicitly
t_data <- t_data |> mutate(type = factor(type, levels = c("birdsongs", "finance")))


levels(t_data$type)

```

**Boxplots: Distribution of Variables by Type**
```{r}
#| echo: true
#| code-fold: true

# Box plot of numerical variables grouped by class
t_data %>%
  pivot_longer(cols = -type, names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = type, y = value, fill = type)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free") +
  theme_minimal() +
  labs(title = "Distribution of Variables by Type")

```

The boxplots reveal clear differences between birdsongs and finance time series across several features. Linearity and covariate2 display the most noticeable separations—finance time series tend to have higher values and a wider spread in both features, making them strong candidates for classification. Entropy also shows some separation, with birdsongs generally having slightly higher median values and finance exhibiting more variability. In contrast, covariate1 and x_acf1 are quite similar across both types, suggesting they may not be as useful for classification. These findings indicate that linearity, covariate2, and entropy could be the strongest predictors moving forward.



**Scatterplot Matrix**
```{r}
#| echo: true
#| code-fold: true

# Convert the 'type' to a factor
t_data <- t_data %>%
  mutate(type = as.factor(type))

# Improved ggpairs plot
ggpairs(t_data,
        columns = c(1,2,3,5,6),
        aes(color = type, alpha = 0.6),
        lower = list(continuous = wrap("points", size = 0.6)),
        diag = list(continuous = wrap("densityDiag", alpha = 0.4)),
        upper = list(continuous = wrap("cor", size = 3))) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

- Linearity: Both bird songs and finance time series are centered around zero with minimal spread, showing no visible separation.
- Entropy: Finance series have a strong negative correlation (-0.539), while birdsongs are mostly uncorrelated (0.055). Finance tends to have lower entropy, while birdsongs are skewed towards higher values.
- x_acf1: Finance displays a positive correlation (0.493) and a wider spread, suggesting stronger autocorrelation patterns compared to birdsongs, which show no correlation (-0.002).
- Covariate1: Minimal differences are observed; distributions are almost identical between the two types, with near-zero correlations.
- Covariate2: Finance has a slight positive correlation (0.023) and is skewed towards higher values, while birdsongs show a slight negative correlation (-0.044).

Overall, entropy and x_acf1 show the strongest separation, with finance exhibiting clear negative correlations with entropy and positive correlations with x_acf1. Covariate2 shows moderate distinction, while linearity and covariate1 display minimal separation.


 
**Assumptions** 

**Tourr**

```{r}
#| echo: true
#| code-fold: true
#| message: false
#| warning: false



# Standardize the numerical columns
data_standardized <- t_data %>%
  mutate(across(where(is.numeric), scale))

# Data for the tour
tour_data <- data_standardized %>%
  select(linearity, entropy, x_acf1, covariate1, covariate2)

#x11()
#animate_xy(tour_data, col = as.factor(data_standardized$type), axes = "bottomleft",
#           rescale=TRUE)

#set.seed(645)
render_gif(tour_data,
           grand_tour(),
           display_xy(half_range = 3.8,
           axes="bottomleft", cex=1.1,
           col = as.factor(data_standardized$type)),
           gif_file = "tour_fb.gif",
           apf = 1/60,
           frames = 1800,
           width = 600, 
           height = 600)

knitr::include_graphics("tour_fb.gif")

```

## Grand Tour

The grand tour reveals how the five features interact across varying projections. While some projections reveal partial separation between birdsongs and financial time series, there remains a significant overlap in many views. This overlap, along with the irregular and unequal spread of each class, suggests that the assumptions of multivariate normality and equal variance-covariance matrices are not fully satisfied. The finance series tend to cluster more tightly, whereas birdsongs exhibit more variation, particularly in entropy and x_acf1. Although classification is still possible, the way the data breaks some of LDA’s assumptions suggests that more flexible models, like decision trees or ensemble methods might handle it more effectively.

**1. Multivariate Normality**

```{r}
library(ggplot2)
library(patchwork)

# List of predictors 
predictors <- setdiff(names(t_data), 'type')
plots <- list()

# QQ plots by class
for (predictor in predictors) {
  plot <- ggplot(t_data, aes(sample = .data[[predictor]], color = type)) +
    stat_qq() +
    stat_qq_line() +
    labs(title = paste("QQ Plot for", predictor, "by Class")) +
    theme_minimal() +
    theme(legend.position = "bottom")

  plots[[predictor]] <- plot
}

#  QQ plots
wrap_plots(plots, ncol = 3)

```

The QQ plots revealed that linearity and covariate2 exhibited heavy tails and significant deviations from normality, while entropy and x_acf1 also deviated, particularly at the tails. In contrast, covariate1 was the closest to normality with its points aligning well with the theoretical line. Therefore, the multivariate normality assumption is not fully satisfied. 

**2. Equal Variance-Covariance Matrices**

The scatterplot matrix above reveals noticeable differences in spread and separation between birdsongs and finance, particularly in entropy, x_acf1, and covariate2. These disparities indicate that the assumption of equal variance-covariance matrices is not met, suggesting LDA may produce biased decision boundaries due to differing variance structures across classes.


**3. Independence**
The dataset is ordered with 500 finance samples followed by 474 birdsongs samples. While this structure might suggest some sequencing, it doesn't automatically imply dependency. Since there are no clear time-based indicators or hierarchical groupings, we can assume the independence assumption is satisfied.



#### Train-Test Split

A stratified train–test split is used to preserve the class balance between birdsongs and finance data. This prepares a fair setting for training models and evaluating their out of sample performance.

**Split the data into training and testing sets**
```{r}
#| echo: true
#| code-fold: true
set.seed(42)  

# Data Split
data_split <- initial_split(t_data, prop = 2/3, strata = type)
train_data <- training(data_split)
test_data <- testing(data_split)

train_data |> count(type)
test_data |> count(type)


```


**Distribution of Classes in Training and Testing Sets**

```{r}
#| echo: true
#| code-fold: true

# Training Set Distribution
train_plot <- train_data |> 
  count(type) |> 
  ggplot(aes(x = type, y = n, fill = type)) +
  geom_bar(stat = "identity") +
  labs(title = "Training Set Distribution", y = "Count", x = "Type") +
  theme_minimal()

# Testing Set Distribution
test_plot <- test_data |> 
  count(type) |> 
  ggplot(aes(x = type, y = n, fill = type)) +
  geom_bar(stat = "identity") +
  labs(title = "Testing Set Distribution", y = "Count", x = "Type") +
  theme_minimal()


train_plot + test_plot

```

The distribution looks good and well-balanced across the training and testing datasets, which means the stratified sampling worked correctly. The bars are nearly equal for both birdsongs and finance, indicating there is no class imbalance.

#### Exploring Linear Classifiers

**Fitting a linear classifier to the training data**

Linear models are fitted to study how well simple decision boundaries separate the two time-series classes. Coefficients and discriminant directions are interpreted to understand which features drive model decisions.

**Logistic model**

```{r}
#| echo: true
#| code-fold: true



# logistic regression model
logistic_mod <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification")

# Fit the model
logistic_fit <- logistic_mod |> 
  fit(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, data = train_data)

# Display the model coefficients
tidy(logistic_fit)
glance(logistic_fit) 
```

- Linearity (Estimate = 0.052, p < 0.001):
Slightly boosts the chances of classifying the series as finance. This means more linear time series are more likely to be financial data.
- Entropy (Estimate = -2.638, p < 0.001):
Has a strong negative effect, making it much more likely for the series to be classified as birdsongs. It reflects the chaotic and unpredictable nature typical of birdsong patterns compared to finance.
- x_acf1 (Estimate = -0.460, p < 0.05):
Shows a moderate negative link with finance. Higher autocorrelation seems to be more common in birdsongs than in financial data.
- Covariate1 (Estimate = 0.019, p = 0.93):
Almost no effect on classification, it’s statistically insignificant. It doesn't really help in telling birdsongs and finance apart.
- Covariate2 (Estimate = 1.355, p < 0.001):
Strongly leans towards identifying finance. It suggests that financial time series are more structured and consistent.

In conclusion, the logistic regression model reveals entropy as the strongest predictor for birdsongs, reflecting their randomness, while covariate2 distinguishes financial data due to its structured patterns. Linearity and x_acf1 provide moderate contribution, and covariate1 is redundant. This indicates birdsongs are more disordered, whereas finance data are more structured.



**LDA **
```{r}
#| echo: true
#| code-fold: true

# Load necessary libraries
library(discrim)
library(tidymodels)

# Define the LDA model specification
lda_spec <- discrim_linear() |>
  set_mode("classification") |>
  set_engine("MASS", prior = c(0.5, 0.5))

# Fit the LDA model to the training data
lda_fit <- lda_spec |> 
  fit(type ~ linearity + entropy + x_acf1 + covariate1 + covariate2, data = train_data)

# Display the fitted model
lda_fit

```

- Group Means Analysis:
Financial time series have higher average values for linearity (17.285) and covariate2 (2.4), indicating more structured and consistent patterns. Birdsongs exhibit higher entropy (0.84), suggesting more randomness and disorder.
- Linear Discriminant Coefficients:
Entropy has the strongest negative coefficient (-1.385), reinforcing its role in distinguishing birdsongs due to their unpredictable nature.
Covariate2 (0.764) and linearity (0.020) positively influence classification towards finance, aligning with their structured behavior.
- x_acf1 (-0.237) is moderately negative, slightly leaning towards birdsongs, but less influential than entropy.
- Covariate1 (0.034) shows a minimal positive influence, suggesting it has a negligible impact on classification.
- Prior Probabilities:
Both classes are equally weighted (0.5), reflecting balanced consideration during classification.

The LDA model leverages entropy as the most powerful discriminator for birdsongs, while linearity and covariate2 drive the classification towards finance. This reinforces the understanding that financial data are more structured, while birdsongs are more random and disordered.

#### Evaluating Classifier Performance

**Evaluate the model** 

Using confusion matrices, prediction probabilities, and misclassification patterns, we assess how logistic regression and LDA perform on both training and testing sets. This shows where linear models succeed and where class overlap leads to errors.

**Trainig data evaluation**

_1. Predictions_ 
```{r}
#| echo: true
#| code-fold: true

# Generate predictions for the Logistic Regression model
logistic_train_pred <- predict(logistic_fit, new_data = train_data, type = "class")
logistic_train_pred <- bind_cols(train_data, logistic_train_pred)

# Generate predictions for the LDA model
lda_train_pred <- predict(lda_fit, new_data = train_data, type = "class")
lda_train_pred <- bind_cols(train_data, lda_train_pred)

# Display the first few rows
head(logistic_train_pred)
head(lda_train_pred)

```
Based on the initial observations from the training data, both the logistic regression and LDA models misclassified the first "birdsongs" instance as "finance." For the remaining rows, both models correctly identified "birdsongs." This indicates a shared misclassification pattern for the first observation, but overall alignment with the true class for the rest of the initial data. No strong bias towards either class is evident from this small sample.

_2. Confusion Matrix_

a. Logistic Regression Confusion Matrix

```{r}
#| echo: true
#| code-fold: true

# confusion matrix for Logistic Regression
logistic_conf_matrix <- logistic_train_pred |>
  count(type, .pred_class) |>
  group_by(type) |>
  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>
  pivot_wider(names_from = .pred_class, values_from = n, values_fill = 0) |>
  select(type, birdsongs, finance, Accuracy)


logistic_conf_matrix

```

The logistic regression model demonstrates strong performance on the training data, achieving an accuracy of 83% for birdsongs and 80% for finance. Among the birdsongs, 262 instances were correctly classified, while 54 were misclassified as finance. For the finance category, 266 were correctly identified, with 67 incorrectly labeled as birdsongs. The model shows a slight bias towards better identification of birdsongs, although this difference is minimal. Overall, the misclassifications are relatively balanced, with finance being slightly more prone to confusion with birdsongs.


b. LDA Confusion Matrix

```{r}
#| echo: true
#| code-fold: true

# confusion matrix for LDA
lda_conf_matrix <- lda_train_pred |>
  count(type, .pred_class) |>
  group_by(type) |>
  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>
  pivot_wider(names_from = .pred_class, values_from = n, values_fill = 0) |>
  select(type, birdsongs, finance, Accuracy)


lda_conf_matrix

```
The LDA model shows a strong performance on the training data, achieving an 86% accuracy for birdsongs and 78% accuracy for finance. In the birdsongs category, 272 instances were correctly classified, while 44 were misclassified as finance. For finance, 261 instances were correctly identified, with 72 incorrectly labeled as birdsongs. Compared to logistic regression, LDA slightly improves accuracy for birdsongs but has a marginally lower accuracy for finance. The model seems to better differentiate birdsongs while facing slightly more difficulty with finance series.

**Test data evaluation**

_1. Predictions_ 
```{r}
#| echo: true
#| code-fold: true

# Generate predictions for the Logistic Regression model on test data
logistic_test_pred <- predict(logistic_fit, new_data = test_data, type = "class")

# Bind the predictions to the test data
logistic_test_pred <- bind_cols(test_data, logistic_test_pred)

# Display the first few rows
head(logistic_test_pred)

lda_test_pred <- predict(lda_fit, new_data = test_data, type = "class")

# Bind the predictions to the test data
lda_test_pred <- bind_cols(test_data, lda_test_pred)

# Display the first few rows
head(lda_test_pred)

```
Looking at the first few predictions, both the Logistic Regression and LDA models seem to confidently classify "birdsongs" correctly. This indicates that, at least for this subset, both models are picking up on the right features to distinguish birdsongs. Confusion matrix should help us understand the braoder performance and classification when it comes to finance.

_2. Confusion Matrix_

a. Logistic Regression Confusion Matrix

```{r}
#| echo: true
#| code-fold: true

# Generate the confusion matrix for Logistic Regression
logistic_test_conf_matrix <- logistic_test_pred |>
  count(type, .pred_class) |>
  group_by(type) |>
  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>
  pivot_wider(names_from = ".pred_class", values_from = n, values_fill = 0) |>
  select(type, birdsongs, finance, Accuracy)

# Display the confusion matrix
logistic_test_conf_matrix

logistic_accuracy <- accuracy(logistic_test_pred, truth = type, estimate = .pred_class)$.estimate
logistic_accuracy
```

The confusion matrix for the Logistic Regression model shows strong performance for Birdsongs, with 134 correctly classified and only 24 misclassified, resulting in an 85% accuracy. For Finance, the model correctly identified 122 instances but misclassified 45 as birdsongs, achieving 73% accuracy. This suggests the model is more effective at identifying birdsongs, while it struggles a bit more with distinguishing finance time series, possibly due to overlapping features.


b. LDA Confusion Matrix

```{r}
#| echo: true
#| code-fold: true


# Generate the confusion matrix for LDA
lda_test_conf_matrix <- lda_test_pred |>
  count(type, .pred_class) |>
  group_by(type) |>
  mutate(Accuracy = n[.pred_class == type] / sum(n)) |>
  pivot_wider(names_from = ".pred_class", values_from = n, values_fill = 0) |>
  select(type, birdsongs, finance, Accuracy)

# Display the confusion matrix
lda_test_conf_matrix

lda_accuracy <- accuracy(lda_test_pred, truth = type, estimate = .pred_class)$.estimate

lda_accuracy
```

The confusion matrix for the LDA model on the test data shows a solid performance for Birdsongs, with 139 correctly classified and only 19 misclassified, resulting in an 88% accuracy. For Finance, the model matched the Logistic Regression with 122 correct predictions and 45 misclassifications, achieving a 73% accuracy. This indicates LDA is slightly more effective at identifying birdsongs compared to Logistic Regression, but similarly struggles to distinguish finance time series.

The accuracy results reveal a slight edge for LDA over Logistic Regression, with LDA achieving 0.80 compared to 0.79 for Logistic Regression. While the difference is minimal, it suggests that LDA might be capturing the underlying structure of the data just a bit more effectively.

**Variable importance**

Based on the observed output of tidy(logistic_fit) and summary of LDA coeffiencts seen above in model fit:

Both LDA and Logistic Regression identify entropy as a strong predictor of birdsongs and covariate2 as a major indicator of finance.
linearity is more pronounced in LDA (17.285 vs. -0.066), suggesting LDA finds it more influential in distinguishing between the two classes.
Both models downplay the importance of covariate1, though LDA still assigns it a positive coefficient.
x_acf1 remains consistently aligned with birdsongs in both models.
Both models align on the importance of entropy and covariate2, but LDA amplifies the role of linearity more significantly than logistic regression.

**Prediction Probabilities**

```{r}
#| echo: true
#| code-fold: true


# Logistic Regression - Training Predictions
logistic_train_prob <- logistic_fit |> 
  augment(new_data = train_data, type.predict = "prob") |> 
  mutate(.pred_correct = ifelse(type == "birdsongs", .pred_birdsongs, .pred_finance))

# Logistic Regression - Test Predictions
logistic_test_prob <- logistic_fit |> 
  augment(new_data = test_data, type.predict = "prob") |> 
  mutate(.pred_correct = ifelse(type == "birdsongs", .pred_birdsongs, .pred_finance))

# LDA - Training Predictions
lda_train_prob <- lda_fit |> 
  augment(new_data = train_data, type.predict = "prob") |> 
  mutate(.pred_correct = ifelse(type == "birdsongs", .pred_birdsongs, .pred_finance))

# LDA - Test Predictions
lda_test_prob <- lda_fit |> 
  augment(new_data = test_data, type.predict = "prob") |> 
  mutate(.pred_correct = ifelse(type == "birdsongs", .pred_birdsongs, .pred_finance))

```

```{r} 
#| echo: true
#| code-fold: true



# Logistic Regression - Training
p1 <- ggplot(logistic_train_prob, aes(x = type, y = .pred_correct, fill = type)) +
  geom_boxplot() +
  labs(title = "Logistic Regression - Training Data", x = "Type", y = "Prediction Confidence") +
  theme_minimal()

# Logistic Regression - Test
p2 <- ggplot(logistic_test_prob, aes(x = type, y = .pred_correct, fill = type)) +
  geom_boxplot() +
  labs(title = "Logistic Regression - Test Data", x = "Type", y = "Prediction Confidence") +
  theme_minimal()

# LDA - Training
p3 <- ggplot(lda_train_prob, aes(x = type, y = .pred_correct, fill = type)) +
  geom_boxplot() +
  labs(title = "LDA - Training Data", x = "Type", y = "Prediction Confidence") +
  theme_minimal()

# LDA - Test
p4 <- ggplot(lda_test_prob, aes(x = type, y = .pred_correct, fill = type)) +
  geom_boxplot() +
  labs(title = "LDA - Test Data", x = "Type", y = "Prediction Confidence") +
  theme_minimal()


(p1 | p2) / (p3 | p4)

```

Based on Training Data:
Both models are quite confident with birdsongs, showing narrow IQRs and medians around 0.75, and are able to differentiate them well.
For finance, the spread is much wider. Logistic Regression has a larger IQR with visible outliers, hinting at more uncertainty compared to birdsongs.
Based on Test Data:
The same trends can be observed, birdsongs are being predicted well with compact IQRs and solid medians.
For finance, both models struggle a bit more. LDA, in particular, shows a wider IQR and more scattered outliers.

Both models are consistently strong with birdsongs but show noticeable uncertainty with finance, especially in the test set. LDA's predictions are more spread out, while Logistic Regression displays more extreme outliers.


**Identifying Misclassifications in Logistic Regression**

```{r}
#| echo: true
#| code-fold: true


# Logistic Regression Misclassifications
logistic_misclassified <- logistic_test_pred |>
  filter(type != .pred_class)

# LDA Misclassifications
lda_misclassified <- lda_test_pred |>
  filter(type != .pred_class)

# Common Misclassifications 
common_misclassifications <- inner_join(
  logistic_misclassified |> select(linearity, entropy, x_acf1, covariate1, covariate2, type),
  lda_misclassified |> select(linearity, entropy, x_acf1, covariate1, covariate2, type),
  by = c("linearity", "entropy", "x_acf1", "covariate1", "covariate2", "type")
)


misclassification_summary <- tibble(
  Model = c("Logistic Regression", "LDA", "Common Misclassifications"),
  Misclassifications = c(nrow(logistic_misclassified), nrow(lda_misclassified), nrow(common_misclassifications))
)

# Summary 

kable(misclassification_summary, caption = "Summary of Misclassifications")



# Logistic Regression breakdown
logistic_misclassified_summary <- logistic_misclassified |>
  count(type, .pred_class) |>
  rename(Misclassified_As = .pred_class, Count = n)


kable(logistic_misclassified_summary, caption = "Logistic Regression Misclassifications")


# LDA breakdown
lda_misclassified_summary <- lda_misclassified |>
  count(type, .pred_class) |>
  rename(Misclassified_As = .pred_class, Count = n)


kable(lda_misclassified_summary, caption = "LDA Misclassifications")


# Common misclassification breakdown
common_misclassifications_summary <- common_misclassifications |>
  count(type) |>
  rename(Count = n)


kable(common_misclassifications_summary, caption = "Common Misclassifications Breakdown")

```


Based on the misclassification analysis:
- Logistic Regression had 69 misclassifications, while LDA had 64.
Both models struggled with the same observations, sharing 62 common errors.
- Logistic Regression misclassified Birdsongs as finance 24 times, while LDA did so 19 times.
- Both models struggled more with Finance, with 45 instances being misclassified as birdsongs in each case.
- Out of the 62 shared errors, 19 were birdsongs incorrectly labeled as finance, and 43 were finance mislabeled as birdsongs.
This highlights a clear difficulty in distinguishing finance series, which seem to overlap more with birdsongs in terms of feature space.
- Finance series are consistently more difficult to classify correctly, while birdsongs show slightly better separation in both models.

**Misclassification Plot**
```{r}
#| echo: true
#| code-fold: true

ggplot() +
  geom_point(data = logistic_misclassified, aes(x = linearity, y = entropy, color = "Logistic Regression"), alpha = 0.7) +
  geom_point(data = lda_misclassified, aes(x = linearity, y = entropy, color = "LDA"), alpha = 0.7) +
  labs(title = "Overlay of Misclassifications: Logistic Regression vs LDA",
       x = "Linearity",
       y = "Entropy") +
  scale_color_manual(values = c("Logistic Regression" = "blue", "LDA" = "red")) +
  theme_minimal()
```


- From the plot, both models struggle the most around the zero region of Linearity, suggesting overlap in features between classes.
- LDA misclassifies across a wider range, especially with negative linearity and higher entropy.
- Logistic Regression's errors are more clustered, indicating it is more selective but still misses specific patterns.
- Several points overlap, confirming the 62 common misclassifications.


#### ROC and AUC Analysis

ROC curves and AUC values are used to compare the ranking ability of the linear models across all possible thresholds. This provides a threshold-free measure of classification quality.

```{r}
#| echo: true
#| code-fold: true

# ROC Curve for Logistic Regression - Test Data
logistic_roc <- roc_curve(logistic_test_prob, truth = type, .pred_birdsongs)
logistic_auc <- roc_auc(logistic_test_prob, truth = type, .pred_birdsongs)

# ROC Curve for LDA - Test Data
lda_roc <- roc_curve(lda_test_prob, truth = type, .pred_birdsongs)
lda_auc <- roc_auc(lda_test_prob, truth = type, .pred_birdsongs)

# Plotting both ROC Curves
logistic_plot <- ggplot(logistic_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "blue") +
  geom_abline(linetype = "dashed") +
  labs(title = "Logistic Regression ROC Curve",
       subtitle = paste("AUC:", round(logistic_auc$.estimate, 3))) +
  theme_minimal()

lda_plot <- ggplot(lda_roc, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "red") +
  geom_abline(linetype = "dashed") +
  labs(title = "LDA ROC Curve",
       subtitle = paste("AUC:", round(lda_auc$.estimate, 3))) +
  theme_minimal()


logistic_plot + lda_plot

```

Logistic Regression
- The ROC curve shows good separation, with an AUC of 0.876, this indicates strong ability to differentiate between birdsongs and finance time series.
- It's clearly above the diagonal, the model performs better than random guessing consistently.
LDA 
- LDA's ROC curve is quite similar to Logistic Regression but has a slightly higher AUC of 0.882.
- This small improvement suggests LDA handles classification boundaries a bit more accurately, especially when separating birdsongs from finance.
Both models perform well, but LDA has a slight advantage with its higher AUC.
While the difference is not massive, it indicates LDA is just a bit better at identifying true positives while keeping false positives lower.


#### Interpreting Model Findings and Class Differences

- The analysis shows that financial time series and birdsongs have pretty distinct patterns. Financial data are generally more structured and linear, with higher linearity and covariate2 values reflecting the smooth, predictable trends you’d expect in markets. On the other hand, birdsongs are much more random and unpredictable, which is captured well by their higher entropy values. 
- Both models picked up on these differences, entropy was a strong indicator for birdsongs, while linearity pointed more towards finance. Interestingly, finance series were sometimes mistaken for birdsongs, suggesting there are some irregularities in financial data that mirror the chaotic patterns of birdsongs. 
- Lastly, the models did a good job capturing these differences, though there are still some tricky overlaps that lead to misclassifications.



#### Tuning a non-linear classifier 

#### Fitting a bad decision tree 

Understanding the need for regularization in decision trees:

- Below, a decision tree is constructed using constarints min_n = 1 and cost_complexity = 0. This is a bad decision tree as it is not regularized and will likely overfit the training data. 

```{r}
#| echo: true
#| code-fold: true

# decision tree model 
tree_spec <- decision_tree() %>%
  set_mode("classification") %>%
  set_engine("rpart", 
             control = rpart.control(minsplit = 1, cp = 0),
             model = TRUE)

# Fit the model 
bad_tree <- tree_spec %>%
  fit(type ~ ., data = train_data)

```


**Prediction Metrics**


**Train Predictions**
```{r}
#| echo: true
#| code-fold: true

train_preds <- predict(bad_tree, train_data) %>%
  bind_cols(train_data %>% select(type))

test_preds <- predict(bad_tree, test_data) %>%
  bind_cols(test_data %>% select(type))

```

```{r}
#| echo: true
#| code-fold: true

train_metrics <- train_preds %>%
  metrics(truth = type, estimate = .pred_class)

kable(train_metrics, caption = "Training Metrics for Bad Decision Tree")

```
The model achieves a perfect 100% accuracy on the training data. This is a classic sign of overfitting, where the tree memorizes the training samples exactly instead of learning generalizable patterns. The Kappa statistic is also 1.00, indicating perfect agreement with the true labels, which is unrealistic for real-world applications.


**Testing Predictions**
```{r}
#| echo: true
#| code-fold: true

test_metrics <- test_preds %>%
  metrics(truth = type, estimate = .pred_class)


kable(test_metrics, caption = "Testing Metrics for Bad Decision Tree")

```

On the testing set, the model's accuracy drops to 84%, and the Kappa statistic reduces to 0.67. Although the accuracy remains relatively high, it is clear that the model struggles to generalize beyond the training data. The noticeable drop from 100% to 84% reflects the impact of overfitting, where the tree’s excessive branching failed to capture meaningful, general patterns.

**Visualizing the Bad Decision Tree**

```{r}
#| echo: true
#| code-fold: true

# Extract the fitted model 
bad_tree_model <- bad_tree %>%
  extract_fit_engine()

# Plot 
rpart.plot::rpart.plot(bad_tree_model, main = "Bad Decision Tree", roundint = FALSE, cex = 0.5)

```

- The decision tree visual shows how deeply the model has branched. It's highly fragmented, with many branches and tiny nodes, expected when min_n = 1, allowing splits on nearly every observation.
- With cost_complexity = 0, there’s no pruning to reduce unnecessary splits, resulting in an overgrown structure that’s cluttered and difficult to interpret.
- The model memorises the training data but fails to generalise, this is a clear case of overfitting. It captures noise instead of meaningful patterns, highlighting the importance of proper tuning.

#### Hyper parameters 

Tree depth, minimum node size, and cost-complexity are tuned using cross-validation to identify a more stable model. This section examines how tuning affects tree behaviour and generalisation.

**Defineing  the decision tree model with the hyperparameters set to tune()**

```{r}
#| echo: true
#| code-fold: true


# decision tree model with tunable hyperparameters
tree_spec <- decision_tree(tree_depth = tune(),
  min_n = tune(),
  cost_complexity = tune()) %>%
  set_mode("classification") %>%
  set_engine("rpart")


```

Creating a grid of hyperparameters with 5 levels for each parameter, and set up a 5-fold cross-validation process to evaluate the model

```{r}
#| echo: true
#| code-fold: true


# grid of hyperparameters
hyper_grid <- grid_regular(
  tree_depth(),
  min_n(),
  cost_complexity(),
  levels = 5
)

# 5-fold cross-validation
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = type)

```

The workflow is defined with the decision tree specification and the formula for prediction. A grid search is then performed to evaluate all combinations of parameters.

```{r}
#| echo: true
#| code-fold: true

# Define the workflow 
tree_workflow <- workflow() %>%
  add_model(tree_spec) %>%
  add_formula(type ~ .)

# Tune the model
set.seed(123)
tuned_results <- tune_grid(
  tree_workflow,
  resamples = cv_folds,
  grid = hyper_grid,
  metrics = metric_set(accuracy, roc_auc)
)

```

Extracting all metrics to identify the best performing configurations. 

```{r}
#| echo: true
#| code-fold: true

# metrics from the tuning process
tree_metrics <- tuned_results %>%
  collect_metrics()

# top configurations by accuracy (sorted)
sorted_configs <- tree_metrics %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

# top 15 rows 
sorted_configs %>%
  slice_head(n = 15)




```

- The hyperparameter tuning process explored different combinations of tree_depth, min_n, and cost_complexity using 5-fold cross-validation. As the tree depth increased, the model's accuracy improved up to a certain point. The best configuration, with an average accuracy of 0.91, was, Tree Depth: 8, 
Min Node Size: 11, Cost Complexity: 1e-10. 
- This setup achieved a strong ROC AUC of 0.91, indicating it was effective at distinguishing between classes. The low standard error values suggest the model performed consistently well across the folds, capturing patterns in the data reliably.


**Plotting the performance metrics**
```{r}
#| echo: true
#| code-fold: true

# performance metrics for accuracy and ROC AUC
autoplot(tuned_results, metric = "accuracy") +
  labs(title = "Model Accuracy Across Hyperparameters")
```



Model Accuracy Across Hyperparameters
- The plot shows how accuracy changes across different combinations of tree depth, min_n, and cost complexity.
- The most consistent and accurate results (around 0.91) came from the combination of tree depth = 8, min_n = 11, and cost complexity = 1e-10.
- Models with smaller min_n values like 2 also reached close to 0.90 accuracy but didn’t improve further, and were more prone to overfitting.
- Higher min_n values (like 30 or 40) gave stable results but didn’t outperform the configuration with min_n = 11.
- Accuracy dropped noticeably when cost complexity increased to 1e-02, suggesting that too much pruning prevented the model from learning useful patterns.

In conclusion, the best-performing model struck a good balance: it was deep enough to capture structure, had enough data in each node to avoid noise, and wasn’t over-pruned.
The final choice:
Tree Depth: 8
Min Node Size: 11
Cost Complexity: 1e-10


**Plot: Model ROC AUC Across Hyperparameters**
```{r}
#| echo: true
#| code-fold: true

autoplot(tuned_results, metric = "roc_auc") +
  labs(title = "Model ROC AUC Across Hyperparameters")

```

Model ROC AUC Across Hyperparameters:
- The highest ROC AUC of 0.95 is observed in, Minimal Node Size = 30, with a Tree Depth of 15, and Cost Complexity set to a very low value of 1e-10.
- For Minimal Node Size = 11, the model achieves 0.91 ROC AUC when paired with a Tree Depth = 8.
When Cost Complexity is too high (e.g., 1e-02), the model is pruned excessively, losing its capacity to effectively separate classes.


Confirming the optimal hyperparameters determined from the grid search are:
Tree Depth: 8
Min Node Size: 11
Cost Complexity: 1e-10

**Selecting the best hyperparameters based on accuracy**
```{r}
#| echo: true
#| code-fold: true

# best hyperparameters based on accuracy
best_params <- select_best(tuned_results, metric = "accuracy")


best_params


```

- As observed before the configuration that performed the best during cross-validation is:
Tree Depth: 8
Min Node Size: 11
Cost Complexity: 1e-10

This configuration achieved the highest average accuracy of 0.91, with strong ROC AUC and low standard errors, indicating reliable and consistent performance across cross-validation folds.


#### Fitting the model using the best hyperparameters

The best hyperparameters are used to fit a refined decision tree, which is then evaluated on the test set. Improvements are compared against the unregularised tree to show the impact of tuning.

**Fit the final model using the best hyperparameters**

```{r}
#| echo: true
#| code-fold: true

# final model with optimal parameters
final_tree_spec <- decision_tree(
  tree_depth = 8,
  min_n = 11,
  cost_complexity = 1e-10
) %>%
  set_mode("classification") %>%
  set_engine("rpart")

final_tree_wf <- workflow() %>%
  add_model(final_tree_spec) %>%
  add_formula(type ~ .) %>%
  fit(data = train_data)

```

**Evaluting the model performance** 

_Training_ 

```{r}
#| echo: true
#| code-fold: true

# predictions on the training data
train_predictions <- predict(final_tree_wf, new_data = train_data, type = "prob") %>%
  bind_cols(predict(final_tree_wf, new_data = train_data)) %>%
  bind_cols(train_data)

# metrics - training set
train_metrics <- train_predictions %>%
  metrics(truth = type, estimate = .pred_class) 


train_metrics

```

The tuned decision tree achieved 95% accuracy and a Kappa of 0.90 on the training data.
This shows the model correctly classified most observations and that its predictions strongly agree with the true labels, beyond chance.
Unlike the overfit model from above, this tree avoids memorising the data, suggesting a good balance between flexibility and generalisation.

_Testing_

```{r}
#| echo: true
#| code-fold: true

# predictions on the testing data
test_predictions <- predict(final_tree_wf, new_data = test_data, type = "prob") %>%
  bind_cols(predict(final_tree_wf, new_data = test_data, type = "class")) %>%
  bind_cols(test_data)


# metrics- testing set
test_metrics <- test_predictions %>%
  metrics(truth = type, estimate = .pred_class) 


test_metrics

```

- On the test set, the model reached 82% accuracy with a Kappa of 0.64.
This is a noticeable drop from the training accuracy of 95% and Kappa of 0.90.
The decrease suggests the model doesn’t overfit, but also shows it picked up on some training specific patterns that didn’t carry over as well.
- Despite that, the test results are good and show the model generalises fairly well.

**Confusion Matrixes**

**Confusion Matrix for Training Data**
```{r}
#| echo: true
#| code-fold: true

# Confusion Matrix for Training Data
train_conf_matrix <- train_predictions %>%
  conf_mat(truth = type, estimate = .pred_class)


train_conf_matrix
```
- The model classified most training examples correctly, with only 34 misclassifications out of 649.
- It confused some finance texts as birdsongs (22) and a few birdsongs as finance (12).
- This shows the model learned the training patterns well without becoming overly specific, which is what we look for after tuning.

**Confusion Matrix for Testing Data**

```{r}
#| echo: true
#| code-fold: true

# Confusion Matrix for Testing Data
test_conf_matrix <- test_predictions %>%
  conf_mat(truth = type, estimate = .pred_class)


test_conf_matrix

```
- The model got most predictions right on the test set, correctly identifying 132 birdsongs and 134 finance texts.
- A total of 59 errors, it misclassifies 33 finance as birdsongs and 26 birdsongs as finance 
- This matches the earlier accuracy of 82%, and it’s a sign that while the model generalises fairly well, it still struggles a bit with overlap between the two classes.

**ROC Curve for Testing Data**
```{r}
#| echo: true
#| code-fold: true

# ROC Curve for Testing Data
roc_curve_test <- test_predictions %>%
  roc_curve(truth = type, .pred_birdsongs) %>%
  autoplot() +
  ggtitle("ROC Curve for Optimized Decision Tree (Test Data)")


roc_curve_test

roc_auc(test_predictions, truth = type, .pred_birdsongs)

```
- The ROC curve for the test set shows that the model separates the two classes well, with a steep rise toward the top left corner.
- The AUC value of 0.90 confirms this, it means that, across all possible thresholds, the model ranks birdsongs higher than finance 90% of the time.
- Even though the test accuracy was lower at 82%, the high AUC shows that the model is still effective at distinguishing between the two classes.

**Comparison with linear models on test data**

- Compared to the linear models, the tuned decision tree performed slightly better on both accuracy and AUC.
- Logistic regression achieved 79% accuracy and an AUC of 0.876, while LDA reached 80% accuracy with an AUC of 0.882.
- The decision tree outperformed both, with 82% accuracy and an AUC of 0.90, suggesting it separated the classes more effectively.
- The tree also had the advantage of capturing non-linear patterns and didn’t rely on assumptions like multivariate normality or equal covariance, which the data didn’t fully satisfy.
- While the linear models were easier to interpret — especially LDA, which clearly highlighted entropy and covariate2 — they struggled more with overlapping feature space, particularly for finance series.
- Given the stronger performance and flexibility, the tuned decision tree would be the more reliable choice in practice. 


#### Which is the better classifier? 

A random forest is tuned and fitted to capture nonlinear patterns and feature interactions. We compare its performance to linear and single-tree models to see how ensemble methods improve stability and accuracy.

#### a. Random Forest Model 

Defining a random forest model using the with 1000 trees, and tuned two key parameters: 
- mtry - number of predictors to consider at each split 
- min_n - minimum observations per node. 

**Define the random forest model specification**
```{r}
#| echo: true
#| code-fold: true

# random forest model 
rf_spec <- rand_forest(
  mode = "classification",
  trees = 1000,
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity")

```

 
**Set up the tuning grid and cross-validation**

5-fold cross-validation using a regular grid of 25 combinations
 
```{r}
#| echo: true
#| code-fold: true

# 5-fold cross-validation
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = type)

# Grid of hyperparameters
rf_grid <- grid_regular(
  mtry(range = c(1, 5)),
  min_n(range = c(2, 40)),
  levels = 5
)

```
 
 
**Tune the random forest model**

```{r}
#| echo: true
#| code-fold: true

# Workflow
rf_workflow <- workflow() %>%
  add_model(rf_spec) %>%
  add_formula(type ~ .)

# Tune the model
set.seed(123)
rf_tune_results <- tune_grid(
  rf_workflow,
  resamples = cv_folds,
  grid = rf_grid,
  metrics = metric_set(accuracy, roc_auc)
)

```
 
**Extract the best hyperparameters based on accuracy**
```{r}
#| echo: true
#| code-fold: true

# Review tuning metrics
collect_metrics(rf_tune_results) %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

# Best hyperparameters
best_rf_params <- select_best(rf_tune_results, metric = "accuracy")
best_rf_params

```

- After tuning 25 combinations of mtry and min_n, the best-performing setup was mtry = 2 and min_n = 11, with an average accuracy of 0.909.
- The low standard error of  0.008, also suggests the model performed consistently across the 5 cross-validation folds.
- This setup performed well across folds, likely because it limited overfitting while still capturing useful patterns.

**Finalize the model with the best hyperparameters and fit it to the training data**
```{r}
#| echo: true
#| code-fold: true

# Final model spec with best params
final_rf_spec <- finalize_model(rf_spec, best_rf_params)

# Final workflow
final_rf_workflow <- rf_workflow %>%
  update_model(final_rf_spec)

# Fit the model
final_rf_fit <- fit(final_rf_workflow, data = train_data)

```
 
**Evaluate the model on training data**

**Training predictions and metrics**
```{r}
#| echo: true
#| code-fold: true

# Training predictions
train_rf_preds <- predict(final_rf_fit, train_data, type = "prob") %>%
  bind_cols(predict(final_rf_fit, train_data)) %>%
  bind_cols(train_data)

train_rf_metrics <- train_rf_preds %>%
  metrics(truth = type, estimate = .pred_class)

train_rf_metrics

```

- The model achieved 97.5% accuracy and a Kappa of 0.95, indicating it classified most training observations correctly with very few errors.
- The high Kappa shows that the predictions closely matched the true labels, not just by chance. While strong performance is expected, it also raises the possibility of slight overfitting.

**Testing predictions and metrics**
```{r}
#| echo: true
#| code-fold: true

# Testing predictions
test_rf_preds <- predict(final_rf_fit, test_data, type = "prob") %>%
  bind_cols(predict(final_rf_fit, test_data)) %>%
  bind_cols(test_data)

test_rf_metrics <- test_rf_preds %>%
  metrics(truth = type, estimate = .pred_class)

test_rf_metrics

```
 
The model achieved 86.5% accuracy and a Kappa of 0.73 on the test set. The ~11% drop from training accuracy is reasonable and indicates that the model didn’t overfit. It was able to learn meaningful patterns from the training data that carry over to new data. The Kappa score still shows strong agreement with the true labels, suggesting reliable generalisation.


#### Exploring Boosted Trees 

A boosted tree model is fitted to explore how sequential learning captures difficult cases. Tuning results and performance metrics help assess bias-variance trade-offs relative to the random forest.

**Boosted tree model**

A boosted tree model defined using the xgboost with 1000 boosting rounds. The key hyperparameters tuned were tree_depth, which controls model complexity, and learn_rate, which determines how quickly the model adapts to errors.

```{r}
#| echo: true
#| code-fold: true

# Model spec
boost_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = 0
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

```

**Set up the tuning grid and 5 fold cross-validation**

A regular grid of 25 combinations created by varying tree_depth and learn_rate across 5 levels each. 

```{r}
#| echo: true
#| code-fold: true


# Define tuning grid
boost_grid <- grid_regular(
  tree_depth(range = c(1, 10)),
  learn_rate(range = c(0.001, 0.3)),
  levels = 5
)

```

#### Tune the boosted tree model
```{r}
#| echo: true
#| code-fold: true

# Workflow
boost_wf <- workflow() %>%
  add_model(boost_spec) %>%
  add_formula(type ~ .)

# Grid search tuning
set.seed(123)
boost_tune_results <- tune_grid(
  boost_wf,
  resamples = cv_folds,
  grid = boost_grid,
  metrics = metric_set(accuracy, roc_auc)
)

```


```{r}
#| echo: true
#| code-fold: true

# Review results and select best
boost_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

best_boost_params <- select_best(boost_tune_results, metric = "accuracy")
best_boost_params

```

- The best configuration was tree_depth = 10 and learn_rate = 1.41, with an average accuracy of 0.903.
- The deeper trees helped capture complex patterns, while the high learning rate allowed faster corrections, improving training speed.
- Although this increases the risk of overfitting, the model showed stable performance across folds with std. error = 0.0125 , suggesting good generalisation during cross-validation.

**Finalize and fit the boosted tree model**

```{r}
#| echo: true
#| code-fold: true

# Final model and workflow
final_boost_spec <- finalize_model(boost_spec, best_boost_params)

final_boost_wf <- boost_wf %>%
  update_model(final_boost_spec)

final_boost_fit <- fit(final_boost_wf, data = train_data)

```


**Evaluate the boosted tree model on training data**
```{r}
#| echo: true
#| code-fold: true

# Training performance
train_boost_preds <- predict(final_boost_fit, train_data, type = "prob") %>%
  bind_cols(predict(final_boost_fit, train_data)) %>%
  bind_cols(train_data)

train_boost_metrics <- train_boost_preds %>%
  metrics(truth = type, estimate = .pred_class)

train_boost_metrics

```

- The boosted tree model achieved 100% accuracy and a Kappa of 1.00 on the training set.
— This confirmins it fully learned the data, it made no classification errors and its predictions perfectly matched the true labels, which is often a sign of overfitting.  It likely captured both true patterns and noise.


**Confusion Matrix for Training Data**

```{r}
#| echo: true
#| code-fold: true

# Confusion Matrix - Training
train_boost_conf <- train_boost_preds %>%
  conf_mat(truth = type, estimate = .pred_class)

train_boost_conf

```
The confusion matrix shows perfect classification on the training set, all 316 birdsongs and 333 finance examples were predicted correctly. This matches the accuracy and Kappa of 1, and clearly suggests the model has overfitted to the training data.

**Evaluate the boosted tree model on testing data**
```{r}
#| echo: true
#| code-fold: true

test_boost_preds <- predict(final_boost_fit, test_data, type = "prob") %>%
  bind_cols(predict(final_boost_fit, test_data)) %>%
  bind_cols(test_data)

test_boost_metrics <- test_boost_preds %>%
  metrics(truth = type, estimate = .pred_class)

test_boost_metrics

```

- On the test set, the boosted tree achieved an accuracy of 84.3% and a Kappa of 0.69. - While this is a noticeable drop from the perfect training results, it’s still a good outcome. The gap between training and testing confirms that the model overfit to the training data to some extent. 
- However, the test accuracy and Kappa still reflect good predictive performance and reasonable generalisation.

**Confusion Matrix for Testing Data**
```{r}
#| echo: true
#| code-fold: true

# Confusion Matrix - Testing
test_boost_conf <- test_boost_preds %>%
  conf_mat(truth = type, estimate = .pred_class)

test_boost_conf

```
- The confusion matrix shows the model correctly classified 135 birdsongs and 139 finance cases, with 28 finance misclassified as birdsongs and 23 birdsongs misclassified as finance.
- This aligns with the test accuracy of 84.3% and Kappa of 0.69, indicating the model generalises reasonably well despite some confusion between the two classes.



#### Classifier Comparison and Final Insights

All classifiers (logistic, LDA, tuned tree, random forest, boosted tree) are compared using ROC curves. AUC scores give a clear performance ranking across modelling strategies.

**ROC Comparison**

```{r}
#| echo: true
#| code-fold: true


# Logistic Regression
roc_log <- roc_curve(logistic_test_prob, truth = type, .pred_birdsongs) %>%
  mutate(model = "Logistic")

# LDA
roc_lda <- roc_curve(lda_test_prob, truth = type, .pred_birdsongs) %>%
  mutate(model = "LDA")

# Decision Tree
roc_tree <- roc_curve(test_predictions, truth = type, .pred_birdsongs) %>%
  mutate(model = "Decision Tree")

# Random Forest 
roc_rf <- roc_curve(test_rf_preds, truth = type, .pred_birdsongs) %>%
  mutate(model = "Random Forest")

# Boosted Tree 
roc_boost <- roc_curve(test_boost_preds, truth = type, .pred_birdsongs) %>%
  mutate(model = "Boosted Tree")


roc_all <- bind_rows(roc_log, roc_lda, roc_tree, roc_rf, roc_boost)

#AUC 
auc_scores <- bind_rows(
  roc_auc(logistic_test_prob, truth = type, .pred_birdsongs) %>% mutate(model = "Logistic"),
  roc_auc(lda_test_prob, truth = type, .pred_birdsongs) %>% mutate(model = "LDA"),
  roc_auc(test_predictions, truth = type, .pred_birdsongs) %>% mutate(model = "Decision Tree"),
  roc_auc(test_rf_preds, truth = type, .pred_birdsongs) %>% mutate(model = "Random Forest"),
  roc_auc(test_boost_preds, truth = type, .pred_birdsongs) %>% mutate(model = "Boosted Tree")
)


auc_labels <- auc_scores %>%
  mutate(label = paste0(model, " (AUC = ", round(.estimate, 3), ")"))

model_order <- auc_labels$model
color_map <- c(
  "Logistic" = "#1f77b4",
  "LDA" = "#17becf",
  "Decision Tree" = "#bcbd22",
  "Random Forest" = "#e377c2",
  "Boosted Tree" = "#ff7f0e"
)
names(color_map) <- model_order
label_map <- auc_labels$label
names(label_map) <- model_order

# Plot ROC curves 
ggplot(roc_all, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(linewidth = 0.6) +
  geom_abline(linetype = "dashed", color = "grey") +
  scale_color_manual(
    values = color_map,
    breaks = model_order,
    labels = label_map
  ) +
  labs(
    title = "ROC Curves for All Models",
    x = "1 - Specificity (False Positive Rate)",
    y = "Sensitivity (True Positive Rate)",
    color = "Model"
  ) +
  theme_minimal()

```
The ROC curves for all five models highlight how well each one distinguishes between birdsongs and finance time series.

- Logistic Regression and LDA have similar curves, with LDA performing slightly better AUC = 0.882 , as comapred to AUC = 0.876 of logistic. This matches earlier oberveations , where LDA handled class separation a bit more effectively.
- The Tuned Decision Tree has a noticeably steeper curve and a higher AUC of 0.904, showing that tuning helped it better capture decision boundaries.
- Boosted Tree performs strongly as well, with an AUC of 0.918. Despite overfitting the training data, its test ROC performance indicates it generalised well overall.
- Random Forest has the highest curve overall, with an AUC of 0.942. It ranked the classes most effectively and remained stable on the test set, confirming its strong overall performance.

In conclusion, the ROC curve shows that Random Forest did the best job at separating the classes, followed closely by Boosted Tree and the Tuned Decision Tree. The linear models performed reasonably well but struggled more with the class overlap in the data. This conclusion is consistent with the earlier observations when fitiing and summarizing the models. Random Forest and Boosted Tree are the most reliable classifiers for this dataset, effectively capturing the complex patterns in the time series data.

#### Which model is best?

The best-performing classifier is identified, and its results are used to characterise how birdsongs differ from financial time-series. This ties the modelling results back to the data structure and feature behaviour observed earlier.

**Best Model and how the time series for financial data and birdsongs typically differ**

After comparing all five models, Random Forest stands out as the most effective classifier, with the highest ROC AUC of 0.942. It consistently generalised well on the test data, outperforming both linear and non-linear alternatives without showing signs of overfitting. This aligns with the model’s strength in handling feature interactions and non-linearities, which are apparent in this dataset. In contrast, Boosted Tree, while also strong (AUC = 0.918), slightly overfit the training data, and Tuned Decision Tree (AUC = 0.904) offered a good balance but didn’t quite match Random Forest’s stability. From the analysis, we can observe that financial time series is more structured, with higher values in linearity and covariate2, while birdsong series is more random and irregular, showing higher entropy and weaker autocorrelation. These differences are also reflected druing the individual model analysis, where linear models like Logistic Regression and LDA struggled more due to class overlap and violated assumptions, whereas tree-based models adapted better to the complexity of the data.


## References

Hadley Wickham, Dianne Cook, Heike Hofmann, Andreas Buja
  (2011). tourr: An R Package for Exploring Multivariate
  Data with Projections. Journal of Statistical Software,
  40(2), 1-18. URL http://www.jstatsoft.org/v40/i02/.
  
Kuhn et al., (2020). Tidymodels: a collection of
  packages for modeling and machine learning using
  tidyverse principles. https://www.tidymodels.org  
  





